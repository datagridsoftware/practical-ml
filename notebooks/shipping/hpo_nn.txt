Fitting 5 folds for each of 10 candidates, totalling 50 fits
Iteration 1, loss = 3.08666837
Iteration 2, loss = 0.81412004
Iteration 3, loss = 0.71307454
Iteration 4, loss = 0.68510993
Iteration 5, loss = 0.67977415
Iteration 6, loss = 0.66961339
Iteration 7, loss = 0.68538851
Iteration 8, loss = 0.68559089
Iteration 9, loss = 0.69210637
Iteration 10, loss = 0.66766531
Iteration 11, loss = 0.68711388
Iteration 12, loss = 0.67116395
Iteration 13, loss = 0.68170278
Iteration 14, loss = 0.67029199
Iteration 15, loss = 0.67398982
Iteration 16, loss = 0.66958727
Iteration 17, loss = 0.65948720
Iteration 18, loss = 0.66963608
Iteration 19, loss = 0.67207522
Iteration 20, loss = 0.67631230
Iteration 21, loss = 0.66531469
Iteration 22, loss = 0.68056295
Iteration 23, loss = 0.65409164
Iteration 24, loss = 0.66337860
Iteration 25, loss = 0.66912217
Iteration 26, loss = 0.66164827
Iteration 27, loss = 0.67176693
Iteration 28, loss = 0.65236434
Iteration 29, loss = 0.67266075
Iteration 30, loss = 0.65184480
Iteration 31, loss = 0.66264517
Iteration 32, loss = 0.66063016
Iteration 33, loss = 0.67905069
Iteration 34, loss = 0.66009773
Iteration 35, loss = 0.66768789
Iteration 36, loss = 0.66877207
Iteration 37, loss = 0.65164093
Iteration 38, loss = 0.66137813
Iteration 39, loss = 0.66373108
Iteration 40, loss = 0.66115010
Iteration 41, loss = 0.66483008
Iteration 42, loss = 0.65880896
Iteration 43, loss = 0.64722234
Iteration 44, loss = 0.66268550
Iteration 45, loss = 0.66172978
Iteration 46, loss = 0.66597639
Iteration 47, loss = 0.64569037
Iteration 48, loss = 0.65799837
Iteration 49, loss = 0.66056372
Iteration 50, loss = 0.67581286
Iteration 51, loss = 0.65040725
Iteration 52, loss = 0.67420155
Iteration 53, loss = 0.64774465
Iteration 54, loss = 0.65121517
Iteration 55, loss = 0.66499405
Iteration 56, loss = 0.65718091
Iteration 57, loss = 0.65195364
Iteration 58, loss = 0.65850061
Iteration 59, loss = 0.66458479
Iteration 60, loss = 0.65680160
Iteration 61, loss = 0.65412085
Iteration 62, loss = 0.65494656
Iteration 63, loss = 0.65394892
Iteration 64, loss = 0.66483050
Iteration 65, loss = 0.65566190
Iteration 66, loss = 0.65281875
Iteration 67, loss = 0.65772055
Iteration 68, loss = 0.66175660
Iteration 69, loss = 0.66866075
Iteration 70, loss = 0.64691626
Iteration 71, loss = 0.65667416
Iteration 72, loss = 0.64938114
Iteration 73, loss = 0.65585159
Iteration 74, loss = 0.65659373
Iteration 75, loss = 0.65760902
Iteration 76, loss = 0.65132482
Iteration 77, loss = 0.65072607
Iteration 78, loss = 0.67029855
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 1/5] END estimator__activation=relu, estimator__hidden_layer_sizes=16;, score=0.599 total time=  46.4s
Iteration 1, loss = 7.22654477
Iteration 2, loss = 1.21762254
Iteration 3, loss = 0.76298806
Iteration 4, loss = 0.71913414
Iteration 5, loss = 0.67715233
Iteration 6, loss = 0.67818777
Iteration 7, loss = 0.65977264
Iteration 8, loss = 0.65325762
Iteration 9, loss = 0.65421903
Iteration 10, loss = 0.66644967
Iteration 11, loss = 0.64625537
Iteration 12, loss = 0.65281644
Iteration 13, loss = 0.65477569
Iteration 14, loss = 0.65275788
Iteration 15, loss = 0.64759988
Iteration 16, loss = 0.64888472
Iteration 17, loss = 0.65166672
Iteration 18, loss = 0.65345711
Iteration 19, loss = 0.65264850
Iteration 20, loss = 0.66455615
Iteration 21, loss = 0.65277687
Iteration 22, loss = 0.65659410
Iteration 23, loss = 0.64714867
Iteration 24, loss = 0.66069815
Iteration 25, loss = 0.64384244
Iteration 26, loss = 0.65595703
Iteration 27, loss = 0.64277501
Iteration 28, loss = 0.65436956
Iteration 29, loss = 0.65502890
Iteration 30, loss = 0.65643266
Iteration 31, loss = 0.63484370
Iteration 32, loss = 0.64664418
Iteration 33, loss = 0.64103547
Iteration 34, loss = 0.64300609
Iteration 35, loss = 0.65609663
Iteration 36, loss = 0.64398528
Iteration 37, loss = 0.64155260
Iteration 38, loss = 0.64306401
Iteration 39, loss = 0.64107754
Iteration 40, loss = 0.66232676
Iteration 41, loss = 0.64745492
Iteration 42, loss = 0.64938418
Iteration 43, loss = 0.64318468
Iteration 44, loss = 0.62914246
Iteration 45, loss = 0.65367489
Iteration 46, loss = 0.64799736
Iteration 47, loss = 0.64237483
Iteration 48, loss = 0.65450466
Iteration 49, loss = 0.65594558
Iteration 50, loss = 0.64135247
Iteration 51, loss = 0.64132847
Iteration 52, loss = 0.63993880
Iteration 53, loss = 0.64854851
Iteration 54, loss = 0.63864292
Iteration 55, loss = 0.63792295
Iteration 56, loss = 0.63212663
Iteration 57, loss = 0.64216693
Iteration 58, loss = 0.63502684
Iteration 59, loss = 0.64370179
Iteration 60, loss = 0.63334083
Iteration 61, loss = 0.63072057
Iteration 62, loss = 0.64805256
Iteration 63, loss = 0.63254514
Iteration 64, loss = 0.62802410
Iteration 65, loss = 0.62752592
Iteration 66, loss = 0.63350109
Iteration 67, loss = 0.63695203
Iteration 68, loss = 0.64725562
Iteration 69, loss = 0.63038145
Iteration 70, loss = 0.63452593
Iteration 71, loss = 0.65247584
Iteration 72, loss = 0.63282456
Iteration 73, loss = 0.63222096
Iteration 74, loss = 0.61991063
Iteration 75, loss = 0.63117703
Iteration 76, loss = 0.63083490
Iteration 77, loss = 0.63811786
Iteration 78, loss = 0.64302423
Iteration 79, loss = 0.63597186
Iteration 80, loss = 0.62837041
Iteration 81, loss = 0.63353279
Iteration 82, loss = 0.62538178
Iteration 83, loss = 0.63778775
Iteration 84, loss = 0.62847083
Iteration 85, loss = 0.64420018
Iteration 86, loss = 0.62538514
Iteration 87, loss = 0.63229426
Iteration 88, loss = 0.62462691
Iteration 89, loss = 0.64078372
Iteration 90, loss = 0.62876284
Iteration 91, loss = 0.62209438
Iteration 92, loss = 0.63504545
Iteration 93, loss = 0.64266058
Iteration 94, loss = 0.62211701
Iteration 95, loss = 0.62204482
Iteration 96, loss = 0.64459216
Iteration 97, loss = 0.62027891
Iteration 98, loss = 0.62183111
Iteration 99, loss = 0.64228309
Iteration 100, loss = 0.64659307
Iteration 101, loss = 0.61947832
Iteration 102, loss = 0.62580442
Iteration 103, loss = 0.62268747
Iteration 104, loss = 0.63207426
Iteration 105, loss = 0.61350584
Iteration 106, loss = 0.62227003
Iteration 107, loss = 0.63392383
Iteration 108, loss = 0.63183738
Iteration 109, loss = 0.63824869
Iteration 110, loss = 0.62663758
Iteration 111, loss = 0.61953313
Iteration 112, loss = 0.62581688
Iteration 113, loss = 0.61871654
Iteration 114, loss = 0.63600572
Iteration 115, loss = 0.62952133
Iteration 116, loss = 0.62438186
Iteration 117, loss = 0.62372229
Iteration 118, loss = 0.61934062
Iteration 119, loss = 0.61939008
Iteration 120, loss = 0.62842330
Iteration 121, loss = 0.61327214
Iteration 122, loss = 0.61257150
Iteration 123, loss = 0.62698307
Iteration 124, loss = 0.61852533
Iteration 125, loss = 0.62688255
Iteration 126, loss = 0.62057283
Iteration 127, loss = 0.62096826
Iteration 128, loss = 0.61746511
Iteration 129, loss = 0.61603598
Iteration 130, loss = 0.62379330
Iteration 131, loss = 0.62752621
Iteration 132, loss = 0.63007000
Iteration 133, loss = 0.60454108
Iteration 134, loss = 0.62603237
Iteration 135, loss = 0.62209414
Iteration 136, loss = 0.62289274
Iteration 137, loss = 0.61274968
Iteration 138, loss = 0.61483255
Iteration 139, loss = 0.62856452
Iteration 140, loss = 0.61337341
Iteration 141, loss = 0.63424562
Iteration 142, loss = 0.62152565
Iteration 143, loss = 0.63575079
Iteration 144, loss = 0.61350132
Iteration 145, loss = 0.62927587
Iteration 146, loss = 0.61381389
Iteration 147, loss = 0.62069567
Iteration 148, loss = 0.61115382
Iteration 149, loss = 0.61658649
Iteration 150, loss = 0.61146478
Iteration 151, loss = 0.62059671
Iteration 152, loss = 0.61606915
Iteration 153, loss = 0.61974349
Iteration 154, loss = 0.60912419
Iteration 155, loss = 0.63095397
Iteration 156, loss = 0.60756558
Iteration 157, loss = 0.61549072
Iteration 158, loss = 0.61115965
Iteration 159, loss = 0.61756707
Iteration 160, loss = 0.62312345
Iteration 161, loss = 0.60900307
Iteration 162, loss = 0.60864167
Iteration 163, loss = 0.61879804
Iteration 164, loss = 0.62687892
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 2/5] END estimator__activation=relu, estimator__hidden_layer_sizes=16;, score=0.643 total time=  54.5s

Iteration 1, loss = 5.34561757
Iteration 2, loss = 0.87010542
Iteration 3, loss = 0.80150767
Iteration 4, loss = 0.79312755
Iteration 5, loss = 0.79052120
Iteration 6, loss = 0.77390408
Iteration 7, loss = 0.78291297
Iteration 8, loss = 0.77052892
Iteration 9, loss = 0.76998767
Iteration 10, loss = 0.83473345
Iteration 11, loss = 0.71250864
Iteration 12, loss = 0.85180460
Iteration 13, loss = 0.72616907
Iteration 14, loss = 0.72752738
Iteration 15, loss = 0.79887914
Iteration 16, loss = 0.72453496
Iteration 17, loss = 0.76688144
Iteration 18, loss = 0.73815056
Iteration 19, loss = 0.75560208
Iteration 20, loss = 0.75741450
Iteration 21, loss = 0.78892629
Iteration 22, loss = 0.73811107
Iteration 23, loss = 0.73056327
Iteration 24, loss = 0.74278745
Iteration 25, loss = 0.77796963
Iteration 26, loss = 0.76975244
Iteration 27, loss = 0.73419397
Iteration 28, loss = 0.76283714
Iteration 29, loss = 0.73839712
Iteration 30, loss = 0.71665127
Iteration 31, loss = 0.78109874
Iteration 32, loss = 0.76418245
Iteration 33, loss = 0.72966645
Iteration 34, loss = 0.74575693
Iteration 35, loss = 0.73563932
Iteration 36, loss = 0.80524960
Iteration 37, loss = 0.71142388
Iteration 38, loss = 0.68088742
Iteration 39, loss = 0.79709958
Iteration 40, loss = 0.74848444
Iteration 41, loss = 0.71631131
Iteration 42, loss = 0.77031018
Iteration 43, loss = 0.74823862
Iteration 44, loss = 0.73839895
Iteration 45, loss = 0.74248158
Iteration 46, loss = 0.77919230
Iteration 47, loss = 0.75151645
Iteration 48, loss = 0.69377717
Iteration 49, loss = 0.72678288
Iteration 50, loss = 0.73866548
Iteration 51, loss = 0.69211819
Iteration 52, loss = 0.75232270
Iteration 53, loss = 0.73812126
Iteration 54, loss = 0.73995851
Iteration 55, loss = 0.74901475
Iteration 56, loss = 0.68816318
Iteration 57, loss = 0.72817714
Iteration 58, loss = 0.74403217
Iteration 59, loss = 0.77917378
Iteration 60, loss = 0.68153209
Iteration 61, loss = 0.73173119
Iteration 62, loss = 0.70938521
Iteration 63, loss = 0.77008815
Iteration 64, loss = 0.73161643
Iteration 65, loss = 0.73502685
Iteration 66, loss = 0.70411528
Iteration 67, loss = 0.72074150
Iteration 68, loss = 0.72875113
Iteration 69, loss = 0.70824122
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 3/5] END estimator__activation=relu, estimator__hidden_layer_sizes=16;, score=0.592 total time=  43.6s
Iteration 1, loss = 7.47823280
Iteration 2, loss = 0.91587850
Iteration 3, loss = 0.69050595
Iteration 4, loss = 0.65723823
Iteration 5, loss = 0.67456772
Iteration 6, loss = 0.65230553
Iteration 7, loss = 0.65219136
Iteration 8, loss = 0.64818366
Iteration 9, loss = 0.65627086
Iteration 10, loss = 0.65448853
Iteration 11, loss = 0.63567913
Iteration 12, loss = 0.64511010
Iteration 13, loss = 0.65198954
Iteration 14, loss = 0.64624025
Iteration 15, loss = 0.66170832
Iteration 16, loss = 0.64537698
Iteration 17, loss = 0.65079310
Iteration 18, loss = 0.66112641
Iteration 19, loss = 0.65378461
Iteration 20, loss = 0.63459947
Iteration 21, loss = 0.64884450
Iteration 22, loss = 0.64849675
Iteration 23, loss = 0.64014934
Iteration 24, loss = 0.64718235
Iteration 25, loss = 0.64625360
Iteration 26, loss = 0.66208914
Iteration 27, loss = 0.65148909
Iteration 28, loss = 0.64826859
Iteration 29, loss = 0.64765614
Iteration 30, loss = 0.63626341
Iteration 31, loss = 0.65834294
Iteration 32, loss = 0.63836364
Iteration 33, loss = 0.65460307
Iteration 34, loss = 0.65530184
Iteration 35, loss = 0.63795173
Iteration 36, loss = 0.65562501
Iteration 37, loss = 0.64001755
Iteration 38, loss = 0.65362136
Iteration 39, loss = 0.64857797
Iteration 40, loss = 0.64189897
Iteration 41, loss = 0.64270243
Iteration 42, loss = 0.65190930
Iteration 43, loss = 0.65913503
Iteration 44, loss = 0.64329623
Iteration 45, loss = 0.65488663
Iteration 46, loss = 0.63715032
Iteration 47, loss = 0.65223914
Iteration 48, loss = 0.66453173
Iteration 49, loss = 0.63789364
Iteration 50, loss = 0.64122923
Iteration 51, loss = 0.65712875
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 4/5] END estimator__activation=relu, estimator__hidden_layer_sizes=16;, score=0.563 total time=  41.4s
Iteration 1, loss = 5.07025889
Iteration 2, loss = 0.98407059
Iteration 3, loss = 0.77287133
Iteration 4, loss = 0.73776278
Iteration 5, loss = 0.72737212
Iteration 6, loss = 0.71180900
Iteration 7, loss = 0.70719895
Iteration 8, loss = 0.68594913
Iteration 9, loss = 0.68238107
Iteration 10, loss = 0.68527173
Iteration 11, loss = 0.67806556
Iteration 12, loss = 0.66786446
Iteration 13, loss = 0.67474770
Iteration 14, loss = 0.65556241
Iteration 15, loss = 0.67652427
Iteration 16, loss = 0.65456785
Iteration 17, loss = 0.66796129
Iteration 18, loss = 0.66615365
Iteration 19, loss = 0.66022401
Iteration 20, loss = 0.66494093
Iteration 21, loss = 0.65736544
Iteration 22, loss = 0.66673846
Iteration 23, loss = 0.65355387
Iteration 24, loss = 0.66094019
Iteration 25, loss = 0.65972017
Iteration 26, loss = 0.65481937
Iteration 27, loss = 0.65261715
Iteration 28, loss = 0.67422028
Iteration 29, loss = 0.64718016
Iteration 30, loss = 0.66597882
Iteration 31, loss = 0.64838048
Iteration 32, loss = 0.65926005
Iteration 33, loss = 0.65337101
Iteration 34, loss = 0.66078163
Iteration 35, loss = 0.66270411
Iteration 36, loss = 0.65153688
Iteration 37, loss = 0.66573924
Iteration 38, loss = 0.67387211
Iteration 39, loss = 0.65845345
Iteration 40, loss = 0.64576653
Iteration 41, loss = 0.64946285
Iteration 42, loss = 0.65374562
Iteration 43, loss = 0.65725769
Iteration 44, loss = 0.66442121
Iteration 45, loss = 0.64652367
Iteration 46, loss = 0.65986446
Iteration 47, loss = 0.64371421
Iteration 48, loss = 0.65233527
Iteration 49, loss = 0.64680713
Iteration 50, loss = 0.65525219
Iteration 51, loss = 0.65901916
Iteration 52, loss = 0.64613628
Iteration 53, loss = 0.64707916
Iteration 54, loss = 0.66306701
Iteration 55, loss = 0.64733543
Iteration 56, loss = 0.65212776
Iteration 57, loss = 0.65416352
Iteration 58, loss = 0.65783417
Iteration 59, loss = 0.65452237
Iteration 60, loss = 0.66056786
Iteration 61, loss = 0.63438960
Iteration 62, loss = 0.64907611
Iteration 63, loss = 0.65900901
Iteration 64, loss = 0.64834055
Iteration 65, loss = 0.65440303
Iteration 66, loss = 0.65525761
Iteration 67, loss = 0.64318021
Iteration 68, loss = 0.64153928
Iteration 69, loss = 0.65255160
Iteration 70, loss = 0.65412556
Iteration 71, loss = 0.64251769
Iteration 72, loss = 0.64969218
Iteration 73, loss = 0.65035366
Iteration 74, loss = 0.66366769
Iteration 75, loss = 0.65968214
Iteration 76, loss = 0.64711389
Iteration 77, loss = 0.64460336
Iteration 78, loss = 0.64828876
Iteration 79, loss = 0.64785962
Iteration 80, loss = 0.65153543
Iteration 81, loss = 0.64485561
Iteration 82, loss = 0.65005111
Iteration 83, loss = 0.65457700
Iteration 84, loss = 0.64119924
Iteration 85, loss = 0.64810531
Iteration 86, loss = 0.66351654
Iteration 87, loss = 0.64307641
Iteration 88, loss = 0.65577399
Iteration 89, loss = 0.65878697
Iteration 90, loss = 0.64816250
Iteration 91, loss = 0.65024709
Iteration 92, loss = 0.63924988
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 5/5] END estimator__activation=relu, estimator__hidden_layer_sizes=16;, score=0.630 total time=  46.1s
Iteration 1, loss = 2.22202475
Iteration 2, loss = 0.75649306
Iteration 3, loss = 0.73908516
Iteration 4, loss = 0.72098547
Iteration 5, loss = 0.67759809
Iteration 6, loss = 0.66671196
Iteration 7, loss = 0.68777874
Iteration 8, loss = 0.68624743
Iteration 9, loss = 0.66964435
Iteration 10, loss = 0.66456635
Iteration 11, loss = 0.66640358
Iteration 12, loss = 0.67235893
Iteration 13, loss = 0.67737097
Iteration 14, loss = 0.65675805
Iteration 15, loss = 0.64968999
Iteration 16, loss = 0.67010752
Iteration 17, loss = 0.66919015
Iteration 18, loss = 0.66008016
Iteration 19, loss = 0.66084215
Iteration 20, loss = 0.66167574
Iteration 21, loss = 0.64993934
Iteration 22, loss = 0.64450575
Iteration 23, loss = 0.66908837
Iteration 24, loss = 0.65012907
Iteration 25, loss = 0.64125012
Iteration 26, loss = 0.65472642
Iteration 27, loss = 0.64420289
Iteration 28, loss = 0.64191627

Iteration 29, loss = 0.65326802
Iteration 30, loss = 0.65142842
Iteration 31, loss = 0.64469087
Iteration 32, loss = 0.63329293
Iteration 33, loss = 0.63808971
Iteration 34, loss = 0.63950848
Iteration 35, loss = 0.64085008
Iteration 36, loss = 0.63979684
Iteration 37, loss = 0.64618615
Iteration 38, loss = 0.66044602
Iteration 39, loss = 0.63420347
Iteration 40, loss = 0.64260895
Iteration 41, loss = 0.62766493
Iteration 42, loss = 0.64496906
Iteration 43, loss = 0.64317415
Iteration 44, loss = 0.64353809
Iteration 45, loss = 0.62799345
Iteration 46, loss = 0.64115602
Iteration 47, loss = 0.62581706
Iteration 48, loss = 0.63340771
Iteration 49, loss = 0.62852791
Iteration 50, loss = 0.63148388
Iteration 51, loss = 0.63137889
Iteration 52, loss = 0.62403123
Iteration 53, loss = 0.62501807
Iteration 54, loss = 0.62702887
Iteration 55, loss = 0.62976506
Iteration 56, loss = 0.62167069
Iteration 57, loss = 0.62149589
Iteration 58, loss = 0.62674965
Iteration 59, loss = 0.63009798
Iteration 60, loss = 0.62974279
Iteration 61, loss = 0.61902155
Iteration 62, loss = 0.62184347
Iteration 63, loss = 0.62320109
Iteration 64, loss = 0.62530245
Iteration 65, loss = 0.61583197
Iteration 66, loss = 0.62001818
Iteration 67, loss = 0.62613768
Iteration 68, loss = 0.61994550
Iteration 69, loss = 0.62360456
Iteration 70, loss = 0.61652280
Iteration 71, loss = 0.61897113
Iteration 72, loss = 0.61933753
Iteration 73, loss = 0.62045849
Iteration 74, loss = 0.61598796
Iteration 75, loss = 0.61956043
Iteration 76, loss = 0.62246512
Iteration 77, loss = 0.61592074
Iteration 78, loss = 0.61480579
Iteration 79, loss = 0.61857096
Iteration 80, loss = 0.61732077
Iteration 81, loss = 0.61356643
Iteration 82, loss = 0.61599085
Iteration 83, loss = 0.61587245
Iteration 84, loss = 0.61764556
Iteration 85, loss = 0.61188623
Iteration 86, loss = 0.60969657
Iteration 87, loss = 0.62198176
Iteration 88, loss = 0.61235477
Iteration 89, loss = 0.61035258
Iteration 90, loss = 0.61429883
Iteration 91, loss = 0.60890211
Iteration 92, loss = 0.61245564
Iteration 93, loss = 0.61277508
Iteration 94, loss = 0.60768427
Iteration 95, loss = 0.61108540
Iteration 96, loss = 0.61127411
Iteration 97, loss = 0.61086357
Iteration 98, loss = 0.60288635
Iteration 99, loss = 0.59865889
Iteration 100, loss = 0.59322064
Iteration 101, loss = 0.58923577
Iteration 102, loss = 0.58381210
Iteration 103, loss = 0.56791646
Iteration 104, loss = 0.55845179
Iteration 105, loss = 0.54919714
Iteration 106, loss = 0.53453427
Iteration 107, loss = 0.52229113
Iteration 108, loss = 0.49747072
Iteration 109, loss = 0.45546146
Iteration 110, loss = 0.41304369
Iteration 111, loss = 0.37820857
Iteration 112, loss = 0.34546507
Iteration 113, loss = 0.31176316
Iteration 114, loss = 0.28152832
Iteration 115, loss = 0.25898303
Iteration 116, loss = 0.24305331
Iteration 117, loss = 0.23312381
Iteration 118, loss = 0.23183856
Iteration 119, loss = 0.21795719
Iteration 120, loss = 0.20346613
Iteration 121, loss = 0.20235078
Iteration 122, loss = 0.22911743
Iteration 123, loss = 0.19553250
Iteration 124, loss = 0.19726298
Iteration 125, loss = 0.21391075
Iteration 126, loss = 0.20460911
Iteration 127, loss = 0.18009515
Iteration 128, loss = 0.18959917
Iteration 129, loss = 0.17768152
Iteration 130, loss = 0.19080796
Iteration 131, loss = 0.18121823
Iteration 132, loss = 0.25409427
Iteration 133, loss = 0.18149825
Iteration 134, loss = 0.20100377
Iteration 135, loss = 0.19227518
Iteration 136, loss = 0.23590719
Iteration 137, loss = 0.19776274
Iteration 138, loss = 0.21833200
Iteration 139, loss = 0.22534066
Iteration 140, loss = 0.18071741
Iteration 141, loss = 0.16612450
Iteration 142, loss = 0.23095550
Iteration 143, loss = 0.19107831
Iteration 144, loss = 0.26666356
Iteration 145, loss = 0.20624767
Iteration 146, loss = 0.22048545
Iteration 147, loss = 0.21650736
Iteration 148, loss = 0.16981749
Iteration 149, loss = 0.19564429
Iteration 150, loss = 0.22475715
Iteration 151, loss = 0.25594764
Iteration 152, loss = 0.18387856
Iteration 153, loss = 0.18734351
Iteration 154, loss = 0.25050473
Iteration 155, loss = 0.22331505
Iteration 156, loss = 0.21432568
Iteration 157, loss = 0.19943260
Iteration 158, loss = 0.25407226
Iteration 159, loss = 0.19249515
Iteration 160, loss = 0.20346996
Iteration 161, loss = 0.16393189
Iteration 162, loss = 0.18877782
Iteration 163, loss = 0.19045451
Iteration 164, loss = 0.26866473
Iteration 165, loss = 0.20468846
Iteration 166, loss = 0.20694776
Iteration 167, loss = 0.21249926
Iteration 168, loss = 0.17580815
Iteration 169, loss = 0.33341591
Iteration 170, loss = 0.25728022
Iteration 171, loss = 0.24347688
Iteration 172, loss = 0.24903774
Iteration 173, loss = 0.23254777
Iteration 174, loss = 0.21471081
Iteration 175, loss = 0.23688545
Iteration 176, loss = 0.25505891
Iteration 177, loss = 0.25290426
Iteration 178, loss = 0.25718153
Iteration 179, loss = 0.21525720
Iteration 180, loss = 0.29503641
Iteration 181, loss = 0.24612692
Iteration 182, loss = 0.19518853
Iteration 183, loss = 0.27948525
Iteration 184, loss = 0.22233430
Iteration 185, loss = 0.20368781
Iteration 186, loss = 0.19608209
Iteration 187, loss = 0.19166208
Iteration 188, loss = 0.19312815
Iteration 189, loss = 0.16975867
Iteration 190, loss = 0.17056455
Iteration 191, loss = 0.17349504
Iteration 192, loss = 0.17492388
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 1/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(16, 16);, score=0.947 total time= 1.1min
Iteration 1, loss = 2.63950175
Iteration 2, loss = 1.04967231
Iteration 3, loss = 0.76397293
Iteration 4, loss = 0.87368566
Iteration 5, loss = 0.82172342
Iteration 6, loss = 0.79285970
Iteration 7, loss = 0.73526030
Iteration 8, loss = 0.79041420
Iteration 9, loss = 0.71483931
Iteration 10, loss = 0.76832795
Iteration 11, loss = 0.79430552
Iteration 12, loss = 0.78374441
Iteration 13, loss = 0.73299246
Iteration 14, loss = 0.77153147
Iteration 15, loss = 0.71261381
Iteration 16, loss = 0.77723490
Iteration 17, loss = 0.76077527
Iteration 18, loss = 0.74032378
Iteration 19, loss = 0.67123350
Iteration 20, loss = 0.75240649
Iteration 21, loss = 0.72725162
Iteration 22, loss = 0.74916470
Iteration 23, loss = 0.73915813
Iteration 24, loss = 0.69322513
Iteration 25, loss = 0.74930147
Iteration 26, loss = 0.70773223
Iteration 27, loss = 0.77396923
Iteration 28, loss = 0.69914962
Iteration 29, loss = 0.72916558
Iteration 30, loss = 0.66293421
Iteration 31, loss = 0.71157914
Iteration 32, loss = 0.71951598
Iteration 33, loss = 0.70024323
Iteration 34, loss = 0.68124061
Iteration 35, loss = 0.68192889
Iteration 36, loss = 0.75702654
Iteration 37, loss = 0.66654416
Iteration 38, loss = 0.71006927
Iteration 39, loss = 0.72513169
Iteration 40, loss = 0.67853962
Iteration 41, loss = 0.68649997
Iteration 42, loss = 0.71597486
Iteration 43, loss = 0.68133471
Iteration 44, loss = 0.68802098
Iteration 45, loss = 0.66965432
Iteration 46, loss = 0.73012650
Iteration 47, loss = 0.67886722
Iteration 48, loss = 0.68142675
Iteration 49, loss = 0.69347972
Iteration 50, loss = 0.66399446
Iteration 51, loss = 0.70048925
Iteration 52, loss = 0.67647428
Iteration 53, loss = 0.67554742
Iteration 54, loss = 0.66382543
Iteration 55, loss = 0.66408377
Iteration 56, loss = 0.69020650
Iteration 57, loss = 0.66938379
Iteration 58, loss = 0.66852714
Iteration 59, loss = 0.66364616
Iteration 60, loss = 0.66962890
Iteration 61, loss = 0.65783335
Iteration 62, loss = 0.66929520
Iteration 63, loss = 0.66377627
Iteration 64, loss = 0.66180358
Iteration 65, loss = 0.66583189
Iteration 66, loss = 0.64873994
Iteration 67, loss = 0.66775239
Iteration 68, loss = 0.65297017
Iteration 69, loss = 0.66315107
Iteration 70, loss = 0.65247102
Iteration 71, loss = 0.65118165
Iteration 72, loss = 0.65748821
Iteration 73, loss = 0.64638924
Iteration 74, loss = 0.66616713
Iteration 75, loss = 0.65973982
Iteration 76, loss = 0.64453962
Iteration 77, loss = 0.64650030
Iteration 78, loss = 0.65020089
Iteration 79, loss = 0.64032932
Iteration 80, loss = 0.65952944
Iteration 81, loss = 0.65216797
Iteration 82, loss = 0.65309235
Iteration 83, loss = 0.64995501
Iteration 84, loss = 0.64019771

Iteration 85, loss = 0.63491797
Iteration 86, loss = 0.63434264
Iteration 87, loss = 0.62333412
Iteration 88, loss = 0.64992054
Iteration 89, loss = 0.64165430
Iteration 90, loss = 0.62668896
Iteration 91, loss = 0.64379266
Iteration 92, loss = 0.63711945
Iteration 93, loss = 0.63676935
Iteration 94, loss = 0.63264967
Iteration 95, loss = 0.63330053
Iteration 96, loss = 0.62698594
Iteration 97, loss = 0.62894989
Iteration 98, loss = 0.63532709
Iteration 99, loss = 0.61959586
Iteration 100, loss = 0.63389970
Iteration 101, loss = 0.63409912
Iteration 102, loss = 0.62286011
Iteration 103, loss = 0.62361064
Iteration 104, loss = 0.61985828
Iteration 105, loss = 0.63029901
Iteration 106, loss = 0.62231056
Iteration 107, loss = 0.61834772
Iteration 108, loss = 0.62341697
Iteration 109, loss = 0.62203124
Iteration 110, loss = 0.62147915
Iteration 111, loss = 0.61829226
Iteration 112, loss = 0.62538249
Iteration 113, loss = 0.62363968
Iteration 114, loss = 0.61457890
Iteration 115, loss = 0.62342572
Iteration 116, loss = 0.61828297
Iteration 117, loss = 0.61429681
Iteration 118, loss = 0.62487739
Iteration 119, loss = 0.61492392
Iteration 120, loss = 0.61772920
Iteration 121, loss = 0.61625016
Iteration 122, loss = 0.61210612
Iteration 123, loss = 0.61396357
Iteration 124, loss = 0.61320384
Iteration 125, loss = 0.61543767
Iteration 126, loss = 0.61117473
Iteration 127, loss = 0.61040478
Iteration 128, loss = 0.61315844
Iteration 129, loss = 0.61325272
Iteration 130, loss = 0.60725267
Iteration 131, loss = 0.60964503
Iteration 132, loss = 0.61223473
Iteration 133, loss = 0.60861227
Iteration 134, loss = 0.61081459
Iteration 135, loss = 0.60751608
Iteration 136, loss = 0.61245788
Iteration 137, loss = 0.61294326
Iteration 138, loss = 0.60482669
Iteration 139, loss = 0.60145140
Iteration 140, loss = 0.60346920
Iteration 141, loss = 0.59875322
Iteration 142, loss = 0.60545206
Iteration 143, loss = 0.59995161
Iteration 144, loss = 0.59903304
Iteration 145, loss = 0.59950164
Iteration 146, loss = 0.59265765
Iteration 147, loss = 0.59681523
Iteration 148, loss = 0.59777483
Iteration 149, loss = 0.59739641
Iteration 150, loss = 0.59146389
Iteration 151, loss = 0.59578612
Iteration 152, loss = 0.59764788
Iteration 153, loss = 0.59194461
Iteration 154, loss = 0.59214537
Iteration 155, loss = 0.59173227
Iteration 156, loss = 0.59496587
Iteration 157, loss = 0.59222744
Iteration 158, loss = 0.59027282
Iteration 159, loss = 0.59034550
Iteration 160, loss = 0.58833759
Iteration 161, loss = 0.59030755
Iteration 162, loss = 0.58927391
Iteration 163, loss = 0.59018272
Iteration 164, loss = 0.58719370
Iteration 165, loss = 0.59386048
Iteration 166, loss = 0.60117928
Iteration 167, loss = 0.60046304
Iteration 168, loss = 0.59891586
Iteration 169, loss = 0.59991449
Iteration 170, loss = 0.59959280
Iteration 171, loss = 0.59779162
Iteration 172, loss = 0.60050606
Iteration 173, loss = 0.59875178
Iteration 174, loss = 0.59914829
Iteration 175, loss = 0.59896325
Iteration 176, loss = 0.60068620
Iteration 177, loss = 0.59723018
Iteration 178, loss = 0.59778805
Iteration 179, loss = 0.59412678
Iteration 180, loss = 0.58769913
Iteration 181, loss = 0.56972033
Iteration 182, loss = 0.52664001
Iteration 183, loss = 0.44307663
Iteration 184, loss = 0.35239344
Iteration 185, loss = 0.31621519
Iteration 186, loss = 0.30467616
Iteration 187, loss = 0.30359889
Iteration 188, loss = 0.29583403
Iteration 189, loss = 0.29604938
Iteration 190, loss = 0.28763536
Iteration 191, loss = 0.28413548
Iteration 192, loss = 0.28458745
Iteration 193, loss = 0.27951391
Iteration 194, loss = 0.27751065
Iteration 195, loss = 0.29186445
Iteration 196, loss = 0.29408410
Iteration 197, loss = 0.29078718
Iteration 198, loss = 0.29206794
Iteration 199, loss = 0.29522939
Iteration 200, loss = 0.29082666
Iteration 201, loss = 0.30409092
Iteration 202, loss = 0.29091560
Iteration 203, loss = 0.29308135
Iteration 204, loss = 0.28924802
Iteration 205, loss = 0.27012711
Iteration 206, loss = 0.28690031
Iteration 207, loss = 0.26902908
Iteration 208, loss = 0.26184428
Iteration 209, loss = 0.25706966
Iteration 210, loss = 0.26685410
Iteration 211, loss = 0.25211811
Iteration 212, loss = 0.24920147
Iteration 213, loss = 0.26571993
Iteration 214, loss = 0.25041498
Iteration 215, loss = 0.24460715
Iteration 216, loss = 0.25083209
Iteration 217, loss = 0.24772170
Iteration 218, loss = 0.24800574
Iteration 219, loss = 0.23560029
Iteration 220, loss = 0.24195798
Iteration 221, loss = 0.23783205
Iteration 222, loss = 0.24106960
Iteration 223, loss = 0.22893429
Iteration 224, loss = 0.29409776
Iteration 225, loss = 0.28845026
Iteration 226, loss = 0.28607184
Iteration 227, loss = 0.28680638
Iteration 228, loss = 0.28365426
Iteration 229, loss = 0.28407686
Iteration 230, loss = 0.29159028
Iteration 231, loss = 0.27810787
Iteration 232, loss = 0.27998184
Iteration 233, loss = 0.28482810
Iteration 234, loss = 0.28408055
Iteration 235, loss = 0.27910554
Iteration 236, loss = 0.28202105
Iteration 237, loss = 0.28457879
Iteration 238, loss = 0.27688782
Iteration 239, loss = 0.28151525
Iteration 240, loss = 0.27699200
Iteration 241, loss = 0.28000206
Iteration 242, loss = 0.28634567
Iteration 243, loss = 0.28121385
Iteration 244, loss = 0.27421363
Iteration 245, loss = 0.27891310
Iteration 246, loss = 0.27933591
Iteration 247, loss = 0.27773173
Iteration 248, loss = 0.28683007
Iteration 249, loss = 0.27783416
Iteration 250, loss = 0.27879355
Iteration 251, loss = 0.27348585
Iteration 252, loss = 0.28594597
Iteration 253, loss = 0.27392960
Iteration 254, loss = 0.27447615
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 2/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(16, 16);, score=0.911 total time= 1.3min
Iteration 1, loss = 2.86641609
Iteration 2, loss = 1.03105630
Iteration 3, loss = 0.83904468
Iteration 4, loss = 0.80353943
Iteration 5, loss = 0.88657203
Iteration 6, loss = 0.73612202
Iteration 7, loss = 0.72295897
Iteration 8, loss = 0.75381074
Iteration 9, loss = 0.71676548
Iteration 10, loss = 0.76087912
Iteration 11, loss = 0.72174564
Iteration 12, loss = 0.71177080
Iteration 13, loss = 0.72424701
Iteration 14, loss = 0.67602934
Iteration 15, loss = 0.71685893
Iteration 16, loss = 0.69510994
Iteration 17, loss = 0.68481138
Iteration 18, loss = 0.71841886
Iteration 19, loss = 0.68573266
Iteration 20, loss = 0.71340010
Iteration 21, loss = 0.70724661
Iteration 22, loss = 0.69565335
Iteration 23, loss = 0.67881474
Iteration 24, loss = 0.68686005
Iteration 25, loss = 0.67159308
Iteration 26, loss = 0.68762700
Iteration 27, loss = 0.68066919
Iteration 28, loss = 0.68844325
Iteration 29, loss = 0.68842816
Iteration 30, loss = 0.67665422
Iteration 31, loss = 0.67688297
Iteration 32, loss = 0.65422236
Iteration 33, loss = 0.70910256
Iteration 34, loss = 0.68163058
Iteration 35, loss = 0.66254019
Iteration 36, loss = 0.67076490
Iteration 37, loss = 0.68665936
Iteration 38, loss = 0.66260813
Iteration 39, loss = 0.67159961
Iteration 40, loss = 0.68921758
Iteration 41, loss = 0.66905856
Iteration 42, loss = 0.66850693
Iteration 43, loss = 0.67257188
Iteration 44, loss = 0.66195349
Iteration 45, loss = 0.66850670
Iteration 46, loss = 0.66842717
Iteration 47, loss = 0.66300526
Iteration 48, loss = 0.65516408
Iteration 49, loss = 0.66520521
Iteration 50, loss = 0.67395313
Iteration 51, loss = 0.66215406
Iteration 52, loss = 0.67453411
Iteration 53, loss = 0.66886578
Iteration 54, loss = 0.65216041
Iteration 55, loss = 0.65716486
Iteration 56, loss = 0.65901412
Iteration 57, loss = 0.66047700
Iteration 58, loss = 0.65631168
Iteration 59, loss = 0.64744447
Iteration 60, loss = 0.66415036
Iteration 61, loss = 0.66206447
Iteration 62, loss = 0.65171196
Iteration 63, loss = 0.65659988
Iteration 64, loss = 0.65574939
Iteration 65, loss = 0.65290607
Iteration 66, loss = 0.66269140
Iteration 67, loss = 0.64611715
Iteration 68, loss = 0.64750824
Iteration 69, loss = 0.65525285
Iteration 70, loss = 0.63834483
Iteration 71, loss = 0.64883281
Iteration 72, loss = 0.63504896
Iteration 73, loss = 0.62914347
Iteration 74, loss = 0.62963015
Iteration 75, loss = 0.63001002
Iteration 76, loss = 0.61597446

Iteration 77, loss = 0.60005118
Iteration 78, loss = 0.61614229
Iteration 79, loss = 0.58351851
Iteration 80, loss = 0.57588870
Iteration 81, loss = 0.55286890
Iteration 82, loss = 0.54939582
Iteration 83, loss = 0.52193800
Iteration 84, loss = 0.50238603
Iteration 85, loss = 0.49668227
Iteration 86, loss = 0.46907560
Iteration 87, loss = 0.42821974
Iteration 88, loss = 0.40405894
Iteration 89, loss = 0.38148563
Iteration 90, loss = 0.35417718
Iteration 91, loss = 0.34975027
Iteration 92, loss = 0.32936237
Iteration 93, loss = 0.32601906
Iteration 94, loss = 0.31340207
Iteration 95, loss = 0.30656582
Iteration 96, loss = 0.32123673
Iteration 97, loss = 0.29849844
Iteration 98, loss = 0.29765556
Iteration 99, loss = 0.29973530
Iteration 100, loss = 0.28866614
Iteration 101, loss = 0.30699841
Iteration 102, loss = 0.28479425
Iteration 103, loss = 0.30498839
Iteration 104, loss = 0.28549116
Iteration 105, loss = 0.31701558
Iteration 106, loss = 0.28886405
Iteration 107, loss = 0.27941726
Iteration 108, loss = 0.30084161
Iteration 109, loss = 0.27936891
Iteration 110, loss = 0.28432620
Iteration 111, loss = 0.27777935
Iteration 112, loss = 0.28006345
Iteration 113, loss = 0.28046550
Iteration 114, loss = 0.28758427
Iteration 115, loss = 0.27861069
Iteration 116, loss = 0.27481238
Iteration 117, loss = 0.27755291
Iteration 118, loss = 0.28023186
Iteration 119, loss = 0.26985898
Iteration 120, loss = 0.27228647
Iteration 121, loss = 0.27475566
Iteration 122, loss = 0.27067184
Iteration 123, loss = 0.27513420
Iteration 124, loss = 0.26789223
Iteration 125, loss = 0.27756866
Iteration 126, loss = 0.26898935
Iteration 127, loss = 0.26324497
Iteration 128, loss = 0.26435872
Iteration 129, loss = 0.26102490
Iteration 130, loss = 0.25693132
Iteration 131, loss = 0.28520290
Iteration 132, loss = 0.26137686
Iteration 133, loss = 0.26741434
Iteration 134, loss = 0.24969932
Iteration 135, loss = 0.25047775
Iteration 136, loss = 0.26216784
Iteration 137, loss = 0.26054520
Iteration 138, loss = 0.26022044
Iteration 139, loss = 0.24949184
Iteration 140, loss = 0.25054841
Iteration 141, loss = 0.25692471
Iteration 142, loss = 0.25440778
Iteration 143, loss = 0.26015141
Iteration 144, loss = 0.25205595
Iteration 145, loss = 0.24848189
Iteration 146, loss = 0.25637448
Iteration 147, loss = 0.25247964
Iteration 148, loss = 0.25319940
Iteration 149, loss = 0.24610609
Iteration 150, loss = 0.25480586
Iteration 151, loss = 0.24841249
Iteration 152, loss = 0.24722054
Iteration 153, loss = 0.24084204
Iteration 154, loss = 0.25644043
Iteration 155, loss = 0.24570399
Iteration 156, loss = 0.25364186
Iteration 157, loss = 0.24059670
Iteration 158, loss = 0.24646369
Iteration 159, loss = 0.24489008
Iteration 160, loss = 0.23949814
Iteration 161, loss = 0.23735291
Iteration 162, loss = 0.23666935
Iteration 163, loss = 0.23596482
Iteration 164, loss = 0.23311139
Iteration 165, loss = 0.23831700
Iteration 166, loss = 0.25152063
Iteration 167, loss = 0.25395191
Iteration 168, loss = 0.24045642
Iteration 169, loss = 0.24345560
Iteration 170, loss = 0.23565299
Iteration 171, loss = 0.23903853
Iteration 172, loss = 0.23224760
Iteration 173, loss = 0.23694662
Iteration 174, loss = 0.23504736
Iteration 175, loss = 0.22477332
Iteration 176, loss = 0.24678951
Iteration 177, loss = 0.22941528
Iteration 178, loss = 0.22679935
Iteration 179, loss = 0.22892414
Iteration 180, loss = 0.22745945
Iteration 181, loss = 0.22946086
Iteration 182, loss = 0.23186164
Iteration 183, loss = 0.23030688
Iteration 184, loss = 0.21880560
Iteration 185, loss = 0.22929976
Iteration 186, loss = 0.22440883
Iteration 187, loss = 0.21741142
Iteration 188, loss = 0.21495991
Iteration 189, loss = 0.22302473
Iteration 190, loss = 0.21607571
Iteration 191, loss = 0.21351186
Iteration 192, loss = 0.21435762
Iteration 193, loss = 0.21799441
Iteration 194, loss = 0.20697061
Iteration 195, loss = 0.20089189
Iteration 196, loss = 0.20228860
Iteration 197, loss = 0.19409328
Iteration 198, loss = 0.19680550
Iteration 199, loss = 0.20970253
Iteration 200, loss = 0.18695159
Iteration 201, loss = 0.18375126
Iteration 202, loss = 0.18472368
Iteration 203, loss = 0.17969552
Iteration 204, loss = 0.17080354
Iteration 205, loss = 0.19427984
Iteration 206, loss = 0.21338268
Iteration 207, loss = 0.26476322
Iteration 208, loss = 0.20797442
Iteration 209, loss = 0.17479100
Iteration 210, loss = 0.16624831
Iteration 211, loss = 0.23604474
Iteration 212, loss = 0.27352147
Iteration 213, loss = 0.25429211
Iteration 214, loss = 0.25739862
Iteration 215, loss = 0.24987197
Iteration 216, loss = 0.25011375
Iteration 217, loss = 0.25134929
Iteration 218, loss = 0.24430704
Iteration 219, loss = 0.25384529
Iteration 220, loss = 0.25175898
Iteration 221, loss = 0.25226872
Iteration 222, loss = 0.24278106
Iteration 223, loss = 0.24642855
Iteration 224, loss = 0.24375743
Iteration 225, loss = 0.25147473
Iteration 226, loss = 0.25329127
Iteration 227, loss = 0.23426379
Iteration 228, loss = 0.24403187
Iteration 229, loss = 0.24366504
Iteration 230, loss = 0.24410176
Iteration 231, loss = 0.24288687
Iteration 232, loss = 0.24273812
Iteration 233, loss = 0.24170468
Iteration 234, loss = 0.23798140
Iteration 235, loss = 0.24193481
Iteration 236, loss = 0.24002866
Iteration 237, loss = 0.23693117
Iteration 238, loss = 0.23939979
Iteration 239, loss = 0.24663851
Iteration 240, loss = 0.23986233
Iteration 241, loss = 0.24491894
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 3/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(16, 16);, score=0.949 total time= 1.3min
Iteration 1, loss = 2.10029893
Iteration 2, loss = 0.98745421
Iteration 3, loss = 0.85717687
Iteration 4, loss = 0.76890959
Iteration 5, loss = 0.77764474
Iteration 6, loss = 0.73597909
Iteration 7, loss = 0.73048450
Iteration 8, loss = 0.73785916
Iteration 9, loss = 0.70331186
Iteration 10, loss = 0.73984909
Iteration 11, loss = 0.75866801
Iteration 12, loss = 0.69720796
Iteration 13, loss = 0.71190851
Iteration 14, loss = 0.74335560
Iteration 15, loss = 0.72458199
Iteration 16, loss = 0.72322173
Iteration 17, loss = 0.71514649
Iteration 18, loss = 0.72006076
Iteration 19, loss = 0.71509956
Iteration 20, loss = 0.69726571
Iteration 21, loss = 0.69666331
Iteration 22, loss = 0.73619094
Iteration 23, loss = 0.69697149
Iteration 24, loss = 0.71304428
Iteration 25, loss = 0.69796244
Iteration 26, loss = 0.67431613
Iteration 27, loss = 0.72049775
Iteration 28, loss = 0.68868296
Iteration 29, loss = 0.71164174
Iteration 30, loss = 0.68982323
Iteration 31, loss = 0.68211641
Iteration 32, loss = 0.71189980
Iteration 33, loss = 0.68285478
Iteration 34, loss = 0.70065634
Iteration 35, loss = 0.70279776
Iteration 36, loss = 0.68100615
Iteration 37, loss = 0.68883470
Iteration 38, loss = 0.67318290
Iteration 39, loss = 0.68440958
Iteration 40, loss = 0.68558207
Iteration 41, loss = 0.67811317
Iteration 42, loss = 0.69990764
Iteration 43, loss = 0.68282630
Iteration 44, loss = 0.67209660
Iteration 45, loss = 0.68858842
Iteration 46, loss = 0.67472871
Iteration 47, loss = 0.68465674
Iteration 48, loss = 0.67546129
Iteration 49, loss = 0.66769635
Iteration 50, loss = 0.66490020
Iteration 51, loss = 0.67740166
Iteration 52, loss = 0.66686733
Iteration 53, loss = 0.68009721
Iteration 54, loss = 0.66312767
Iteration 55, loss = 0.67558020
Iteration 56, loss = 0.64644965
Iteration 57, loss = 0.66264394
Iteration 58, loss = 0.66260708
Iteration 59, loss = 0.66327954
Iteration 60, loss = 0.65914697
Iteration 61, loss = 0.66614585
Iteration 62, loss = 0.67162246
Iteration 63, loss = 0.67357591
Iteration 64, loss = 0.66666795
Iteration 65, loss = 0.65118834
Iteration 66, loss = 0.66086000
Iteration 67, loss = 0.65574937
Iteration 68, loss = 0.65914403
Iteration 69, loss = 0.65754940
Iteration 70, loss = 0.67460309
Iteration 71, loss = 0.65061433
Iteration 72, loss = 0.64912899
Iteration 73, loss = 0.64408318
Iteration 74, loss = 0.66374265
Iteration 75, loss = 0.64782176
Iteration 76, loss = 0.65253493
Iteration 77, loss = 0.64938360
Iteration 78, loss = 0.64445180
Iteration 79, loss = 0.66072722
Iteration 80, loss = 0.65502647
Iteration 81, loss = 0.65175519
Iteration 82, loss = 0.64970316

Iteration 83, loss = 0.64811101
Iteration 84, loss = 0.64518268
Iteration 85, loss = 0.64767777
Iteration 86, loss = 0.64446698
Iteration 87, loss = 0.64321671
Iteration 88, loss = 0.65074384
Iteration 89, loss = 0.64621388
Iteration 90, loss = 0.64092376
Iteration 91, loss = 0.65001008
Iteration 92, loss = 0.64708497
Iteration 93, loss = 0.63758419
Iteration 94, loss = 0.65130247
Iteration 95, loss = 0.64834191
Iteration 96, loss = 0.64314666
Iteration 97, loss = 0.64715385
Iteration 98, loss = 0.64952853
Iteration 99, loss = 0.63286707
Iteration 100, loss = 0.64037834
Iteration 101, loss = 0.64497775
Iteration 102, loss = 0.63924371
Iteration 103, loss = 0.65152268
Iteration 104, loss = 0.64385929
Iteration 105, loss = 0.63644776
Iteration 106, loss = 0.63880288
Iteration 107, loss = 0.64682157
Iteration 108, loss = 0.65766960
Iteration 109, loss = 0.63626134
Iteration 110, loss = 0.63629957
Iteration 111, loss = 0.63558533
Iteration 112, loss = 0.63493337
Iteration 113, loss = 0.63846835
Iteration 114, loss = 0.63498191
Iteration 115, loss = 0.63657683
Iteration 116, loss = 0.64132864
Iteration 117, loss = 0.63560819
Iteration 118, loss = 0.64601205
Iteration 119, loss = 0.63358534
Iteration 120, loss = 0.63993755
Iteration 121, loss = 0.64243352
Iteration 122, loss = 0.62881864
Iteration 123, loss = 0.62438527
Iteration 124, loss = 0.63659351
Iteration 125, loss = 0.63954851
Iteration 126, loss = 0.62381078
Iteration 127, loss = 0.63316485
Iteration 128, loss = 0.63235620
Iteration 129, loss = 0.63250267
Iteration 130, loss = 0.62703028
Iteration 131, loss = 0.62070057
Iteration 132, loss = 0.62959392
Iteration 133, loss = 0.62971624
Iteration 134, loss = 0.62333072
Iteration 135, loss = 0.61814185
Iteration 136, loss = 0.63210956
Iteration 137, loss = 0.62327858
Iteration 138, loss = 0.62657195
Iteration 139, loss = 0.62293735
Iteration 140, loss = 0.62710718
Iteration 141, loss = 0.62363461
Iteration 142, loss = 0.62754534
Iteration 143, loss = 0.62399506
Iteration 144, loss = 0.61982212
Iteration 145, loss = 0.63067163
Iteration 146, loss = 0.62477489
Iteration 147, loss = 0.61764859
Iteration 148, loss = 0.61509427
Iteration 149, loss = 0.62417687
Iteration 150, loss = 0.62204098
Iteration 151, loss = 0.61905108
Iteration 152, loss = 0.61866219
Iteration 153, loss = 0.61622850
Iteration 154, loss = 0.61740455
Iteration 155, loss = 0.62396841
Iteration 156, loss = 0.61825702
Iteration 157, loss = 0.61709692
Iteration 158, loss = 0.61473761
Iteration 159, loss = 0.61984322
Iteration 160, loss = 0.61601620
Iteration 161, loss = 0.61503469
Iteration 162, loss = 0.61645127
Iteration 163, loss = 0.61645715
Iteration 164, loss = 0.61743139
Iteration 165, loss = 0.61666687
Iteration 166, loss = 0.61295012
Iteration 167, loss = 0.61121777
Iteration 168, loss = 0.61497467
Iteration 169, loss = 0.61289481
Iteration 170, loss = 0.61305289
Iteration 171, loss = 0.61217123
Iteration 172, loss = 0.61426677
Iteration 173, loss = 0.60849975
Iteration 174, loss = 0.61349239
Iteration 175, loss = 0.61238114
Iteration 176, loss = 0.61212059
Iteration 177, loss = 0.61037028
Iteration 178, loss = 0.61194266
Iteration 179, loss = 0.60717944
Iteration 180, loss = 0.60881423
Iteration 181, loss = 0.60871176
Iteration 182, loss = 0.60924741
Iteration 183, loss = 0.60995058
Iteration 184, loss = 0.61164590
Iteration 185, loss = 0.61105824
Iteration 186, loss = 0.61092655
Iteration 187, loss = 0.60954180
Iteration 188, loss = 0.60690931
Iteration 189, loss = 0.61168339
Iteration 190, loss = 0.61343802
Iteration 191, loss = 0.60854672
Iteration 192, loss = 0.61177670
Iteration 193, loss = 0.61036484
Iteration 194, loss = 0.60892004
Iteration 195, loss = 0.60758866
Iteration 196, loss = 0.60780495
Iteration 197, loss = 0.60848116
Iteration 198, loss = 0.60686455
Iteration 199, loss = 0.60798768
Iteration 200, loss = 0.60888492
Iteration 201, loss = 0.60542671
Iteration 202, loss = 0.61184545
Iteration 203, loss = 0.60601194
Iteration 204, loss = 0.60525701
Iteration 205, loss = 0.60534302
Iteration 206, loss = 0.60734528
Iteration 207, loss = 0.60731887
Iteration 208, loss = 0.60448716
Iteration 209, loss = 0.60741564
Iteration 210, loss = 0.60853268
Iteration 211, loss = 0.60645629
Iteration 212, loss = 0.60234819
Iteration 213, loss = 0.60514111
Iteration 214, loss = 0.60548699
Iteration 215, loss = 0.60625123
Iteration 216, loss = 0.60687785
Iteration 217, loss = 0.60459949
Iteration 218, loss = 0.60615760
Iteration 219, loss = 0.60408565
Iteration 220, loss = 0.60500716
Iteration 221, loss = 0.60395730
Iteration 222, loss = 0.60390202
Iteration 223, loss = 0.60448770
Iteration 224, loss = 0.60371577
Iteration 225, loss = 0.60485844
Iteration 226, loss = 0.60299401
Iteration 227, loss = 0.60606009
Iteration 228, loss = 0.60253606
Iteration 229, loss = 0.60399727
Iteration 230, loss = 0.60338146
Iteration 231, loss = 0.60612077
Iteration 232, loss = 0.60123707
Iteration 233, loss = 0.60255737
Iteration 234, loss = 0.60461738
Iteration 235, loss = 0.60194738
Iteration 236, loss = 0.60222276
Iteration 237, loss = 0.60406870
Iteration 238, loss = 0.60348710
Iteration 239, loss = 0.60347369
Iteration 240, loss = 0.60302832
Iteration 241, loss = 0.60181799
Iteration 242, loss = 0.60325612
Iteration 243, loss = 0.60076969
Iteration 244, loss = 0.60180252
Iteration 245, loss = 0.60291300
Iteration 246, loss = 0.60166874
Iteration 247, loss = 0.60177663
Iteration 248, loss = 0.60168074
Iteration 249, loss = 0.60196986
Iteration 250, loss = 0.60171204
Iteration 251, loss = 0.60258057
Iteration 252, loss = 0.60220040
Iteration 253, loss = 0.60170766
Iteration 254, loss = 0.60114026
Iteration 255, loss = 0.60299626
Iteration 256, loss = 0.59972537
Iteration 257, loss = 0.60266507
Iteration 258, loss = 0.60253731
Iteration 259, loss = 0.60167292
Iteration 260, loss = 0.60197505
Iteration 261, loss = 0.60218124
Iteration 262, loss = 0.60232671
Iteration 263, loss = 0.60043308
Iteration 264, loss = 0.60145716
Iteration 265, loss = 0.60167159
Iteration 266, loss = 0.60025494
Iteration 267, loss = 0.60087772
Iteration 268, loss = 0.60222798
Iteration 269, loss = 0.60142150
Iteration 270, loss = 0.60065006
Iteration 271, loss = 0.60083408
Iteration 272, loss = 0.60059148
Iteration 273, loss = 0.59972478
Iteration 274, loss = 0.60117945
Iteration 275, loss = 0.60050271
Iteration 276, loss = 0.60129785
Iteration 277, loss = 0.60061133
Iteration 278, loss = 0.60082678
Iteration 279, loss = 0.60133593
Iteration 280, loss = 0.60032137
Iteration 281, loss = 0.60049611
Iteration 282, loss = 0.60020064
Iteration 283, loss = 0.60137810
Iteration 284, loss = 0.60061463
Iteration 285, loss = 0.59978879
Iteration 286, loss = 0.60085404
Iteration 287, loss = 0.59953435
Iteration 288, loss = 0.60075069
Iteration 289, loss = 0.60075111
Iteration 290, loss = 0.60057852
Iteration 291, loss = 0.59942232
Iteration 292, loss = 0.60078913
Iteration 293, loss = 0.59987855
Iteration 294, loss = 0.59952063
Iteration 295, loss = 0.60105009
Iteration 296, loss = 0.59990829
Iteration 297, loss = 0.59908594
Iteration 298, loss = 0.60150436
Iteration 299, loss = 0.60008010
Iteration 300, loss = 0.60036466
Iteration 301, loss = 0.60044650
Iteration 302, loss = 0.59969449
Iteration 303, loss = 0.60050341
Iteration 304, loss = 0.59978350
Iteration 305, loss = 0.60003099
Iteration 306, loss = 0.59945649
Iteration 307, loss = 0.60043539
Iteration 308, loss = 0.60044661
Iteration 309, loss = 0.60005601
Iteration 310, loss = 0.59998462
Iteration 311, loss = 0.60039012
Iteration 312, loss = 0.60068825
Iteration 313, loss = 0.59984906
Iteration 314, loss = 0.60016649
Iteration 315, loss = 0.59992015
Iteration 316, loss = 0.59936118
Iteration 317, loss = 0.60092094
Iteration 318, loss = 0.59919024
Iteration 319, loss = 0.59871866
Iteration 320, loss = 0.59915254
Iteration 321, loss = 0.59960760
Iteration 322, loss = 0.59994989
Iteration 323, loss = 0.60012817
Iteration 324, loss = 0.60049484
Iteration 325, loss = 0.59902869
Iteration 326, loss = 0.59952398
Iteration 327, loss = 0.59890650
Iteration 328, loss = 0.59951282
Iteration 329, loss = 0.60020267
Iteration 330, loss = 0.60001447
Iteration 331, loss = 0.60029915
Iteration 332, loss = 0.59881172

Iteration 333, loss = 0.59946720
Iteration 334, loss = 0.59938993
Iteration 335, loss = 0.60007093
Iteration 336, loss = 0.60119530
Iteration 337, loss = 0.59969226
Iteration 338, loss = 0.59873491
Iteration 339, loss = 0.59999477
Iteration 340, loss = 0.59972092
Iteration 341, loss = 0.59856234
Iteration 342, loss = 0.59923456
Iteration 343, loss = 0.60079618
Iteration 344, loss = 0.59884008
Iteration 345, loss = 0.60010852
Iteration 346, loss = 0.60016850
Iteration 347, loss = 0.59951684
Iteration 348, loss = 0.59894836
Iteration 349, loss = 0.59904849
Iteration 350, loss = 0.59908679
Iteration 351, loss = 0.59997742
Iteration 352, loss = 0.59899250
Iteration 353, loss = 0.59963208
Iteration 354, loss = 0.59920616
Iteration 355, loss = 0.59856776
Iteration 356, loss = 0.59890818
Iteration 357, loss = 0.59873126
Iteration 358, loss = 0.60022642
Iteration 359, loss = 0.59919847
Iteration 360, loss = 0.59926945
Iteration 361, loss = 0.59828093
Iteration 362, loss = 0.60062874
Iteration 363, loss = 0.60063426
Iteration 364, loss = 0.60048610
Iteration 365, loss = 0.59921397
Iteration 366, loss = 0.59821577
Iteration 367, loss = 0.59862318
Iteration 368, loss = 0.59864963
Iteration 369, loss = 0.59913681
Iteration 370, loss = 0.59897604
Iteration 371, loss = 0.59910183
Iteration 372, loss = 0.59846384
Iteration 373, loss = 0.59899067
Iteration 374, loss = 0.59931400
Iteration 375, loss = 0.59809395
Iteration 376, loss = 0.59752613
Iteration 377, loss = 0.59749446
Iteration 378, loss = 0.59737457
Iteration 379, loss = 0.59634634
Iteration 380, loss = 0.59675239
Iteration 381, loss = 0.59623307
Iteration 382, loss = 0.59529327
Iteration 383, loss = 0.59541934
Iteration 384, loss = 0.59392901
Iteration 385, loss = 0.58131424
Iteration 386, loss = 0.53672445
Iteration 387, loss = 0.44368421
Iteration 388, loss = 0.35665793
Iteration 389, loss = 0.31468232
Iteration 390, loss = 0.28312802
Iteration 391, loss = 0.27441564
Iteration 392, loss = 0.25853928
Iteration 393, loss = 0.24301045
Iteration 394, loss = 0.23178417
Iteration 395, loss = 0.22227525
Iteration 396, loss = 0.20894227
Iteration 397, loss = 0.20738583
Iteration 398, loss = 0.19744884
Iteration 399, loss = 0.19452213
Iteration 400, loss = 0.18792578
Iteration 401, loss = 0.18179443
Iteration 402, loss = 0.17778313
Iteration 403, loss = 0.17818166
Iteration 404, loss = 0.17425894
Iteration 405, loss = 0.16704342
Iteration 406, loss = 0.17566253
Iteration 407, loss = 0.17300213
Iteration 408, loss = 0.16639997
Iteration 409, loss = 0.16555675
Iteration 410, loss = 0.15804401
Iteration 411, loss = 0.15962316
Iteration 412, loss = 0.25350791
Iteration 413, loss = 0.35394619
Iteration 414, loss = 0.34089385
Iteration 415, loss = 0.32999202
Iteration 416, loss = 0.30057900
Iteration 417, loss = 0.25724472
Iteration 418, loss = 0.22923666
Iteration 419, loss = 0.21892180
Iteration 420, loss = 0.21319537
Iteration 421, loss = 0.21187050
Iteration 422, loss = 0.20737173
Iteration 423, loss = 0.20023107
Iteration 424, loss = 0.20113262
Iteration 425, loss = 0.19356733
Iteration 426, loss = 0.20857036
Iteration 427, loss = 0.19338673
Iteration 428, loss = 0.19504399
Iteration 429, loss = 0.19322549
Iteration 430, loss = 0.19747043
Iteration 431, loss = 0.18883426
Iteration 432, loss = 0.19594658
Iteration 433, loss = 0.19170819
Iteration 434, loss = 0.19691792
Iteration 435, loss = 0.19004711
Iteration 436, loss = 0.18825429
Iteration 437, loss = 0.18781550
Iteration 438, loss = 0.19214074
Iteration 439, loss = 0.19139847
Iteration 440, loss = 0.18681764
Iteration 441, loss = 0.20098650
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 4/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(16, 16);, score=0.953 total time= 1.8min
Iteration 1, loss = 3.28755662
Iteration 2, loss = 0.82851416
Iteration 3, loss = 0.75190496
Iteration 4, loss = 0.76495137
Iteration 5, loss = 0.72078211
Iteration 6, loss = 0.72340006
Iteration 7, loss = 0.71372632
Iteration 8, loss = 0.69153063
Iteration 9, loss = 0.73164268
Iteration 10, loss = 0.69632841
Iteration 11, loss = 0.73716912
Iteration 12, loss = 0.69845521
Iteration 13, loss = 0.72704517
Iteration 14, loss = 0.70815451
Iteration 15, loss = 0.73581176
Iteration 16, loss = 0.71414252
Iteration 17, loss = 0.69954936
Iteration 18, loss = 0.70097745
Iteration 19, loss = 0.71987771
Iteration 20, loss = 0.69874245
Iteration 21, loss = 0.68354202
Iteration 22, loss = 0.73776537
Iteration 23, loss = 0.69570330
Iteration 24, loss = 0.68670906
Iteration 25, loss = 0.71541337
Iteration 26, loss = 0.70529230
Iteration 27, loss = 0.66859216
Iteration 28, loss = 0.69719752
Iteration 29, loss = 0.70980683
Iteration 30, loss = 0.65810641
Iteration 31, loss = 0.68443256
Iteration 32, loss = 0.66059359
Iteration 33, loss = 0.70008325
Iteration 34, loss = 0.68011384
Iteration 35, loss = 0.69292007
Iteration 36, loss = 0.66455088
Iteration 37, loss = 0.68826715
Iteration 38, loss = 0.67101778
Iteration 39, loss = 0.65257817
Iteration 40, loss = 0.69429728
Iteration 41, loss = 0.66769688
Iteration 42, loss = 0.65594197
Iteration 43, loss = 0.66872847
Iteration 44, loss = 0.66158281
Iteration 45, loss = 0.68254561
Iteration 46, loss = 0.67028545
Iteration 47, loss = 0.67148075
Iteration 48, loss = 0.64802886
Iteration 49, loss = 0.65061052
Iteration 50, loss = 0.67344072
Iteration 51, loss = 0.67032895
Iteration 52, loss = 0.67042974
Iteration 53, loss = 0.66654845
Iteration 54, loss = 0.64781231
Iteration 55, loss = 0.64682913
Iteration 56, loss = 0.64950344
Iteration 57, loss = 0.66391680
Iteration 58, loss = 0.67414205
Iteration 59, loss = 0.65567107
Iteration 60, loss = 0.64858875
Iteration 61, loss = 0.64179257
Iteration 62, loss = 0.65299420
Iteration 63, loss = 0.67805414
Iteration 64, loss = 0.64278291
Iteration 65, loss = 0.64372435
Iteration 66, loss = 0.64259081
Iteration 67, loss = 0.64923384
Iteration 68, loss = 0.65584715
Iteration 69, loss = 0.65035588
Iteration 70, loss = 0.64439030
Iteration 71, loss = 0.64690648
Iteration 72, loss = 0.64068615
Iteration 73, loss = 0.64653248
Iteration 74, loss = 0.64407955
Iteration 75, loss = 0.64810086
Iteration 76, loss = 0.64209744
Iteration 77, loss = 0.63545954
Iteration 78, loss = 0.63730077
Iteration 79, loss = 0.63761347
Iteration 80, loss = 0.64591292
Iteration 81, loss = 0.63413762
Iteration 82, loss = 0.64012243
Iteration 83, loss = 0.62427499
Iteration 84, loss = 0.65110592
Iteration 85, loss = 0.63545748
Iteration 86, loss = 0.63768591
Iteration 87, loss = 0.63478014
Iteration 88, loss = 0.64068674
Iteration 89, loss = 0.63678718
Iteration 90, loss = 0.63209799
Iteration 91, loss = 0.63481021
Iteration 92, loss = 0.62835327
Iteration 93, loss = 0.64758615
Iteration 94, loss = 0.63927095
Iteration 95, loss = 0.63314555
Iteration 96, loss = 0.62602918
Iteration 97, loss = 0.62637312
Iteration 98, loss = 0.63562648
Iteration 99, loss = 0.62993530
Iteration 100, loss = 0.63322775
Iteration 101, loss = 0.63255558
Iteration 102, loss = 0.61992105
Iteration 103, loss = 0.62509372
Iteration 104, loss = 0.63225933
Iteration 105, loss = 0.62685379
Iteration 106, loss = 0.63026946
Iteration 107, loss = 0.62979066
Iteration 108, loss = 0.62679662
Iteration 109, loss = 0.62590697
Iteration 110, loss = 0.62450562
Iteration 111, loss = 0.63438192
Iteration 112, loss = 0.62653584
Iteration 113, loss = 0.62816261
Iteration 114, loss = 0.63245227
Iteration 115, loss = 0.62329419
Iteration 116, loss = 0.63057836
Iteration 117, loss = 0.62346754
Iteration 118, loss = 0.62443335
Iteration 119, loss = 0.62035320
Iteration 120, loss = 0.62059294
Iteration 121, loss = 0.62572762
Iteration 122, loss = 0.61735864
Iteration 123, loss = 0.63315210
Iteration 124, loss = 0.62476391
Iteration 125, loss = 0.62276475
Iteration 126, loss = 0.61768667
Iteration 127, loss = 0.61887951
Iteration 128, loss = 0.62328450
Iteration 129, loss = 0.61552429
Iteration 130, loss = 0.62745911
Iteration 131, loss = 0.62068750
Iteration 132, loss = 0.62573609
Iteration 133, loss = 0.62078233
Iteration 134, loss = 0.61811414
Iteration 135, loss = 0.61659269
Iteration 136, loss = 0.61770197
Iteration 137, loss = 0.62472525
Iteration 138, loss = 0.61384514

Iteration 139, loss = 0.61494697
Iteration 140, loss = 0.62442812
Iteration 141, loss = 0.61276100
Iteration 142, loss = 0.61236033
Iteration 143, loss = 0.61782074
Iteration 144, loss = 0.61613345
Iteration 145, loss = 0.61561820
Iteration 146, loss = 0.61026330
Iteration 147, loss = 0.61917213
Iteration 148, loss = 0.61321369
Iteration 149, loss = 0.61529296
Iteration 150, loss = 0.61544709
Iteration 151, loss = 0.62429013
Iteration 152, loss = 0.61558387
Iteration 153, loss = 0.61080976
Iteration 154, loss = 0.61425415
Iteration 155, loss = 0.61298229
Iteration 156, loss = 0.61460054
Iteration 157, loss = 0.60791637
Iteration 158, loss = 0.61415379
Iteration 159, loss = 0.60752797
Iteration 160, loss = 0.61137775
Iteration 161, loss = 0.61111632
Iteration 162, loss = 0.61512241
Iteration 163, loss = 0.61115939
Iteration 164, loss = 0.61578219
Iteration 165, loss = 0.61162684
Iteration 166, loss = 0.60950637
Iteration 167, loss = 0.61153000
Iteration 168, loss = 0.61263279
Iteration 169, loss = 0.61415331
Iteration 170, loss = 0.61652019
Iteration 171, loss = 0.60699128
Iteration 172, loss = 0.61380370
Iteration 173, loss = 0.60713007
Iteration 174, loss = 0.60923541
Iteration 175, loss = 0.61331829
Iteration 176, loss = 0.61123425
Iteration 177, loss = 0.61081102
Iteration 178, loss = 0.60868859
Iteration 179, loss = 0.61195076
Iteration 180, loss = 0.61139540
Iteration 181, loss = 0.60711787
Iteration 182, loss = 0.60799756
Iteration 183, loss = 0.61001253
Iteration 184, loss = 0.60875624
Iteration 185, loss = 0.60977450
Iteration 186, loss = 0.60943163
Iteration 187, loss = 0.60570493
Iteration 188, loss = 0.60777817
Iteration 189, loss = 0.60523132
Iteration 190, loss = 0.60689460
Iteration 191, loss = 0.60722517
Iteration 192, loss = 0.60843579
Iteration 193, loss = 0.60563633
Iteration 194, loss = 0.60717100
Iteration 195, loss = 0.60547167
Iteration 196, loss = 0.60787782
Iteration 197, loss = 0.60773363
Iteration 198, loss = 0.60909948
Iteration 199, loss = 0.60528346
Iteration 200, loss = 0.60735792
Iteration 201, loss = 0.60577324
Iteration 202, loss = 0.60568633
Iteration 203, loss = 0.60321643
Iteration 204, loss = 0.60670510
Iteration 205, loss = 0.60582900
Iteration 206, loss = 0.60662538
Iteration 207, loss = 0.60539985
Iteration 208, loss = 0.60596388
Iteration 209, loss = 0.60347393
Iteration 210, loss = 0.60389388
Iteration 211, loss = 0.60777279
Iteration 212, loss = 0.60437468
Iteration 213, loss = 0.60329731
Iteration 214, loss = 0.60501587
Iteration 215, loss = 0.60218734
Iteration 216, loss = 0.60320334
Iteration 217, loss = 0.60361505
Iteration 218, loss = 0.60430167
Iteration 219, loss = 0.60411830
Iteration 220, loss = 0.60376435
Iteration 221, loss = 0.60312645
Iteration 222, loss = 0.60365904
Iteration 223, loss = 0.60503782
Iteration 224, loss = 0.60168828
Iteration 225, loss = 0.60279294
Iteration 226, loss = 0.60384892
Iteration 227, loss = 0.60277995
Iteration 228, loss = 0.60465054
Iteration 229, loss = 0.60206075
Iteration 230, loss = 0.60214270
Iteration 231, loss = 0.60330186
Iteration 232, loss = 0.60267454
Iteration 233, loss = 0.60337750
Iteration 234, loss = 0.60146026
Iteration 235, loss = 0.60302047
Iteration 236, loss = 0.60226835
Iteration 237, loss = 0.60350902
Iteration 238, loss = 0.60330358
Iteration 239, loss = 0.60124581
Iteration 240, loss = 0.60185818
Iteration 241, loss = 0.60227584
Iteration 242, loss = 0.60129181
Iteration 243, loss = 0.60083446
Iteration 244, loss = 0.60336375
Iteration 245, loss = 0.60028585
Iteration 246, loss = 0.60209624
Iteration 247, loss = 0.60307471
Iteration 248, loss = 0.60249625
Iteration 249, loss = 0.60074586
Iteration 250, loss = 0.60216613
Iteration 251, loss = 0.60107624
Iteration 252, loss = 0.59997723
Iteration 253, loss = 0.59948837
Iteration 254, loss = 0.60102556
Iteration 255, loss = 0.60057775
Iteration 256, loss = 0.60152509
Iteration 257, loss = 0.60158834
Iteration 258, loss = 0.60211947
Iteration 259, loss = 0.60327066
Iteration 260, loss = 0.60059563
Iteration 261, loss = 0.60005725
Iteration 262, loss = 0.60151130
Iteration 263, loss = 0.60011988
Iteration 264, loss = 0.60092545
Iteration 265, loss = 0.60001393
Iteration 266, loss = 0.59949814
Iteration 267, loss = 0.60074819
Iteration 268, loss = 0.59973960
Iteration 269, loss = 0.60114731
Iteration 270, loss = 0.60160643
Iteration 271, loss = 0.59953157
Iteration 272, loss = 0.59911233
Iteration 273, loss = 0.60058530
Iteration 274, loss = 0.59855160
Iteration 275, loss = 0.59906557
Iteration 276, loss = 0.59886415
Iteration 277, loss = 0.59726684
Iteration 278, loss = 0.59727417
Iteration 279, loss = 0.59663647
Iteration 280, loss = 0.59557550
Iteration 281, loss = 0.59471861
Iteration 282, loss = 0.59489135
Iteration 283, loss = 0.59425311
Iteration 284, loss = 0.59384450
Iteration 285, loss = 0.59304682
Iteration 286, loss = 0.59298620
Iteration 287, loss = 0.59118425
Iteration 288, loss = 0.59189726
Iteration 289, loss = 0.59296131
Iteration 290, loss = 0.59022556
Iteration 291, loss = 0.59132030
Iteration 292, loss = 0.59142593
Iteration 293, loss = 0.58897781
Iteration 294, loss = 0.59049845
Iteration 295, loss = 0.58926610
Iteration 296, loss = 0.59132922
Iteration 297, loss = 0.58922871
Iteration 298, loss = 0.58833429
Iteration 299, loss = 0.59068301
Iteration 300, loss = 0.59007590
Iteration 301, loss = 0.58747858
Iteration 302, loss = 0.58936047
Iteration 303, loss = 0.59125467
Iteration 304, loss = 0.59957990
Iteration 305, loss = 0.59913202
Iteration 306, loss = 0.59913107
Iteration 307, loss = 0.59993745
Iteration 308, loss = 0.59993396
Iteration 309, loss = 0.59923135
Iteration 310, loss = 0.59870142
Iteration 311, loss = 0.59903090
Iteration 312, loss = 0.59928482
Iteration 313, loss = 0.59920580
Iteration 314, loss = 0.60022462
Iteration 315, loss = 0.60026259
Iteration 316, loss = 0.59999665
Iteration 317, loss = 0.59886767
Iteration 318, loss = 0.60037890
Iteration 319, loss = 0.59958701
Iteration 320, loss = 0.59985614
Iteration 321, loss = 0.59905269
Iteration 322, loss = 0.59933650
Iteration 323, loss = 0.59879558
Iteration 324, loss = 0.60037884
Iteration 325, loss = 0.59996128
Iteration 326, loss = 0.59879208
Iteration 327, loss = 0.60038506
Iteration 328, loss = 0.59892108
Iteration 329, loss = 0.59919178
Iteration 330, loss = 0.59893861
Iteration 331, loss = 0.59990471
Iteration 332, loss = 0.59882286
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 5/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(16, 16);, score=0.655 total time= 1.5min
Iteration 1, loss = 3.47852300
Iteration 2, loss = 0.84395465
Iteration 3, loss = 0.80183335
Iteration 4, loss = 0.79758218
Iteration 5, loss = 0.78256655
Iteration 6, loss = 0.73913169
Iteration 7, loss = 0.73026489
Iteration 8, loss = 0.73462567
Iteration 9, loss = 0.69632717
Iteration 10, loss = 0.68222054
Iteration 11, loss = 0.68096345
Iteration 12, loss = 0.66329950
Iteration 13, loss = 0.65805306
Iteration 14, loss = 0.65993568
Iteration 15, loss = 0.64646718
Iteration 16, loss = 0.64202891
Iteration 17, loss = 0.64927617
Iteration 18, loss = 0.63531748
Iteration 19, loss = 0.64860407
Iteration 20, loss = 0.63236299
Iteration 21, loss = 0.63376930
Iteration 22, loss = 0.63997810
Iteration 23, loss = 0.63777518
Iteration 24, loss = 0.62897809
Iteration 25, loss = 0.64109157
Iteration 26, loss = 0.63024379
Iteration 27, loss = 0.62976121
Iteration 28, loss = 0.62639159
Iteration 29, loss = 0.62286042
Iteration 30, loss = 0.62503057
Iteration 31, loss = 0.63048335
Iteration 32, loss = 0.61914892
Iteration 33, loss = 0.63178503
Iteration 34, loss = 0.62608921
Iteration 35, loss = 0.62310346
Iteration 36, loss = 0.62376644
Iteration 37, loss = 0.62511065
Iteration 38, loss = 0.61648118
Iteration 39, loss = 0.62592077
Iteration 40, loss = 0.62240337
Iteration 41, loss = 0.62308616
Iteration 42, loss = 0.61521448
Iteration 43, loss = 0.62234612
Iteration 44, loss = 0.61613930
Iteration 45, loss = 0.61851054
Iteration 46, loss = 0.61418731
Iteration 47, loss = 0.61881903
Iteration 48, loss = 0.62007843
Iteration 49, loss = 0.61651219
Iteration 50, loss = 0.61362973

Iteration 51, loss = 0.61137484
Iteration 52, loss = 0.61350214
Iteration 53, loss = 0.60966366
Iteration 54, loss = 0.61287050
Iteration 55, loss = 0.60850397
Iteration 56, loss = 0.60922367
Iteration 57, loss = 0.60827066
Iteration 58, loss = 0.61016998
Iteration 59, loss = 0.60952507
Iteration 60, loss = 0.60277266
Iteration 61, loss = 0.60555819
Iteration 62, loss = 0.59980014
Iteration 63, loss = 0.53979492
Iteration 64, loss = 0.40758383
Iteration 65, loss = 0.33457931
Iteration 66, loss = 0.30432886
Iteration 67, loss = 0.28908152
Iteration 68, loss = 0.27736944
Iteration 69, loss = 0.27501548
Iteration 70, loss = 0.26851211
Iteration 71, loss = 0.27113784
Iteration 72, loss = 0.26482023
Iteration 73, loss = 0.26247613
Iteration 74, loss = 0.27397907
Iteration 75, loss = 0.25230666
Iteration 76, loss = 0.25922623
Iteration 77, loss = 0.26082462
Iteration 78, loss = 0.26638096
Iteration 79, loss = 0.25984394
Iteration 80, loss = 0.25075766
Iteration 81, loss = 0.26408248
Iteration 82, loss = 0.24846333
Iteration 83, loss = 0.25291070
Iteration 84, loss = 0.25597961
Iteration 85, loss = 0.24432990
Iteration 86, loss = 0.26409731
Iteration 87, loss = 0.24276764
Iteration 88, loss = 0.24321147
Iteration 89, loss = 0.24485849
Iteration 90, loss = 0.24465892
Iteration 91, loss = 0.23877121
Iteration 92, loss = 0.23505133
Iteration 93, loss = 0.24726976
Iteration 94, loss = 0.23266754
Iteration 95, loss = 0.23928501
Iteration 96, loss = 0.21971056
Iteration 97, loss = 0.22870462
Iteration 98, loss = 0.22077217
Iteration 99, loss = 0.22066007
Iteration 100, loss = 0.21244671
Iteration 101, loss = 0.21096876
Iteration 102, loss = 0.20925833
Iteration 103, loss = 0.21537489
Iteration 104, loss = 0.20385303
Iteration 105, loss = 0.20070114
Iteration 106, loss = 0.18693175
Iteration 107, loss = 0.18342520
Iteration 108, loss = 0.18307501
Iteration 109, loss = 0.18128649
Iteration 110, loss = 0.17165420
Iteration 111, loss = 0.16608511
Iteration 112, loss = 0.16490155
Iteration 113, loss = 0.17198807
Iteration 114, loss = 0.15660899
Iteration 115, loss = 0.15414816
Iteration 116, loss = 0.16234049
Iteration 117, loss = 0.16245877
Iteration 118, loss = 0.16092810
Iteration 119, loss = 0.14954360
Iteration 120, loss = 0.15560694
Iteration 121, loss = 0.14345829
Iteration 122, loss = 0.17803544
Iteration 123, loss = 0.27057578
Iteration 124, loss = 0.22474550
Iteration 125, loss = 0.21843391
Iteration 126, loss = 0.20434597
Iteration 127, loss = 0.19247716
Iteration 128, loss = 0.18669213
Iteration 129, loss = 0.17720899
Iteration 130, loss = 0.17759804
Iteration 131, loss = 0.16748418
Iteration 132, loss = 0.16286699
Iteration 133, loss = 0.14881177
Iteration 134, loss = 0.15805846
Iteration 135, loss = 0.18744902
Iteration 136, loss = 0.13911753
Iteration 137, loss = 0.14721211
Iteration 138, loss = 0.15879876
Iteration 139, loss = 0.17944840
Iteration 140, loss = 0.20314699
Iteration 141, loss = 0.18619640
Iteration 142, loss = 0.19848870
Iteration 143, loss = 0.26649638
Iteration 144, loss = 0.21838055
Iteration 145, loss = 0.24347188
Iteration 146, loss = 0.24954648
Iteration 147, loss = 0.24044018
Iteration 148, loss = 0.25060900
Iteration 149, loss = 0.25662914
Iteration 150, loss = 0.23067271
Iteration 151, loss = 0.25438643
Iteration 152, loss = 0.23643682
Iteration 153, loss = 0.25425653
Iteration 154, loss = 0.23310680
Iteration 155, loss = 0.26501805
Iteration 156, loss = 0.24727733
Iteration 157, loss = 0.25357600
Iteration 158, loss = 0.23620599
Iteration 159, loss = 0.24325798
Iteration 160, loss = 0.22086138
Iteration 161, loss = 0.23611674
Iteration 162, loss = 0.23146276
Iteration 163, loss = 0.22483418
Iteration 164, loss = 0.21560228
Iteration 165, loss = 0.20558680
Iteration 166, loss = 0.21217065
Iteration 167, loss = 0.19713954
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 1/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(16, 16, 16);, score=0.925 total time= 1.2min
Iteration 1, loss = 2.55017580
Iteration 2, loss = 0.81492471
Iteration 3, loss = 0.77610580
Iteration 4, loss = 0.75970188
Iteration 5, loss = 0.74918501
Iteration 6, loss = 0.73164392
Iteration 7, loss = 0.72064302
Iteration 8, loss = 0.70402994
Iteration 9, loss = 0.69767905
Iteration 10, loss = 0.72417022
Iteration 11, loss = 0.68166852
Iteration 12, loss = 0.68096864
Iteration 13, loss = 0.66167984
Iteration 14, loss = 0.67223185
Iteration 15, loss = 0.68629004
Iteration 16, loss = 0.66460562
Iteration 17, loss = 0.65706518
Iteration 18, loss = 0.66296340
Iteration 19, loss = 0.66581632
Iteration 20, loss = 0.65790966
Iteration 21, loss = 0.66316690
Iteration 22, loss = 0.65672142
Iteration 23, loss = 0.64576248
Iteration 24, loss = 0.65976611
Iteration 25, loss = 0.64162194
Iteration 26, loss = 0.63561365
Iteration 27, loss = 0.64952390
Iteration 28, loss = 0.64330330
Iteration 29, loss = 0.64741418
Iteration 30, loss = 0.63180879
Iteration 31, loss = 0.63935042
Iteration 32, loss = 0.62735552
Iteration 33, loss = 0.64358923
Iteration 34, loss = 0.63900741
Iteration 35, loss = 0.63056664
Iteration 36, loss = 0.62838727
Iteration 37, loss = 0.62625389
Iteration 38, loss = 0.63471814
Iteration 39, loss = 0.62562065
Iteration 40, loss = 0.62687488
Iteration 41, loss = 0.62905418
Iteration 42, loss = 0.61878364
Iteration 43, loss = 0.62306801
Iteration 44, loss = 0.62113847
Iteration 45, loss = 0.61987737
Iteration 46, loss = 0.61763952
Iteration 47, loss = 0.61524400
Iteration 48, loss = 0.61560160
Iteration 49, loss = 0.61659245
Iteration 50, loss = 0.61439126
Iteration 51, loss = 0.62427457
Iteration 52, loss = 0.61232146
Iteration 53, loss = 0.61851929
Iteration 54, loss = 0.61657840
Iteration 55, loss = 0.61621355
Iteration 56, loss = 0.61329198
Iteration 57, loss = 0.61157632
Iteration 58, loss = 0.61088448
Iteration 59, loss = 0.61228809
Iteration 60, loss = 0.61095483
Iteration 61, loss = 0.60855940
Iteration 62, loss = 0.61139411
Iteration 63, loss = 0.61306017
Iteration 64, loss = 0.61070654
Iteration 65, loss = 0.61411621
Iteration 66, loss = 0.61118226
Iteration 67, loss = 0.60892294
Iteration 68, loss = 0.60964291
Iteration 69, loss = 0.60714740
Iteration 70, loss = 0.61049492
Iteration 71, loss = 0.60819636
Iteration 72, loss = 0.60431597
Iteration 73, loss = 0.60722453
Iteration 74, loss = 0.60554866
Iteration 75, loss = 0.60393917
Iteration 76, loss = 0.60399636
Iteration 77, loss = 0.60172331
Iteration 78, loss = 0.60332998
Iteration 79, loss = 0.60094235
Iteration 80, loss = 0.60369946
Iteration 81, loss = 0.60109154
Iteration 82, loss = 0.60161918
Iteration 83, loss = 0.60203188
Iteration 84, loss = 0.59577508
Iteration 85, loss = 0.59512416
Iteration 86, loss = 0.57879432
Iteration 87, loss = 0.55428315
Iteration 88, loss = 0.50739377
Iteration 89, loss = 0.45758918
Iteration 90, loss = 0.39982641
Iteration 91, loss = 0.40155403
Iteration 92, loss = 0.35379513
Iteration 93, loss = 0.31804347
Iteration 94, loss = 0.29439069
Iteration 95, loss = 0.27343262
Iteration 96, loss = 0.22656927
Iteration 97, loss = 0.26027227
Iteration 98, loss = 0.29875933
Iteration 99, loss = 0.20881137
Iteration 100, loss = 0.29595080
Iteration 101, loss = 0.29370301
Iteration 102, loss = 0.38530610
Iteration 103, loss = 0.27987059
Iteration 104, loss = 0.28819964
Iteration 105, loss = 0.28742459
Iteration 106, loss = 0.29718245
Iteration 107, loss = 0.42257393
Iteration 108, loss = 0.63274885
Iteration 109, loss = 0.61981657
Iteration 110, loss = 0.60784230
Iteration 111, loss = 0.60620234
Iteration 112, loss = 0.60421851
Iteration 113, loss = 0.60305534
Iteration 114, loss = 0.60357592
Iteration 115, loss = 0.60506394
Iteration 116, loss = 0.60635961
Iteration 117, loss = 0.60177641
Iteration 118, loss = 0.60197448
Iteration 119, loss = 0.60030178
Iteration 120, loss = 0.60426097
Iteration 121, loss = 0.60104437
Iteration 122, loss = 0.59902113
Iteration 123, loss = 0.60123407
Iteration 124, loss = 0.60069095
Iteration 125, loss = 0.60050060
Iteration 126, loss = 0.59914630
Iteration 127, loss = 0.60241481
Iteration 128, loss = 0.60111340
Iteration 129, loss = 0.59926866
Iteration 130, loss = 0.60044849
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.

[CV 2/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(16, 16, 16);, score=0.684 total time= 1.1min
Iteration 1, loss = 1.39039860
Iteration 2, loss = 0.74158035
Iteration 3, loss = 0.72281074
Iteration 4, loss = 0.69154421
Iteration 5, loss = 0.67980150
Iteration 6, loss = 0.67917114
Iteration 7, loss = 0.65694816
Iteration 8, loss = 0.65138685
Iteration 9, loss = 0.64834259
Iteration 10, loss = 0.66130803
Iteration 11, loss = 0.63801058
Iteration 12, loss = 0.64651708
Iteration 13, loss = 0.64022695
Iteration 14, loss = 0.63924351
Iteration 15, loss = 0.64930378
Iteration 16, loss = 0.63398460
Iteration 17, loss = 0.63773532
Iteration 18, loss = 0.63472913
Iteration 19, loss = 0.63625261
Iteration 20, loss = 0.62902006
Iteration 21, loss = 0.62536940
Iteration 22, loss = 0.63122613
Iteration 23, loss = 0.62374717
Iteration 24, loss = 0.61900631
Iteration 25, loss = 0.63101084
Iteration 26, loss = 0.62191798
Iteration 27, loss = 0.62495105
Iteration 28, loss = 0.61920474
Iteration 29, loss = 0.62516278
Iteration 30, loss = 0.62321549
Iteration 31, loss = 0.61994651
Iteration 32, loss = 0.61819106
Iteration 33, loss = 0.61869032
Iteration 34, loss = 0.62353030
Iteration 35, loss = 0.61746425
Iteration 36, loss = 0.61466085
Iteration 37, loss = 0.61757620
Iteration 38, loss = 0.61597663
Iteration 39, loss = 0.61624017
Iteration 40, loss = 0.61386819
Iteration 41, loss = 0.61351597
Iteration 42, loss = 0.61329987
Iteration 43, loss = 0.61362245
Iteration 44, loss = 0.61130107
Iteration 45, loss = 0.61057797
Iteration 46, loss = 0.61087287
Iteration 47, loss = 0.60922472
Iteration 48, loss = 0.61354236
Iteration 49, loss = 0.61015146
Iteration 50, loss = 0.61012908
Iteration 51, loss = 0.60971242
Iteration 52, loss = 0.60894157
Iteration 53, loss = 0.61044999
Iteration 54, loss = 0.60449171
Iteration 55, loss = 0.60766764
Iteration 56, loss = 0.60377724
Iteration 57, loss = 0.60465371
Iteration 58, loss = 0.60223791
Iteration 59, loss = 0.60408825
Iteration 60, loss = 0.60081628
Iteration 61, loss = 0.60176498
Iteration 62, loss = 0.60090703
Iteration 63, loss = 0.59902412
Iteration 64, loss = 0.59864018
Iteration 65, loss = 0.60070391
Iteration 66, loss = 0.59598977
Iteration 67, loss = 0.59482353
Iteration 68, loss = 0.59277707
Iteration 69, loss = 0.59411129
Iteration 70, loss = 0.59336112
Iteration 71, loss = 0.58867430
Iteration 72, loss = 0.58720853
Iteration 73, loss = 0.58757209
Iteration 74, loss = 0.58376954
Iteration 75, loss = 0.58423403
Iteration 76, loss = 0.57687373
Iteration 77, loss = 0.57777265
Iteration 78, loss = 0.58099958
Iteration 79, loss = 0.57556103
Iteration 80, loss = 0.57977793
Iteration 81, loss = 0.57925459
Iteration 82, loss = 0.57797510
Iteration 83, loss = 0.57477763
Iteration 84, loss = 0.57751182
Iteration 85, loss = 0.57804380
Iteration 86, loss = 0.57427249
Iteration 87, loss = 0.57436160
Iteration 88, loss = 0.57535134
Iteration 89, loss = 0.57608980
Iteration 90, loss = 0.57736718
Iteration 91, loss = 0.57525316
Iteration 92, loss = 0.57476349
Iteration 93, loss = 0.57546110
Iteration 94, loss = 0.57635060
Iteration 95, loss = 0.57517168
Iteration 96, loss = 0.57337235
Iteration 97, loss = 0.57579377
Iteration 98, loss = 0.57318879
Iteration 99, loss = 0.57204972
Iteration 100, loss = 0.57394991
Iteration 101, loss = 0.57298055
Iteration 102, loss = 0.57176714
Iteration 103, loss = 0.57350841
Iteration 104, loss = 0.57289008
Iteration 105, loss = 0.57152469
Iteration 106, loss = 0.57047502
Iteration 107, loss = 0.57219698
Iteration 108, loss = 0.57145472
Iteration 109, loss = 0.56984776
Iteration 110, loss = 0.57116075
Iteration 111, loss = 0.57075279
Iteration 112, loss = 0.57346904
Iteration 113, loss = 0.56924230
Iteration 114, loss = 0.57330963
Iteration 115, loss = 0.57064136
Iteration 116, loss = 0.56968560
Iteration 117, loss = 0.57187322
Iteration 118, loss = 0.57310552
Iteration 119, loss = 0.57135830
Iteration 120, loss = 0.57078473
Iteration 121, loss = 0.57020483
Iteration 122, loss = 0.56930765
Iteration 123, loss = 0.57028443
Iteration 124, loss = 0.56886141
Iteration 125, loss = 0.57097698
Iteration 126, loss = 0.57086049
Iteration 127, loss = 0.56902130
Iteration 128, loss = 0.57011527
Iteration 129, loss = 0.56951783
Iteration 130, loss = 0.57063740
Iteration 131, loss = 0.57044186
Iteration 132, loss = 0.56878549
Iteration 133, loss = 0.57047637
Iteration 134, loss = 0.56892856
Iteration 135, loss = 0.57028486
Iteration 136, loss = 0.57234236
Iteration 137, loss = 0.56979208
Iteration 138, loss = 0.56855458
Iteration 139, loss = 0.56664537
Iteration 140, loss = 0.57319329
Iteration 141, loss = 0.56950936
Iteration 142, loss = 0.56848360
Iteration 143, loss = 0.56740155
Iteration 144, loss = 0.57054427
Iteration 145, loss = 0.56704330
Iteration 146, loss = 0.56735027
Iteration 147, loss = 0.56874087
Iteration 148, loss = 0.56855957
Iteration 149, loss = 0.56957436
Iteration 150, loss = 0.56881910
Iteration 151, loss = 0.56848242
Iteration 152, loss = 0.56615323
Iteration 153, loss = 0.56733622
Iteration 154, loss = 0.56783003
Iteration 155, loss = 0.56633639
Iteration 156, loss = 0.56714054
Iteration 157, loss = 0.56621364
Iteration 158, loss = 0.56828573
Iteration 159, loss = 0.56876668
Iteration 160, loss = 0.56602212
Iteration 161, loss = 0.56777381
Iteration 162, loss = 0.56610937
Iteration 163, loss = 0.56846886
Iteration 164, loss = 0.56757839
Iteration 165, loss = 0.56767911
Iteration 166, loss = 0.56752973
Iteration 167, loss = 0.56766370
Iteration 168, loss = 0.56700795
Iteration 169, loss = 0.56840563
Iteration 170, loss = 0.56559595
Iteration 171, loss = 0.56866742
Iteration 172, loss = 0.56670413
Iteration 173, loss = 0.56698802
Iteration 174, loss = 0.56657190
Iteration 175, loss = 0.56633924
Iteration 176, loss = 0.56663277
Iteration 177, loss = 0.56579016
Iteration 178, loss = 0.56721576
Iteration 179, loss = 0.56659674
Iteration 180, loss = 0.56669992
Iteration 181, loss = 0.56596734
Iteration 182, loss = 0.56611358
Iteration 183, loss = 0.56681193
Iteration 184, loss = 0.57021065
Iteration 185, loss = 0.56544600
Iteration 186, loss = 0.56592705
Iteration 187, loss = 0.56494106
Iteration 188, loss = 0.56819352
Iteration 189, loss = 0.56600782
Iteration 190, loss = 0.56687175
Iteration 191, loss = 0.56596872
Iteration 192, loss = 0.56616647
Iteration 193, loss = 0.56704874
Iteration 194, loss = 0.56605933
Iteration 195, loss = 0.56759618
Iteration 196, loss = 0.56504466
Iteration 197, loss = 0.56634678
Iteration 198, loss = 0.56464128
Iteration 199, loss = 0.56659448
Iteration 200, loss = 0.56504089
Iteration 201, loss = 0.56598945
Iteration 202, loss = 0.56557225
Iteration 203, loss = 0.56539974
Iteration 204, loss = 0.56616133
Iteration 205, loss = 0.56821312
Iteration 206, loss = 0.56571728
Iteration 207, loss = 0.56525199
Iteration 208, loss = 0.56572478
Iteration 209, loss = 0.56688244
Iteration 210, loss = 0.56607543
Iteration 211, loss = 0.56461727
Iteration 212, loss = 0.56734887
Iteration 213, loss = 0.56537412
Iteration 214, loss = 0.56446805
Iteration 215, loss = 0.56519041
Iteration 216, loss = 0.56642372
Iteration 217, loss = 0.56435443
Iteration 218, loss = 0.56489229
Iteration 219, loss = 0.56593916
Iteration 220, loss = 0.56405034
Iteration 221, loss = 0.56583838
Iteration 222, loss = 0.56534177
Iteration 223, loss = 0.56579673
Iteration 224, loss = 0.56444522
Iteration 225, loss = 0.56458961
Iteration 226, loss = 0.56445810
Iteration 227, loss = 0.56481477
Iteration 228, loss = 0.56632675
Iteration 229, loss = 0.56626146
Iteration 230, loss = 0.56424613
Iteration 231, loss = 0.56529500
Iteration 232, loss = 0.56531982
Iteration 233, loss = 0.56495663
Iteration 234, loss = 0.56314560
Iteration 235, loss = 0.56532780
Iteration 236, loss = 0.56328517
Iteration 237, loss = 0.56500377
Iteration 238, loss = 0.56396384
Iteration 239, loss = 0.56233287
Iteration 240, loss = 0.56173886
Iteration 241, loss = 0.55768527
Iteration 242, loss = 0.54712502
Iteration 243, loss = 0.51610620
Iteration 244, loss = 0.47002715
Iteration 245, loss = 0.40965127
Iteration 246, loss = 0.37366151
Iteration 247, loss = 0.34813157
Iteration 248, loss = 0.32608679

Iteration 249, loss = 0.30659090
Iteration 250, loss = 0.29574351
Iteration 251, loss = 0.28405651
Iteration 252, loss = 0.27788593
Iteration 253, loss = 0.25940560
Iteration 254, loss = 0.26975917
Iteration 255, loss = 0.26432836
Iteration 256, loss = 0.40811382
Iteration 257, loss = 0.36669996
Iteration 258, loss = 0.34565634
Iteration 259, loss = 0.33494436
Iteration 260, loss = 0.31554035
Iteration 261, loss = 0.32117754
Iteration 262, loss = 0.30236024
Iteration 263, loss = 0.30028449
Iteration 264, loss = 0.29766915
Iteration 265, loss = 0.50167673
Iteration 266, loss = 0.54985859
Iteration 267, loss = 0.54541336
Iteration 268, loss = 0.54040696
Iteration 269, loss = 0.53777657
Iteration 270, loss = 0.53247094
Iteration 271, loss = 0.52278150
Iteration 272, loss = 0.50366101
Iteration 273, loss = 0.43546881
Iteration 274, loss = 0.33441416
Iteration 275, loss = 0.30048595
Iteration 276, loss = 0.28301084
Iteration 277, loss = 0.27054997
Iteration 278, loss = 0.27284486
Iteration 279, loss = 0.26445377
Iteration 280, loss = 0.27105391
Iteration 281, loss = 0.26110212
Iteration 282, loss = 0.25622593
Iteration 283, loss = 0.25492032
Iteration 284, loss = 0.25753726
Iteration 285, loss = 0.24693277
Iteration 286, loss = 0.25275685
Iteration 287, loss = 0.24248532
Iteration 288, loss = 0.23539448
Iteration 289, loss = 0.23972147
Iteration 290, loss = 0.23894821
Iteration 291, loss = 0.23176151
Iteration 292, loss = 0.21346584
Iteration 293, loss = 0.20862224
Iteration 294, loss = 0.19860278
Iteration 295, loss = 0.17910124
Iteration 296, loss = 0.16832493
Iteration 297, loss = 0.18619246
Iteration 298, loss = 0.19597419
Iteration 299, loss = 0.16072830
Iteration 300, loss = 0.15973980
Iteration 301, loss = 0.17162344
Iteration 302, loss = 0.19353085
Iteration 303, loss = 0.15790937
Iteration 304, loss = 0.30197313
Iteration 305, loss = 0.68864467
Iteration 306, loss = 0.55071177
Iteration 307, loss = 0.48756420
Iteration 308, loss = 0.47537810
Iteration 309, loss = 0.46707097
Iteration 310, loss = 0.45701400
Iteration 311, loss = 0.44899387
Iteration 312, loss = 0.43564780
Iteration 313, loss = 0.42227290
Iteration 314, loss = 0.40440947
Iteration 315, loss = 0.38252794
Iteration 316, loss = 0.35988334
Iteration 317, loss = 0.35743791
Iteration 318, loss = 0.31477348
Iteration 319, loss = 0.28403217
Iteration 320, loss = 0.27011303
Iteration 321, loss = 0.25705649
Iteration 322, loss = 0.24708005
Iteration 323, loss = 0.24285416
Iteration 324, loss = 0.23239320
Iteration 325, loss = 0.23077385
Iteration 326, loss = 0.22036720
Iteration 327, loss = 0.21302030
Iteration 328, loss = 0.21227554
Iteration 329, loss = 0.20575208
Iteration 330, loss = 0.20615458
Iteration 331, loss = 0.20445773
Iteration 332, loss = 0.19737800
Iteration 333, loss = 0.19753003
Iteration 334, loss = 0.19161395
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 3/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(16, 16, 16);, score=0.947 total time= 1.8min
Iteration 1, loss = 2.08910482
Iteration 2, loss = 0.76736834
Iteration 3, loss = 0.73984305
Iteration 4, loss = 0.72406566
Iteration 5, loss = 0.71701028
Iteration 6, loss = 0.70405472
Iteration 7, loss = 0.68459305
Iteration 8, loss = 0.67746291
Iteration 9, loss = 0.66329266
Iteration 10, loss = 0.65210212
Iteration 11, loss = 0.64626030
Iteration 12, loss = 0.64318626
Iteration 13, loss = 0.63879491
Iteration 14, loss = 0.63175526
Iteration 15, loss = 0.63003152
Iteration 16, loss = 0.62775440
Iteration 17, loss = 0.62221576
Iteration 18, loss = 0.63033024
Iteration 19, loss = 0.62160443
Iteration 20, loss = 0.62350307
Iteration 21, loss = 0.61837240
Iteration 22, loss = 0.61606051
Iteration 23, loss = 0.62006698
Iteration 24, loss = 0.61966419
Iteration 25, loss = 0.61583612
Iteration 26, loss = 0.61980151
Iteration 27, loss = 0.61543569
Iteration 28, loss = 0.61320167
Iteration 29, loss = 0.61553508
Iteration 30, loss = 0.61880035
Iteration 31, loss = 0.61447547
Iteration 32, loss = 0.61154240
Iteration 33, loss = 0.61568155
Iteration 34, loss = 0.61281796
Iteration 35, loss = 0.61337998
Iteration 36, loss = 0.60896478
Iteration 37, loss = 0.60870778
Iteration 38, loss = 0.61193526
Iteration 39, loss = 0.61066314
Iteration 40, loss = 0.60869953
Iteration 41, loss = 0.61029135
Iteration 42, loss = 0.60864593
Iteration 43, loss = 0.60969658
Iteration 44, loss = 0.60779086
Iteration 45, loss = 0.61139949
Iteration 46, loss = 0.60625788
Iteration 47, loss = 0.60874591
Iteration 48, loss = 0.61001264
Iteration 49, loss = 0.60495548
Iteration 50, loss = 0.60677152
Iteration 51, loss = 0.60949921
Iteration 52, loss = 0.60678372
Iteration 53, loss = 0.60550165
Iteration 54, loss = 0.60544887
Iteration 55, loss = 0.60850942
Iteration 56, loss = 0.60420046
Iteration 57, loss = 0.60611239
Iteration 58, loss = 0.60614319
Iteration 59, loss = 0.60480278
Iteration 60, loss = 0.60487686
Iteration 61, loss = 0.60478114
Iteration 62, loss = 0.60587823
Iteration 63, loss = 0.60544243
Iteration 64, loss = 0.60276577
Iteration 65, loss = 0.60394246
Iteration 66, loss = 0.60346097
Iteration 67, loss = 0.60024879
Iteration 68, loss = 0.60093394
Iteration 69, loss = 0.59926762
Iteration 70, loss = 0.59978418
Iteration 71, loss = 0.59880402
Iteration 72, loss = 0.59827382
Iteration 73, loss = 0.59596494
Iteration 74, loss = 0.59029609
Iteration 75, loss = 0.58733553
Iteration 76, loss = 0.58328974
Iteration 77, loss = 0.57482446
Iteration 78, loss = 0.58132405
Iteration 79, loss = 0.57366284
Iteration 80, loss = 0.57642306
Iteration 81, loss = 0.57084670
Iteration 82, loss = 0.57493582
Iteration 83, loss = 0.57218639
Iteration 84, loss = 0.57208925
Iteration 85, loss = 0.57082651
Iteration 86, loss = 0.57040068
Iteration 87, loss = 0.57275805
Iteration 88, loss = 0.57418355
Iteration 89, loss = 0.57183686
Iteration 90, loss = 0.57099892
Iteration 91, loss = 0.57532997
Iteration 92, loss = 0.57832905
Iteration 93, loss = 0.57412580
Iteration 94, loss = 0.57839532
Iteration 95, loss = 0.59153739
Iteration 96, loss = 0.58047160
Iteration 97, loss = 0.57879386
Iteration 98, loss = 0.57861717
Iteration 99, loss = 0.57455200
Iteration 100, loss = 0.57947222
Iteration 101, loss = 0.57482567
Iteration 102, loss = 0.57468737
Iteration 103, loss = 0.57733781
Iteration 104, loss = 0.57425345
Iteration 105, loss = 0.57443802
Iteration 106, loss = 0.57370600
Iteration 107, loss = 0.57470469
Iteration 108, loss = 0.57593997
Iteration 109, loss = 0.57396123
Iteration 110, loss = 0.57314620
Iteration 111, loss = 0.57205294
Iteration 112, loss = 0.57298432
Iteration 113, loss = 0.57376700
Iteration 114, loss = 0.57372898
Iteration 115, loss = 0.57510552
Iteration 116, loss = 0.57497754
Iteration 117, loss = 0.57179201
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 4/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(16, 16, 16);, score=0.692 total time= 1.0min
Iteration 1, loss = 2.74352933
Iteration 2, loss = 1.16532803
Iteration 3, loss = 0.90015652
Iteration 4, loss = 0.74772944
Iteration 5, loss = 0.74382729
Iteration 6, loss = 0.76802221
Iteration 7, loss = 0.84879377
Iteration 8, loss = 0.81889493
Iteration 9, loss = 0.73011056
Iteration 10, loss = 0.74907375
Iteration 11, loss = 0.70412110
Iteration 12, loss = 0.66687137
Iteration 13, loss = 0.76680043
Iteration 14, loss = 0.69417218
Iteration 15, loss = 0.72810609
Iteration 16, loss = 0.67851169
Iteration 17, loss = 0.71983626
Iteration 18, loss = 0.68723165
Iteration 19, loss = 0.67504892
Iteration 20, loss = 0.71921182
Iteration 21, loss = 0.68911393
Iteration 22, loss = 0.68332302
Iteration 23, loss = 0.68944725
Iteration 24, loss = 0.69755479
Iteration 25, loss = 0.66668189
Iteration 26, loss = 0.66365202
Iteration 27, loss = 0.66485012
Iteration 28, loss = 0.67635430
Iteration 29, loss = 0.70804165
Iteration 30, loss = 0.64879388
Iteration 31, loss = 0.65429586
Iteration 32, loss = 0.66307911
Iteration 33, loss = 0.66564063
Iteration 34, loss = 0.65395996
Iteration 35, loss = 0.67334042
Iteration 36, loss = 0.65203937
Iteration 37, loss = 0.66561169
Iteration 38, loss = 0.65539081

Iteration 39, loss = 0.65688927
Iteration 40, loss = 0.65300112
Iteration 41, loss = 0.65330737
Iteration 42, loss = 0.65475426
Iteration 43, loss = 0.64507466
Iteration 44, loss = 0.65813572
Iteration 45, loss = 0.64588233
Iteration 46, loss = 0.65142569
Iteration 47, loss = 0.64913206
Iteration 48, loss = 0.63604962
Iteration 49, loss = 0.64941444
Iteration 50, loss = 0.63905361
Iteration 51, loss = 0.63995808
Iteration 52, loss = 0.63719474
Iteration 53, loss = 0.63212076
Iteration 54, loss = 0.62968957
Iteration 55, loss = 0.65157547
Iteration 56, loss = 0.62561270
Iteration 57, loss = 0.62981546
Iteration 58, loss = 0.63640000
Iteration 59, loss = 0.63284810
Iteration 60, loss = 0.63652049
Iteration 61, loss = 0.62734572
Iteration 62, loss = 0.62773245
Iteration 63, loss = 0.63191733
Iteration 64, loss = 0.62765260
Iteration 65, loss = 0.62873486
Iteration 66, loss = 0.62674606
Iteration 67, loss = 0.63263341
Iteration 68, loss = 0.62368697
Iteration 69, loss = 0.61917107
Iteration 70, loss = 0.62013257
Iteration 71, loss = 0.62391677
Iteration 72, loss = 0.61923000
Iteration 73, loss = 0.62140595
Iteration 74, loss = 0.61684523
Iteration 75, loss = 0.62142235
Iteration 76, loss = 0.63091452
Iteration 77, loss = 0.61843513
Iteration 78, loss = 0.62625276
Iteration 79, loss = 0.61354122
Iteration 80, loss = 0.61633103
Iteration 81, loss = 0.62860665
Iteration 82, loss = 0.61411397
Iteration 83, loss = 0.62226163
Iteration 84, loss = 0.61431945
Iteration 85, loss = 0.61914760
Iteration 86, loss = 0.61923522
Iteration 87, loss = 0.61616447
Iteration 88, loss = 0.61244636
Iteration 89, loss = 0.61307997
Iteration 90, loss = 0.61771076
Iteration 91, loss = 0.61103666
Iteration 92, loss = 0.61255932
Iteration 93, loss = 0.61255643
Iteration 94, loss = 0.61254843
Iteration 95, loss = 0.61586522
Iteration 96, loss = 0.61132414
Iteration 97, loss = 0.60934410
Iteration 98, loss = 0.61184774
Iteration 99, loss = 0.60745698
Iteration 100, loss = 0.60857021
Iteration 101, loss = 0.60872447
Iteration 102, loss = 0.60877313
Iteration 103, loss = 0.60928238
Iteration 104, loss = 0.60665694
Iteration 105, loss = 0.60514056
Iteration 106, loss = 0.60613764
Iteration 107, loss = 0.60385031
Iteration 108, loss = 0.59805304
Iteration 109, loss = 0.59036771
Iteration 110, loss = 0.58861443
Iteration 111, loss = 0.58428103
Iteration 112, loss = 0.57382512
Iteration 113, loss = 0.56477374
Iteration 114, loss = 0.55899276
Iteration 115, loss = 0.53774118
Iteration 116, loss = 0.53399315
Iteration 117, loss = 0.51098750
Iteration 118, loss = 0.49582851
Iteration 119, loss = 0.46672731
Iteration 120, loss = 0.45586252
Iteration 121, loss = 0.42619060
Iteration 122, loss = 0.41982742
Iteration 123, loss = 0.40037182
Iteration 124, loss = 0.39260694
Iteration 125, loss = 0.38969066
Iteration 126, loss = 0.53811948
Iteration 127, loss = 0.69548422
Iteration 128, loss = 0.68848383
Iteration 129, loss = 0.68443696
Iteration 130, loss = 0.65392739
Iteration 131, loss = 0.59722216
Iteration 132, loss = 0.55766224
Iteration 133, loss = 0.53077595
Iteration 134, loss = 0.50763623
Iteration 135, loss = 0.48467921
Iteration 136, loss = 0.45985338
Iteration 137, loss = 0.43587643
Iteration 138, loss = 0.39665315
Iteration 139, loss = 0.37104525
Iteration 140, loss = 0.35061673
Iteration 141, loss = 0.33336464
Iteration 142, loss = 0.31256007
Iteration 143, loss = 0.30265071
Iteration 144, loss = 0.31251299
Iteration 145, loss = 0.29543418
Iteration 146, loss = 0.28948126
Iteration 147, loss = 0.29664701
Iteration 148, loss = 0.28424155
Iteration 149, loss = 0.29849748
Iteration 150, loss = 0.28857596
Iteration 151, loss = 0.28869484
Iteration 152, loss = 0.28762368
Iteration 153, loss = 0.28823778
Iteration 154, loss = 0.28839435
Iteration 155, loss = 0.28869045
Iteration 156, loss = 0.28233517
Iteration 157, loss = 0.28785274
Iteration 158, loss = 0.28202263
Iteration 159, loss = 0.29074562
Iteration 160, loss = 0.28410084
Iteration 161, loss = 0.28921944
Iteration 162, loss = 0.28614225
Iteration 163, loss = 0.27998357
Iteration 164, loss = 0.28733660
Iteration 165, loss = 0.28180680
Iteration 166, loss = 0.29082500
Iteration 167, loss = 0.28346857
Iteration 168, loss = 0.28430645
Iteration 169, loss = 0.27877359
Iteration 170, loss = 0.28938706
Iteration 171, loss = 0.28620635
Iteration 172, loss = 0.29025599
Iteration 173, loss = 0.28305280
Iteration 174, loss = 0.28959494
Iteration 175, loss = 0.28460394
Iteration 176, loss = 0.28785330
Iteration 177, loss = 0.28376891
Iteration 178, loss = 0.28457310
Iteration 179, loss = 0.28139586
Iteration 180, loss = 0.28548186
Iteration 181, loss = 0.28559956
Iteration 182, loss = 0.28428083
Iteration 183, loss = 0.28485571
Iteration 184, loss = 0.28076327
Iteration 185, loss = 0.28372730
Iteration 186, loss = 0.28617943
Iteration 187, loss = 0.27720623
Iteration 188, loss = 0.28105737
Iteration 189, loss = 0.28493264
Iteration 190, loss = 0.28206108
Iteration 191, loss = 0.27733046
Iteration 192, loss = 0.28658887
Iteration 193, loss = 0.28138318
Iteration 194, loss = 0.28641904
Iteration 195, loss = 0.27927938
Iteration 196, loss = 0.28195330
Iteration 197, loss = 0.27407428
Iteration 198, loss = 0.28479485
Iteration 199, loss = 0.28119640
Iteration 200, loss = 0.28178255
Iteration 201, loss = 0.28600613
Iteration 202, loss = 0.28021350
Iteration 203, loss = 0.28448549
Iteration 204, loss = 0.27990837
Iteration 205, loss = 0.28444684
Iteration 206, loss = 0.28085596
Iteration 207, loss = 0.27740701
Iteration 208, loss = 0.28437106
Iteration 209, loss = 0.27518656
Iteration 210, loss = 0.28299496
Iteration 211, loss = 0.27920956
Iteration 212, loss = 0.28336708
Iteration 213, loss = 0.28258333
Iteration 214, loss = 0.27743931
Iteration 215, loss = 0.27994974
Iteration 216, loss = 0.27653841
Iteration 217, loss = 0.27763842
Iteration 218, loss = 0.27950787
Iteration 219, loss = 0.27478619
Iteration 220, loss = 0.27760458
Iteration 221, loss = 0.28083487
Iteration 222, loss = 0.28358882
Iteration 223, loss = 0.27725097
Iteration 224, loss = 0.28131182
Iteration 225, loss = 0.27717074
Iteration 226, loss = 0.28885271
Iteration 227, loss = 0.27861870
Iteration 228, loss = 0.28348406
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 5/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(16, 16, 16);, score=0.917 total time= 1.4min
Iteration 1, loss = 3.97765845
Iteration 2, loss = 1.48075557
Iteration 3, loss = 1.53753050
Iteration 4, loss = 1.04605742
Iteration 5, loss = 1.18347607
Iteration 6, loss = 1.03958610
Iteration 7, loss = 1.14886035
Iteration 8, loss = 1.01597114
Iteration 9, loss = 1.06585279
Iteration 10, loss = 0.89278395
Iteration 11, loss = 0.97150880
Iteration 12, loss = 0.96081682
Iteration 13, loss = 0.85982714
Iteration 14, loss = 0.88774750
Iteration 15, loss = 1.02869233
Iteration 16, loss = 0.89009090
Iteration 17, loss = 0.85569506
Iteration 18, loss = 0.86567349
Iteration 19, loss = 0.77838677
Iteration 20, loss = 0.81810469
Iteration 21, loss = 0.89342333
Iteration 22, loss = 0.78866991
Iteration 23, loss = 0.74604103
Iteration 24, loss = 0.91680877
Iteration 25, loss = 0.73140705
Iteration 26, loss = 0.81427685
Iteration 27, loss = 0.71578957
Iteration 28, loss = 0.81298012
Iteration 29, loss = 0.78009350
Iteration 30, loss = 0.73185507
Iteration 31, loss = 0.76649438
Iteration 32, loss = 0.77885456
Iteration 33, loss = 0.75126070
Iteration 34, loss = 0.76577735
Iteration 35, loss = 0.77614920
Iteration 36, loss = 0.77549406
Iteration 37, loss = 0.73824388
Iteration 38, loss = 0.73906280
Iteration 39, loss = 0.73871121
Iteration 40, loss = 0.72430576
Iteration 41, loss = 0.70942933
Iteration 42, loss = 0.72938586
Iteration 43, loss = 0.76319574
Iteration 44, loss = 0.71048485
Iteration 45, loss = 0.71714129
Iteration 46, loss = 0.68328031
Iteration 47, loss = 0.72457023
Iteration 48, loss = 0.73014473
Iteration 49, loss = 0.69575159
Iteration 50, loss = 0.71288005
Iteration 51, loss = 0.70104347
Iteration 52, loss = 0.70664067
Iteration 53, loss = 0.72463648
Iteration 54, loss = 0.68484202
Iteration 55, loss = 0.65932574
Iteration 56, loss = 0.72774012

Iteration 57, loss = 0.67517293
Iteration 58, loss = 0.72099077
Iteration 59, loss = 0.67846758
Iteration 60, loss = 0.69422261
Iteration 61, loss = 0.69974146
Iteration 62, loss = 0.66935055
Iteration 63, loss = 0.68955469
Iteration 64, loss = 0.68827719
Iteration 65, loss = 0.66868220
Iteration 66, loss = 0.66357992
Iteration 67, loss = 0.69468933
Iteration 68, loss = 0.67112335
Iteration 69, loss = 0.67779464
Iteration 70, loss = 0.64910787
Iteration 71, loss = 0.66939460
Iteration 72, loss = 0.67971963
Iteration 73, loss = 0.65110614
Iteration 74, loss = 0.65382492
Iteration 75, loss = 0.65717478
Iteration 76, loss = 0.65178621
Iteration 77, loss = 0.60971556
Iteration 78, loss = 0.54333933
Iteration 79, loss = 0.47700294
Iteration 80, loss = 0.41164621
Iteration 81, loss = 0.39086390
Iteration 82, loss = 0.34129417
Iteration 83, loss = 0.34803913
Iteration 84, loss = 0.31829946
Iteration 85, loss = 0.32156619
Iteration 86, loss = 0.31816198
Iteration 87, loss = 0.34364223
Iteration 88, loss = 0.30613801
Iteration 89, loss = 0.31538163
Iteration 90, loss = 0.30464937
Iteration 91, loss = 0.32256596
Iteration 92, loss = 0.32068719
Iteration 93, loss = 0.31386875
Iteration 94, loss = 0.30986941
Iteration 95, loss = 0.31109290
Iteration 96, loss = 0.31712628
Iteration 97, loss = 0.32307030
Iteration 98, loss = 0.30610570
Iteration 99, loss = 0.30599193
Iteration 100, loss = 0.32177043
Iteration 101, loss = 0.30480949
Iteration 102, loss = 0.30416662
Iteration 103, loss = 0.31915969
Iteration 104, loss = 0.30164504
Iteration 105, loss = 0.30680343
Iteration 106, loss = 0.31312585
Iteration 107, loss = 0.30539269
Iteration 108, loss = 0.30368061
Iteration 109, loss = 0.31240415
Iteration 110, loss = 0.30229236
Iteration 111, loss = 0.30527138
Iteration 112, loss = 0.30844497
Iteration 113, loss = 0.29915817
Iteration 114, loss = 0.30828498
Iteration 115, loss = 0.29230425
Iteration 116, loss = 0.30286060
Iteration 117, loss = 0.29213257
Iteration 118, loss = 0.29525848
Iteration 119, loss = 0.30837431
Iteration 120, loss = 0.29292520
Iteration 121, loss = 0.29687793
Iteration 122, loss = 0.30189884
Iteration 123, loss = 0.29757782
Iteration 124, loss = 0.30428241
Iteration 125, loss = 0.29352060
Iteration 126, loss = 0.30353478
Iteration 127, loss = 0.29687947
Iteration 128, loss = 0.29689742
Iteration 129, loss = 0.29400334
Iteration 130, loss = 0.29869491
Iteration 131, loss = 0.29688746
Iteration 132, loss = 0.29646861
Iteration 133, loss = 0.28811523
Iteration 134, loss = 0.29063680
Iteration 135, loss = 0.29141390
Iteration 136, loss = 0.30131753
Iteration 137, loss = 0.29295514
Iteration 138, loss = 0.28994959
Iteration 139, loss = 0.28982008
Iteration 140, loss = 0.28851459
Iteration 141, loss = 0.29325999
Iteration 142, loss = 0.28823015
Iteration 143, loss = 0.29853543
Iteration 144, loss = 0.28781545
Iteration 145, loss = 0.29818608
Iteration 146, loss = 0.28401886
Iteration 147, loss = 0.30113637
Iteration 148, loss = 0.28329621
Iteration 149, loss = 0.28134947
Iteration 150, loss = 0.28950031
Iteration 151, loss = 0.29792021
Iteration 152, loss = 0.28754497
Iteration 153, loss = 0.30031519
Iteration 154, loss = 0.28257564
Iteration 155, loss = 0.28644619
Iteration 156, loss = 0.27324747
Iteration 157, loss = 0.25341884
Iteration 158, loss = 0.26146746
Iteration 159, loss = 0.23984786
Iteration 160, loss = 0.23065206
Iteration 161, loss = 0.20880455
Iteration 162, loss = 0.19283189
Iteration 163, loss = 0.24004934
Iteration 164, loss = 0.24145017
Iteration 165, loss = 0.24730144
Iteration 166, loss = 0.23190024
Iteration 167, loss = 0.30035649
Iteration 168, loss = 0.27753022
Iteration 169, loss = 0.30757058
Iteration 170, loss = 0.29182086
Iteration 171, loss = 0.28124753
Iteration 172, loss = 0.25828356
Iteration 173, loss = 0.24378218
Iteration 174, loss = 0.25306690
Iteration 175, loss = 0.24145516
Iteration 176, loss = 0.25592013
Iteration 177, loss = 0.23918824
Iteration 178, loss = 0.31816263
Iteration 179, loss = 0.30521156
Iteration 180, loss = 0.28154105
Iteration 181, loss = 0.29822459
Iteration 182, loss = 0.21285366
Iteration 183, loss = 0.25114236
Iteration 184, loss = 0.22827755
Iteration 185, loss = 0.26382278
Iteration 186, loss = 0.22820111
Iteration 187, loss = 0.22097682
Iteration 188, loss = 0.34351399
Iteration 189, loss = 0.28428415
Iteration 190, loss = 0.27777993
Iteration 191, loss = 0.27938431
Iteration 192, loss = 0.28212052
Iteration 193, loss = 0.27938814
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 1/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(24, 24);, score=0.804 total time= 1.2min
Iteration 1, loss = 2.17743786
Iteration 2, loss = 1.07877786
Iteration 3, loss = 0.89277537
Iteration 4, loss = 0.95037226
Iteration 5, loss = 0.84164622
Iteration 6, loss = 0.78302775
Iteration 7, loss = 0.73435261
Iteration 8, loss = 0.87615051
Iteration 9, loss = 0.83816594
Iteration 10, loss = 0.85675067
Iteration 11, loss = 0.78274498
Iteration 12, loss = 0.76003464
Iteration 13, loss = 0.79137032
Iteration 14, loss = 0.76564462
Iteration 15, loss = 0.73424988
Iteration 16, loss = 0.78885510
Iteration 17, loss = 0.71559769
Iteration 18, loss = 0.77012593
Iteration 19, loss = 0.73866851
Iteration 20, loss = 0.73888725
Iteration 21, loss = 0.72896548
Iteration 22, loss = 0.71455217
Iteration 23, loss = 0.68062215
Iteration 24, loss = 0.73362218
Iteration 25, loss = 0.72043115
Iteration 26, loss = 0.69079218
Iteration 27, loss = 0.70831000
Iteration 28, loss = 0.70282494
Iteration 29, loss = 0.66857862
Iteration 30, loss = 0.69477515
Iteration 31, loss = 0.69260591
Iteration 32, loss = 0.68439034
Iteration 33, loss = 0.67262481
Iteration 34, loss = 0.65924322
Iteration 35, loss = 0.68761233
Iteration 36, loss = 0.65797055
Iteration 37, loss = 0.66856228
Iteration 38, loss = 0.66739919
Iteration 39, loss = 0.65262522
Iteration 40, loss = 0.66598182
Iteration 41, loss = 0.64770015
Iteration 42, loss = 0.64709217
Iteration 43, loss = 0.66823669
Iteration 44, loss = 0.65449962
Iteration 45, loss = 0.64388587
Iteration 46, loss = 0.65435750
Iteration 47, loss = 0.64510289
Iteration 48, loss = 0.64639945
Iteration 49, loss = 0.64624704
Iteration 50, loss = 0.64084458
Iteration 51, loss = 0.64447441
Iteration 52, loss = 0.64106051
Iteration 53, loss = 0.64061814
Iteration 54, loss = 0.64102026
Iteration 55, loss = 0.63397794
Iteration 56, loss = 0.63885771
Iteration 57, loss = 0.63392705
Iteration 58, loss = 0.64750192
Iteration 59, loss = 0.63645342
Iteration 60, loss = 0.62584421
Iteration 61, loss = 0.64041533
Iteration 62, loss = 0.63305057
Iteration 63, loss = 0.63160767
Iteration 64, loss = 0.63219990
Iteration 65, loss = 0.63636793
Iteration 66, loss = 0.62385035
Iteration 67, loss = 0.62171735
Iteration 68, loss = 0.62960755
Iteration 69, loss = 0.62860468
Iteration 70, loss = 0.62215223
Iteration 71, loss = 0.62613221
Iteration 72, loss = 0.62391516
Iteration 73, loss = 0.62555877
Iteration 74, loss = 0.62682355
Iteration 75, loss = 0.62657366
Iteration 76, loss = 0.62046600
Iteration 77, loss = 0.62022606
Iteration 78, loss = 0.61988480
Iteration 79, loss = 0.61701797
Iteration 80, loss = 0.62574746
Iteration 81, loss = 0.61570605
Iteration 82, loss = 0.61564992
Iteration 83, loss = 0.61623944
Iteration 84, loss = 0.61919791
Iteration 85, loss = 0.61665496
Iteration 86, loss = 0.61924761
Iteration 87, loss = 0.61173912
Iteration 88, loss = 0.61840146
Iteration 89, loss = 0.61115249
Iteration 90, loss = 0.61378587
Iteration 91, loss = 0.61193298
Iteration 92, loss = 0.61350729
Iteration 93, loss = 0.61059276
Iteration 94, loss = 0.61777250
Iteration 95, loss = 0.61453538
Iteration 96, loss = 0.61168334
Iteration 97, loss = 0.61277967
Iteration 98, loss = 0.60840389
Iteration 99, loss = 0.60971291
Iteration 100, loss = 0.60872248
Iteration 101, loss = 0.60718784
Iteration 102, loss = 0.60954970
Iteration 103, loss = 0.60565900
Iteration 104, loss = 0.60908258
Iteration 105, loss = 0.60708919
Iteration 106, loss = 0.60839942
Iteration 107, loss = 0.60917258
Iteration 108, loss = 0.60654818
Iteration 109, loss = 0.60680763
Iteration 110, loss = 0.60543455

Iteration 111, loss = 0.60564390
Iteration 112, loss = 0.60640514
Iteration 113, loss = 0.60564101
Iteration 114, loss = 0.60917915
Iteration 115, loss = 0.60290709
Iteration 116, loss = 0.60451895
Iteration 117, loss = 0.60664749
Iteration 118, loss = 0.60166234
Iteration 119, loss = 0.60532372
Iteration 120, loss = 0.60251049
Iteration 121, loss = 0.60650313
Iteration 122, loss = 0.60187377
Iteration 123, loss = 0.60266584
Iteration 124, loss = 0.59989550
Iteration 125, loss = 0.59467772
Iteration 126, loss = 0.56559265
Iteration 127, loss = 0.52606526
Iteration 128, loss = 0.48784054
Iteration 129, loss = 0.44777486
Iteration 130, loss = 0.41330849
Iteration 131, loss = 0.38481776
Iteration 132, loss = 0.35214992
Iteration 133, loss = 0.32443788
Iteration 134, loss = 0.30444364
Iteration 135, loss = 0.28759884
Iteration 136, loss = 0.27520663
Iteration 137, loss = 0.26581931
Iteration 138, loss = 0.25727635
Iteration 139, loss = 0.24858405
Iteration 140, loss = 0.23170571
Iteration 141, loss = 0.23341293
Iteration 142, loss = 0.23109379
Iteration 143, loss = 0.22646501
Iteration 144, loss = 0.22398103
Iteration 145, loss = 0.21195004
Iteration 146, loss = 0.20915707
Iteration 147, loss = 0.22543101
Iteration 148, loss = 0.20394539
Iteration 149, loss = 0.20556945
Iteration 150, loss = 0.20088406
Iteration 151, loss = 0.19592134
Iteration 152, loss = 0.19771727
Iteration 153, loss = 0.18974760
Iteration 154, loss = 0.19535625
Iteration 155, loss = 0.19527385
Iteration 156, loss = 0.18465702
Iteration 157, loss = 0.19121928
Iteration 158, loss = 0.19019135
Iteration 159, loss = 0.19255488
Iteration 160, loss = 0.17676245
Iteration 161, loss = 0.18786094
Iteration 162, loss = 0.19178148
Iteration 163, loss = 0.18207300
Iteration 164, loss = 0.17437175
Iteration 165, loss = 0.19029431
Iteration 166, loss = 0.17433232
Iteration 167, loss = 0.17935790
Iteration 168, loss = 0.17735398
Iteration 169, loss = 0.17465403
Iteration 170, loss = 0.17321898
Iteration 171, loss = 0.18191316
Iteration 172, loss = 0.17957872
Iteration 173, loss = 0.17702124
Iteration 174, loss = 0.16719436
Iteration 175, loss = 0.17884109
Iteration 176, loss = 0.16857423
Iteration 177, loss = 0.18146175
Iteration 178, loss = 0.17814218
Iteration 179, loss = 0.16991022
Iteration 180, loss = 0.16320997
Iteration 181, loss = 0.16783793
Iteration 182, loss = 0.18321343
Iteration 183, loss = 0.16708293
Iteration 184, loss = 0.18042890
Iteration 185, loss = 0.16423778
Iteration 186, loss = 0.15743569
Iteration 187, loss = 0.17700005
Iteration 188, loss = 0.16163971
Iteration 189, loss = 0.16644623
Iteration 190, loss = 0.16452126
Iteration 191, loss = 0.15645482
Iteration 192, loss = 0.17109702
Iteration 193, loss = 0.16549251
Iteration 194, loss = 0.16558015
Iteration 195, loss = 0.16116548
Iteration 196, loss = 0.16997799
Iteration 197, loss = 0.16145304
Iteration 198, loss = 0.16733869
Iteration 199, loss = 0.15694951
Iteration 200, loss = 0.15453458
Iteration 201, loss = 0.16887209
Iteration 202, loss = 0.17521388
Iteration 203, loss = 0.14809744
Iteration 204, loss = 0.15543161
Iteration 205, loss = 0.16650235
Iteration 206, loss = 0.15693209
Iteration 207, loss = 0.14774890
Iteration 208, loss = 0.16215449
Iteration 209, loss = 0.15251973
Iteration 210, loss = 0.16496113
Iteration 211, loss = 0.14201561
Iteration 212, loss = 0.15734729
Iteration 213, loss = 0.16915973
Iteration 214, loss = 0.14536546
Iteration 215, loss = 0.15053209
Iteration 216, loss = 0.16502726
Iteration 217, loss = 0.14898759
Iteration 218, loss = 0.15155552
Iteration 219, loss = 0.32591046
Iteration 220, loss = 0.28025016
Iteration 221, loss = 0.26122269
Iteration 222, loss = 0.25196321
Iteration 223, loss = 0.24534496
Iteration 224, loss = 0.24951840
Iteration 225, loss = 0.22252677
Iteration 226, loss = 0.23826423
Iteration 227, loss = 0.23621521
Iteration 228, loss = 0.21243197
Iteration 229, loss = 0.19855690
Iteration 230, loss = 0.20142289
Iteration 231, loss = 0.18525173
Iteration 232, loss = 0.18428837
Iteration 233, loss = 0.17103628
Iteration 234, loss = 0.17160340
Iteration 235, loss = 0.17084928
Iteration 236, loss = 0.16028226
Iteration 237, loss = 0.16393491
Iteration 238, loss = 0.15742194
Iteration 239, loss = 0.15327641
Iteration 240, loss = 0.14725815
Iteration 241, loss = 0.15147706
Iteration 242, loss = 0.14894937
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 2/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(24, 24);, score=0.921 total time= 1.4min
Iteration 1, loss = 3.75665038
Iteration 2, loss = 0.80623303
Iteration 3, loss = 0.74729527
Iteration 4, loss = 0.70967912
Iteration 5, loss = 0.68632235
Iteration 6, loss = 0.69438838
Iteration 7, loss = 0.71243990
Iteration 8, loss = 0.66747506
Iteration 9, loss = 0.70424381
Iteration 10, loss = 0.69644159
Iteration 11, loss = 0.67461526
Iteration 12, loss = 0.69130955
Iteration 13, loss = 0.66370212
Iteration 14, loss = 0.67698746
Iteration 15, loss = 0.68107704
Iteration 16, loss = 0.66401520
Iteration 17, loss = 0.68695532
Iteration 18, loss = 0.67539439
Iteration 19, loss = 0.66392087
Iteration 20, loss = 0.67772245
Iteration 21, loss = 0.67980622
Iteration 22, loss = 0.68038318
Iteration 23, loss = 0.66565085
Iteration 24, loss = 0.65466346
Iteration 25, loss = 0.67068742
Iteration 26, loss = 0.68957084
Iteration 27, loss = 0.67298759
Iteration 28, loss = 0.65396193
Iteration 29, loss = 0.68156412
Iteration 30, loss = 0.65008141
Iteration 31, loss = 0.66041530
Iteration 32, loss = 0.64924723
Iteration 33, loss = 0.65269494
Iteration 34, loss = 0.68083616
Iteration 35, loss = 0.65179634
Iteration 36, loss = 0.63946471
Iteration 37, loss = 0.65399616
Iteration 38, loss = 0.65591114
Iteration 39, loss = 0.66177230
Iteration 40, loss = 0.64978046
Iteration 41, loss = 0.65586556
Iteration 42, loss = 0.64715878
Iteration 43, loss = 0.66100824
Iteration 44, loss = 0.64669420
Iteration 45, loss = 0.65462973
Iteration 46, loss = 0.66424809
Iteration 47, loss = 0.65452815
Iteration 48, loss = 0.64085837
Iteration 49, loss = 0.65317986
Iteration 50, loss = 0.63659841
Iteration 51, loss = 0.65000402
Iteration 52, loss = 0.64017786
Iteration 53, loss = 0.64333424
Iteration 54, loss = 0.65050116
Iteration 55, loss = 0.64904134
Iteration 56, loss = 0.65709299
Iteration 57, loss = 0.63879482
Iteration 58, loss = 0.63658336
Iteration 59, loss = 0.63572214
Iteration 60, loss = 0.63774153
Iteration 61, loss = 0.64532081
Iteration 62, loss = 0.63493709
Iteration 63, loss = 0.64264053
Iteration 64, loss = 0.64063622
Iteration 65, loss = 0.64625803
Iteration 66, loss = 0.62633775
Iteration 67, loss = 0.64235815
Iteration 68, loss = 0.64145413
Iteration 69, loss = 0.63095905
Iteration 70, loss = 0.63296496
Iteration 71, loss = 0.63554109
Iteration 72, loss = 0.63308816
Iteration 73, loss = 0.63723830
Iteration 74, loss = 0.62832269
Iteration 75, loss = 0.64278983
Iteration 76, loss = 0.63095585
Iteration 77, loss = 0.63128776
Iteration 78, loss = 0.62369073
Iteration 79, loss = 0.62367195
Iteration 80, loss = 0.63747487
Iteration 81, loss = 0.63088271
Iteration 82, loss = 0.61641978
Iteration 83, loss = 0.63780540
Iteration 84, loss = 0.62497165
Iteration 85, loss = 0.62723876
Iteration 86, loss = 0.62558470
Iteration 87, loss = 0.62477800
Iteration 88, loss = 0.63192859
Iteration 89, loss = 0.62400001
Iteration 90, loss = 0.62222941
Iteration 91, loss = 0.62899472
Iteration 92, loss = 0.62830302
Iteration 93, loss = 0.62092440
Iteration 94, loss = 0.62287659
Iteration 95, loss = 0.61978244
Iteration 96, loss = 0.62570187
Iteration 97, loss = 0.61892969
Iteration 98, loss = 0.62174985
Iteration 99, loss = 0.61743413
Iteration 100, loss = 0.62449652
Iteration 101, loss = 0.62283474
Iteration 102, loss = 0.62048225
Iteration 103, loss = 0.62145353
Iteration 104, loss = 0.61553501
Iteration 105, loss = 0.62204964
Iteration 106, loss = 0.61789192
Iteration 107, loss = 0.62333058
Iteration 108, loss = 0.61539576
Iteration 109, loss = 0.61627766
Iteration 110, loss = 0.61969880
Iteration 111, loss = 0.61731833
Iteration 112, loss = 0.61832170
Iteration 113, loss = 0.61263659
Iteration 114, loss = 0.61254373

Iteration 115, loss = 0.61456577
Iteration 116, loss = 0.61377957
Iteration 117, loss = 0.61843767
Iteration 118, loss = 0.62002291
Iteration 119, loss = 0.61587346
Iteration 120, loss = 0.61292055
Iteration 121, loss = 0.61493865
Iteration 122, loss = 0.61240193
Iteration 123, loss = 0.61415236
Iteration 124, loss = 0.61798572
Iteration 125, loss = 0.61246618
Iteration 126, loss = 0.61230898
Iteration 127, loss = 0.61519789
Iteration 128, loss = 0.61091076
Iteration 129, loss = 0.61183686
Iteration 130, loss = 0.61149415
Iteration 131, loss = 0.61587571
Iteration 132, loss = 0.61030686
Iteration 133, loss = 0.61543132
Iteration 134, loss = 0.60839208
Iteration 135, loss = 0.61034605
Iteration 136, loss = 0.61007444
Iteration 137, loss = 0.61130276
Iteration 138, loss = 0.61238620
Iteration 139, loss = 0.60768312
Iteration 140, loss = 0.61095626
Iteration 141, loss = 0.61196682
Iteration 142, loss = 0.60924249
Iteration 143, loss = 0.60804459
Iteration 144, loss = 0.60850103
Iteration 145, loss = 0.60843991
Iteration 146, loss = 0.60790105
Iteration 147, loss = 0.60983968
Iteration 148, loss = 0.60784417
Iteration 149, loss = 0.60883809
Iteration 150, loss = 0.60862685
Iteration 151, loss = 0.60650643
Iteration 152, loss = 0.60720614
Iteration 153, loss = 0.61009596
Iteration 154, loss = 0.60626589
Iteration 155, loss = 0.60736340
Iteration 156, loss = 0.60786201
Iteration 157, loss = 0.60698264
Iteration 158, loss = 0.60662725
Iteration 159, loss = 0.60616899
Iteration 160, loss = 0.60547676
Iteration 161, loss = 0.60695306
Iteration 162, loss = 0.60659856
Iteration 163, loss = 0.60678392
Iteration 164, loss = 0.60761642
Iteration 165, loss = 0.60576504
Iteration 166, loss = 0.60480073
Iteration 167, loss = 0.60692182
Iteration 168, loss = 0.60463851
Iteration 169, loss = 0.60390908
Iteration 170, loss = 0.60779818
Iteration 171, loss = 0.60700419
Iteration 172, loss = 0.60371979
Iteration 173, loss = 0.60465746
Iteration 174, loss = 0.60556453
Iteration 175, loss = 0.60488716
Iteration 176, loss = 0.60485793
Iteration 177, loss = 0.60483992
Iteration 178, loss = 0.60439642
Iteration 179, loss = 0.60526989
Iteration 180, loss = 0.60390717
Iteration 181, loss = 0.60458136
Iteration 182, loss = 0.60608529
Iteration 183, loss = 0.60441518
Iteration 184, loss = 0.60538583
Iteration 185, loss = 0.60281613
Iteration 186, loss = 0.60343243
Iteration 187, loss = 0.60321878
Iteration 188, loss = 0.60300315
Iteration 189, loss = 0.60352143
Iteration 190, loss = 0.60404532
Iteration 191, loss = 0.60372801
Iteration 192, loss = 0.60533834
Iteration 193, loss = 0.60379299
Iteration 194, loss = 0.60247272
Iteration 195, loss = 0.60557637
Iteration 196, loss = 0.60345623
Iteration 197, loss = 0.60314670
Iteration 198, loss = 0.60417415
Iteration 199, loss = 0.60223608
Iteration 200, loss = 0.60340905
Iteration 201, loss = 0.60268294
Iteration 202, loss = 0.60152122
Iteration 203, loss = 0.60310795
Iteration 204, loss = 0.60162497
Iteration 205, loss = 0.60360800
Iteration 206, loss = 0.60224758
Iteration 207, loss = 0.60245313
Iteration 208, loss = 0.60232385
Iteration 209, loss = 0.60203171
Iteration 210, loss = 0.60285675
Iteration 211, loss = 0.60303730
Iteration 212, loss = 0.60211454
Iteration 213, loss = 0.60307628
Iteration 214, loss = 0.60260038
Iteration 215, loss = 0.60248101
Iteration 216, loss = 0.60326603
Iteration 217, loss = 0.60112956
Iteration 218, loss = 0.60072088
Iteration 219, loss = 0.60161389
Iteration 220, loss = 0.60126547
Iteration 221, loss = 0.60163588
Iteration 222, loss = 0.60277845
Iteration 223, loss = 0.60244581
Iteration 224, loss = 0.60199155
Iteration 225, loss = 0.60230626
Iteration 226, loss = 0.60145199
Iteration 227, loss = 0.60283039
Iteration 228, loss = 0.60196600
Iteration 229, loss = 0.60084306
Iteration 230, loss = 0.60050911
Iteration 231, loss = 0.60128550
Iteration 232, loss = 0.60069223
Iteration 233, loss = 0.60169361
Iteration 234, loss = 0.60223666
Iteration 235, loss = 0.60074047
Iteration 236, loss = 0.60154157
Iteration 237, loss = 0.60220954
Iteration 238, loss = 0.60166187
Iteration 239, loss = 0.60355084
Iteration 240, loss = 0.60147824
Iteration 241, loss = 0.60166136
Iteration 242, loss = 0.60043452
Iteration 243, loss = 0.59984809
Iteration 244, loss = 0.60090201
Iteration 245, loss = 0.60164371
Iteration 246, loss = 0.60076147
Iteration 247, loss = 0.60078419
Iteration 248, loss = 0.60105482
Iteration 249, loss = 0.60085505
Iteration 250, loss = 0.60169280
Iteration 251, loss = 0.60029341
Iteration 252, loss = 0.60082914
Iteration 253, loss = 0.60103563
Iteration 254, loss = 0.60000767
Iteration 255, loss = 0.60034942
Iteration 256, loss = 0.60105750
Iteration 257, loss = 0.60084942
Iteration 258, loss = 0.60140584
Iteration 259, loss = 0.60174396
Iteration 260, loss = 0.60109835
Iteration 261, loss = 0.59975649
Iteration 262, loss = 0.59953525
Iteration 263, loss = 0.60038084
Iteration 264, loss = 0.60006264
Iteration 265, loss = 0.59970963
Iteration 266, loss = 0.60127022
Iteration 267, loss = 0.59885669
Iteration 268, loss = 0.59939635
Iteration 269, loss = 0.60073976
Iteration 270, loss = 0.59908533
Iteration 271, loss = 0.59800274
Iteration 272, loss = 0.59957027
Iteration 273, loss = 0.60062650
Iteration 274, loss = 0.59783067
Iteration 275, loss = 0.59822575
Iteration 276, loss = 0.59961976
Iteration 277, loss = 0.59844435
Iteration 278, loss = 0.59799336
Iteration 279, loss = 0.59880516
Iteration 280, loss = 0.59848795
Iteration 281, loss = 0.59751780
Iteration 282, loss = 0.59925248
Iteration 283, loss = 0.59785167
Iteration 284, loss = 0.59757779
Iteration 285, loss = 0.59697051
Iteration 286, loss = 0.59830796
Iteration 287, loss = 0.59715669
Iteration 288, loss = 0.59977337
Iteration 289, loss = 0.59983879
Iteration 290, loss = 0.59665113
Iteration 291, loss = 0.59782008
Iteration 292, loss = 0.59779508
Iteration 293, loss = 0.59653526
Iteration 294, loss = 0.59728143
Iteration 295, loss = 0.59638470
Iteration 296, loss = 0.59663941
Iteration 297, loss = 0.59704034
Iteration 298, loss = 0.59665227
Iteration 299, loss = 0.59579230
Iteration 300, loss = 0.59577905
Iteration 301, loss = 0.59673419
Iteration 302, loss = 0.59687350
Iteration 303, loss = 0.59622936
Iteration 304, loss = 0.59618122
Iteration 305, loss = 0.59552310
Iteration 306, loss = 0.59486721
Iteration 307, loss = 0.59560366
Iteration 308, loss = 0.59532856
Iteration 309, loss = 0.59443464
Iteration 310, loss = 0.59467245
Iteration 311, loss = 0.59329225
Iteration 312, loss = 0.59559171
Iteration 313, loss = 0.59354453
Iteration 314, loss = 0.59183911
Iteration 315, loss = 0.59264836
Iteration 316, loss = 0.58698389
Iteration 317, loss = 0.58376939
Iteration 318, loss = 0.57435791
Iteration 319, loss = 0.56781260
Iteration 320, loss = 0.55278556
Iteration 321, loss = 0.54144977
Iteration 322, loss = 0.52516403
Iteration 323, loss = 0.51038614
Iteration 324, loss = 0.49965206
Iteration 325, loss = 0.49011929
Iteration 326, loss = 0.48062745
Iteration 327, loss = 0.47399165
Iteration 328, loss = 0.46696531
Iteration 329, loss = 0.45958571
Iteration 330, loss = 0.45486312
Iteration 331, loss = 0.45280256
Iteration 332, loss = 0.44574510
Iteration 333, loss = 0.44156579
Iteration 334, loss = 0.43845868
Iteration 335, loss = 0.43388979
Iteration 336, loss = 0.43459932
Iteration 337, loss = 0.43385207
Iteration 338, loss = 0.42198448
Iteration 339, loss = 0.41248665
Iteration 340, loss = 0.40444509
Iteration 341, loss = 0.38676506
Iteration 342, loss = 0.36711952
Iteration 343, loss = 0.32740011
Iteration 344, loss = 0.30699121
Iteration 345, loss = 0.29194744
Iteration 346, loss = 0.28418341
Iteration 347, loss = 0.27861201
Iteration 348, loss = 0.28355595
Iteration 349, loss = 0.27726140
Iteration 350, loss = 0.27944037
Iteration 351, loss = 0.27636604
Iteration 352, loss = 0.27943119
Iteration 353, loss = 0.27401144
Iteration 354, loss = 0.27835830
Iteration 355, loss = 0.27606909
Iteration 356, loss = 0.27675163
Iteration 357, loss = 0.27443309
Iteration 358, loss = 0.27807168
Iteration 359, loss = 0.27561775
Iteration 360, loss = 0.27618738
Iteration 361, loss = 0.27227329
Iteration 362, loss = 0.27990910
Iteration 363, loss = 0.27406390
Iteration 364, loss = 0.27333982

Iteration 365, loss = 0.27458197
Iteration 366, loss = 0.28243651
Iteration 367, loss = 0.27632329
Iteration 368, loss = 0.27174034
Iteration 369, loss = 0.27815944
Iteration 370, loss = 0.27471254
Iteration 371, loss = 0.27463907
Iteration 372, loss = 0.27278632
Iteration 373, loss = 0.27141598
Iteration 374, loss = 0.27106437
Iteration 375, loss = 0.27006659
Iteration 376, loss = 0.27491915
Iteration 377, loss = 0.27302852
Iteration 378, loss = 0.27228035
Iteration 379, loss = 0.27404175
Iteration 380, loss = 0.26913423
Iteration 381, loss = 0.27158397
Iteration 382, loss = 0.27558787
Iteration 383, loss = 0.27186836
Iteration 384, loss = 0.27194683
Iteration 385, loss = 0.27272061
Iteration 386, loss = 0.26756205
Iteration 387, loss = 0.27332433
Iteration 388, loss = 0.27380145
Iteration 389, loss = 0.27251848
Iteration 390, loss = 0.26904386
Iteration 391, loss = 0.27454718
Iteration 392, loss = 0.27097959
Iteration 393, loss = 0.27041527
Iteration 394, loss = 0.27124347
Iteration 395, loss = 0.27175490
Iteration 396, loss = 0.27208330
Iteration 397, loss = 0.27190297
Iteration 398, loss = 0.27223202
Iteration 399, loss = 0.26962507
Iteration 400, loss = 0.27071577
Iteration 401, loss = 0.27066606
Iteration 402, loss = 0.27261369
Iteration 403, loss = 0.27174890
Iteration 404, loss = 0.27019633
Iteration 405, loss = 0.27831571
Iteration 406, loss = 0.26863791
Iteration 407, loss = 0.27382187
Iteration 408, loss = 0.26839541
Iteration 409, loss = 0.27097688
Iteration 410, loss = 0.26879079
Iteration 411, loss = 0.26767491
Iteration 412, loss = 0.27596564
Iteration 413, loss = 0.26595723
Iteration 414, loss = 0.26645681
Iteration 415, loss = 0.27132603
Iteration 416, loss = 0.26721091
Iteration 417, loss = 0.26894536
Iteration 418, loss = 0.26684072
Iteration 419, loss = 0.27024845
Iteration 420, loss = 0.26862462
Iteration 421, loss = 0.26762569
Iteration 422, loss = 0.26435270
Iteration 423, loss = 0.26337530
Iteration 424, loss = 0.27394142
Iteration 425, loss = 0.26589136
Iteration 426, loss = 0.26390003
Iteration 427, loss = 0.26566592
Iteration 428, loss = 0.26366160
Iteration 429, loss = 0.26190917
Iteration 430, loss = 0.26711053
Iteration 431, loss = 0.26455485
Iteration 432, loss = 0.26087261
Iteration 433, loss = 0.26198039
Iteration 434, loss = 0.26392565
Iteration 435, loss = 0.26115104
Iteration 436, loss = 0.24595800
Iteration 437, loss = 0.23683236
Iteration 438, loss = 0.23808946
Iteration 439, loss = 0.22529254
Iteration 440, loss = 0.22244053
Iteration 441, loss = 0.21726705
Iteration 442, loss = 0.20962681
Iteration 443, loss = 0.20622899
Iteration 444, loss = 0.20118577
Iteration 445, loss = 0.20363885
Iteration 446, loss = 0.19402799
Iteration 447, loss = 0.19055743
Iteration 448, loss = 0.19008345
Iteration 449, loss = 0.18603536
Iteration 450, loss = 0.18615618
Iteration 451, loss = 0.18286953
Iteration 452, loss = 0.18733182
Iteration 453, loss = 0.17933313
Iteration 454, loss = 0.17631618
Iteration 455, loss = 0.17766886
Iteration 456, loss = 0.17374341
Iteration 457, loss = 0.18184642
Iteration 458, loss = 0.16859139
Iteration 459, loss = 0.17364059
Iteration 460, loss = 0.17620093
Iteration 461, loss = 0.16619155
Iteration 462, loss = 0.16906906
Iteration 463, loss = 0.17074615
Iteration 464, loss = 0.16969556
Iteration 465, loss = 0.16971690
Iteration 466, loss = 0.16383537
Iteration 467, loss = 0.16698209
Iteration 468, loss = 0.16406599
Iteration 469, loss = 0.16534999
Iteration 470, loss = 0.16685047
Iteration 471, loss = 0.16803923
Iteration 472, loss = 0.15968809
Iteration 473, loss = 0.16294379
Iteration 474, loss = 0.15767584
Iteration 475, loss = 0.16666642
Iteration 476, loss = 0.16185181
Iteration 477, loss = 0.15842002
Iteration 478, loss = 0.16058481
Iteration 479, loss = 0.16179848
Iteration 480, loss = 0.15944729
Iteration 481, loss = 0.15650678
Iteration 482, loss = 0.15412836
Iteration 483, loss = 0.20202763
Iteration 484, loss = 0.16156010
Iteration 485, loss = 0.17234536
Iteration 486, loss = 0.15989966
Iteration 487, loss = 0.18268539
Iteration 488, loss = 0.15955092
Iteration 489, loss = 0.16186270
Iteration 490, loss = 0.16560265
Iteration 491, loss = 0.16769017
Iteration 492, loss = 0.16487916
Iteration 493, loss = 0.15911221
Iteration 494, loss = 0.17950536
Iteration 495, loss = 0.16652272
Iteration 496, loss = 0.15870820
Iteration 497, loss = 0.16029529
Iteration 498, loss = 0.16356184
Iteration 499, loss = 0.16277732
Iteration 500, loss = 0.17157627
Iteration 501, loss = 0.15402917
Iteration 502, loss = 0.18889704
Iteration 503, loss = 0.15265285
Iteration 504, loss = 0.15761021
Iteration 505, loss = 0.15822839
Iteration 506, loss = 0.15486897
Iteration 507, loss = 0.18082927
Iteration 508, loss = 0.15392144
Iteration 509, loss = 0.16775655
Iteration 510, loss = 0.15275633
Iteration 511, loss = 0.15986771
Iteration 512, loss = 0.17930163
Iteration 513, loss = 0.14897445
Iteration 514, loss = 0.19019891
Iteration 515, loss = 0.14832033
Iteration 516, loss = 0.15112756
Iteration 517, loss = 0.15021544
Iteration 518, loss = 0.15843724
Iteration 519, loss = 0.15985962
Iteration 520, loss = 0.15371026
Iteration 521, loss = 0.15420737
Iteration 522, loss = 0.15822690
Iteration 523, loss = 0.15190486
Iteration 524, loss = 0.15724844
Iteration 525, loss = 0.14804570
Iteration 526, loss = 0.15247225
Iteration 527, loss = 0.16120163
Iteration 528, loss = 0.81028562
Iteration 529, loss = 0.68679675
Iteration 530, loss = 0.54522507
Iteration 531, loss = 0.29062837
Iteration 532, loss = 0.18909870
Iteration 533, loss = 0.17653033
Iteration 534, loss = 0.15167182
Iteration 535, loss = 0.15935388
Iteration 536, loss = 0.13732831
Iteration 537, loss = 0.14735062
Iteration 538, loss = 0.13403143
Iteration 539, loss = 0.13021171
Iteration 540, loss = 0.14700728
Iteration 541, loss = 0.13177949
Iteration 542, loss = 0.13831545
Iteration 543, loss = 0.13056180
Iteration 544, loss = 0.13914109
Iteration 545, loss = 0.12804249
Iteration 546, loss = 0.13456795
Iteration 547, loss = 0.13176477
Iteration 548, loss = 0.13085135
Iteration 549, loss = 0.13497092
Iteration 550, loss = 0.15123714
Iteration 551, loss = 0.12630493
Iteration 552, loss = 0.13939523
Iteration 553, loss = 0.14202641
Iteration 554, loss = 0.12588899
Iteration 555, loss = 0.12929645
Iteration 556, loss = 0.12798526
Iteration 557, loss = 0.15321250
Iteration 558, loss = 0.12546593
Iteration 559, loss = 0.13154647
Iteration 560, loss = 0.13813344
Iteration 561, loss = 0.12812015
Iteration 562, loss = 0.13104151
Iteration 563, loss = 0.13434966
Iteration 564, loss = 0.12671929
Iteration 565, loss = 0.13427232
Iteration 566, loss = 0.13511004
Iteration 567, loss = 0.12920747
Iteration 568, loss = 0.14044359
Iteration 569, loss = 0.13369991
Iteration 570, loss = 0.13058453
Iteration 571, loss = 0.14908764
Iteration 572, loss = 0.13317489
Iteration 573, loss = 0.13483851
Iteration 574, loss = 0.14042641
Iteration 575, loss = 0.13181749
Iteration 576, loss = 0.12919748
Iteration 577, loss = 0.12684977
Iteration 578, loss = 0.12601641
Iteration 579, loss = 0.16394355
Iteration 580, loss = 0.12345456
Iteration 581, loss = 0.13725814
Iteration 582, loss = 0.12725376
Iteration 583, loss = 0.14377057
Iteration 584, loss = 0.12524543
Iteration 585, loss = 0.14252606
Iteration 586, loss = 0.13231721
Iteration 587, loss = 0.12456678
Iteration 588, loss = 0.15705181
Iteration 589, loss = 0.12328637
Iteration 590, loss = 0.14687939
Iteration 591, loss = 0.13052495
Iteration 592, loss = 0.13329034
Iteration 593, loss = 0.28198867
Iteration 594, loss = 0.50678714
Iteration 595, loss = 0.14900502
Iteration 596, loss = 0.12713686
Iteration 597, loss = 0.18545248
Iteration 598, loss = 0.12776799
Iteration 599, loss = 0.14138007
Iteration 600, loss = 0.12965123
Iteration 601, loss = 0.12653258
Iteration 602, loss = 0.13624346
Iteration 603, loss = 0.14493345
Iteration 604, loss = 0.18616726
Iteration 605, loss = 0.12189480
Iteration 606, loss = 0.13911771
Iteration 607, loss = 0.12977040
Iteration 608, loss = 0.13405776
Iteration 609, loss = 0.12128272
Iteration 610, loss = 0.16513346
Iteration 611, loss = 0.20994265
Iteration 612, loss = 0.12626570
Iteration 613, loss = 0.13882595
Iteration 614, loss = 0.12245669

Iteration 615, loss = 0.14059750
Iteration 616, loss = 0.12421583
Iteration 617, loss = 0.12821070
Iteration 618, loss = 0.13085461
Iteration 619, loss = 0.13689687
Iteration 620, loss = 0.14088795
Iteration 621, loss = 0.12647874
Iteration 622, loss = 0.12342699
Iteration 623, loss = 0.14131369
Iteration 624, loss = 0.12468822
Iteration 625, loss = 0.13556184
Iteration 626, loss = 0.12248426
Iteration 627, loss = 0.28638711
Iteration 628, loss = 0.14888465
Iteration 629, loss = 0.12313399
Iteration 630, loss = 0.15658999
Iteration 631, loss = 0.12326384
Iteration 632, loss = 0.13270307
Iteration 633, loss = 0.12838124
Iteration 634, loss = 0.18010483
Iteration 635, loss = 0.12316924
Iteration 636, loss = 0.12779161
Iteration 637, loss = 0.15838380
Iteration 638, loss = 0.12424499
Iteration 639, loss = 0.14328729
Iteration 640, loss = 0.13593315
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 3/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(24, 24);, score=0.886 total time= 2.6min
Iteration 1, loss = 3.22723385
Iteration 2, loss = 0.95214856
Iteration 3, loss = 0.85169086
Iteration 4, loss = 0.84470959
Iteration 5, loss = 0.80352745
Iteration 6, loss = 0.78539224
Iteration 7, loss = 0.89021799
Iteration 8, loss = 0.75716966
Iteration 9, loss = 0.77756142
Iteration 10, loss = 0.68722080
Iteration 11, loss = 0.83287642
Iteration 12, loss = 0.69282602
Iteration 13, loss = 0.78470632
Iteration 14, loss = 0.70470065
Iteration 15, loss = 0.76304066
Iteration 16, loss = 0.71780468
Iteration 17, loss = 0.70368060
Iteration 18, loss = 0.67197544
Iteration 19, loss = 0.70787753
Iteration 20, loss = 0.71179868
Iteration 21, loss = 0.70793295
Iteration 22, loss = 0.67401502
Iteration 23, loss = 0.68808209
Iteration 24, loss = 0.70251398
Iteration 25, loss = 0.69412391
Iteration 26, loss = 0.71900579
Iteration 27, loss = 0.71680492
Iteration 28, loss = 0.68037706
Iteration 29, loss = 0.66729510
Iteration 30, loss = 0.68418953
Iteration 31, loss = 0.70892279
Iteration 32, loss = 0.66255854
Iteration 33, loss = 0.68870274
Iteration 34, loss = 0.68408039
Iteration 35, loss = 0.65845157
Iteration 36, loss = 0.67524420
Iteration 37, loss = 0.65837473
Iteration 38, loss = 0.68262338
Iteration 39, loss = 0.67229080
Iteration 40, loss = 0.64364605
Iteration 41, loss = 0.67542984
Iteration 42, loss = 0.67554691
Iteration 43, loss = 0.65453766
Iteration 44, loss = 0.66273431
Iteration 45, loss = 0.64914083
Iteration 46, loss = 0.67316979
Iteration 47, loss = 0.66219078
Iteration 48, loss = 0.67456142
Iteration 49, loss = 0.66489755
Iteration 50, loss = 0.65089232
Iteration 51, loss = 0.66540091
Iteration 52, loss = 0.64239964
Iteration 53, loss = 0.65434624
Iteration 54, loss = 0.64818134
Iteration 55, loss = 0.65619910
Iteration 56, loss = 0.65793842
Iteration 57, loss = 0.64771228
Iteration 58, loss = 0.64649624
Iteration 59, loss = 0.64532599
Iteration 60, loss = 0.64349134
Iteration 61, loss = 0.64812177
Iteration 62, loss = 0.64336369
Iteration 63, loss = 0.65325581
Iteration 64, loss = 0.63836774
Iteration 65, loss = 0.64205701
Iteration 66, loss = 0.65595981
Iteration 67, loss = 0.64037329
Iteration 68, loss = 0.63576349
Iteration 69, loss = 0.63890494
Iteration 70, loss = 0.64072962
Iteration 71, loss = 0.63666728
Iteration 72, loss = 0.63597560
Iteration 73, loss = 0.64350201
Iteration 74, loss = 0.63243658
Iteration 75, loss = 0.63962237
Iteration 76, loss = 0.63699762
Iteration 77, loss = 0.63355533
Iteration 78, loss = 0.64051386
Iteration 79, loss = 0.62906704
Iteration 80, loss = 0.62431225
Iteration 81, loss = 0.63572400
Iteration 82, loss = 0.63877862
Iteration 83, loss = 0.63416684
Iteration 84, loss = 0.62990235
Iteration 85, loss = 0.63063264
Iteration 86, loss = 0.63515998
Iteration 87, loss = 0.62489527
Iteration 88, loss = 0.62176176
Iteration 89, loss = 0.63142565
Iteration 90, loss = 0.62392078
Iteration 91, loss = 0.63163351
Iteration 92, loss = 0.62041322
Iteration 93, loss = 0.62867298
Iteration 94, loss = 0.63056539
Iteration 95, loss = 0.62092226
Iteration 96, loss = 0.62038471
Iteration 97, loss = 0.62428510
Iteration 98, loss = 0.61921069
Iteration 99, loss = 0.62637085
Iteration 100, loss = 0.62432314
Iteration 101, loss = 0.61675770
Iteration 102, loss = 0.62221502
Iteration 103, loss = 0.61232751
Iteration 104, loss = 0.62074501
Iteration 105, loss = 0.61842752
Iteration 106, loss = 0.62173020
Iteration 107, loss = 0.61385352
Iteration 108, loss = 0.61896166
Iteration 109, loss = 0.61873172
Iteration 110, loss = 0.61580342
Iteration 111, loss = 0.61538138
Iteration 112, loss = 0.61764416
Iteration 113, loss = 0.61300295
Iteration 114, loss = 0.61995852
Iteration 115, loss = 0.61724518
Iteration 116, loss = 0.61629843
Iteration 117, loss = 0.61123645
Iteration 118, loss = 0.61217494
Iteration 119, loss = 0.61260240
Iteration 120, loss = 0.61172800
Iteration 121, loss = 0.61224212
Iteration 122, loss = 0.60776049
Iteration 123, loss = 0.61213921
Iteration 124, loss = 0.61670243
Iteration 125, loss = 0.61064669
Iteration 126, loss = 0.61034239
Iteration 127, loss = 0.60926992
Iteration 128, loss = 0.60921997
Iteration 129, loss = 0.61249604
Iteration 130, loss = 0.61113188
Iteration 131, loss = 0.61172839
Iteration 132, loss = 0.60687198
Iteration 133, loss = 0.61006696
Iteration 134, loss = 0.61038083
Iteration 135, loss = 0.60799031
Iteration 136, loss = 0.60546562
Iteration 137, loss = 0.60497473
Iteration 138, loss = 0.60222693
Iteration 139, loss = 0.60534062
Iteration 140, loss = 0.60490123
Iteration 141, loss = 0.60148359
Iteration 142, loss = 0.59992849
Iteration 143, loss = 0.60078817
Iteration 144, loss = 0.59799764
Iteration 145, loss = 0.60023719
Iteration 146, loss = 0.59421208
Iteration 147, loss = 0.59288694
Iteration 148, loss = 0.57803759
Iteration 149, loss = 0.55889271
Iteration 150, loss = 0.51562396
Iteration 151, loss = 0.46945417
Iteration 152, loss = 0.42852911
Iteration 153, loss = 0.38790736
Iteration 154, loss = 0.35815250
Iteration 155, loss = 0.33496575
Iteration 156, loss = 0.32920937
Iteration 157, loss = 0.30262822
Iteration 158, loss = 0.29732902
Iteration 159, loss = 0.28758135
Iteration 160, loss = 0.29872749
Iteration 161, loss = 0.27977643
Iteration 162, loss = 0.27930384
Iteration 163, loss = 0.26873068
Iteration 164, loss = 0.27287113
Iteration 165, loss = 0.26240816
Iteration 166, loss = 0.27102228
Iteration 167, loss = 0.26344903
Iteration 168, loss = 0.26689139
Iteration 169, loss = 0.25410708
Iteration 170, loss = 0.26637163
Iteration 171, loss = 0.25433156
Iteration 172, loss = 0.25697496
Iteration 173, loss = 0.25229278
Iteration 174, loss = 0.25139233
Iteration 175, loss = 0.25242583
Iteration 176, loss = 0.25215095
Iteration 177, loss = 0.25086750
Iteration 178, loss = 0.24809081
Iteration 179, loss = 0.23833165
Iteration 180, loss = 0.26231824
Iteration 181, loss = 0.24021352
Iteration 182, loss = 0.24957900
Iteration 183, loss = 0.23650720
Iteration 184, loss = 0.24149020
Iteration 185, loss = 0.23787452
Iteration 186, loss = 0.23503765
Iteration 187, loss = 0.25445247
Iteration 188, loss = 0.22800483
Iteration 189, loss = 0.23483686
Iteration 190, loss = 0.23356257
Iteration 191, loss = 0.24496023
Iteration 192, loss = 0.23162653
Iteration 193, loss = 0.23534297
Iteration 194, loss = 0.23236888
Iteration 195, loss = 0.22902619
Iteration 196, loss = 0.22907552
Iteration 197, loss = 0.21621420
Iteration 198, loss = 0.21718296
Iteration 199, loss = 0.22881357
Iteration 200, loss = 0.22709878
Iteration 201, loss = 0.21859072
Iteration 202, loss = 0.21846166
Iteration 203, loss = 0.21451306
Iteration 204, loss = 0.21323558
Iteration 205, loss = 0.21642855
Iteration 206, loss = 0.20780509
Iteration 207, loss = 0.21135671
Iteration 208, loss = 0.21104758
Iteration 209, loss = 0.21480887
Iteration 210, loss = 0.20848456
Iteration 211, loss = 0.20362241
Iteration 212, loss = 0.20689638
Iteration 213, loss = 0.20748246
Iteration 214, loss = 0.21030865
Iteration 215, loss = 0.21116182
Iteration 216, loss = 0.20488335
Iteration 217, loss = 0.21174305
Iteration 218, loss = 0.20538585
Iteration 219, loss = 0.21189617
Iteration 220, loss = 0.20560938

Iteration 221, loss = 0.20128423
Iteration 222, loss = 0.20395848
Iteration 223, loss = 0.20270953
Iteration 224, loss = 0.21256891
Iteration 225, loss = 0.20033706
Iteration 226, loss = 0.20085535
Iteration 227, loss = 0.20120862
Iteration 228, loss = 0.20495435
Iteration 229, loss = 0.19454807
Iteration 230, loss = 0.20561331
Iteration 231, loss = 0.20748618
Iteration 232, loss = 0.19655708
Iteration 233, loss = 0.20253813
Iteration 234, loss = 0.20975446
Iteration 235, loss = 0.19528955
Iteration 236, loss = 0.21123563
Iteration 237, loss = 0.20421982
Iteration 238, loss = 0.19385183
Iteration 239, loss = 0.19824261
Iteration 240, loss = 0.21391145
Iteration 241, loss = 0.19438413
Iteration 242, loss = 0.19484107
Iteration 243, loss = 0.19997089
Iteration 244, loss = 0.19838834
Iteration 245, loss = 0.20822928
Iteration 246, loss = 0.18844883
Iteration 247, loss = 0.18998738
Iteration 248, loss = 0.19989239
Iteration 249, loss = 0.19787289
Iteration 250, loss = 0.19913389
Iteration 251, loss = 0.19982137
Iteration 252, loss = 0.20872158
Iteration 253, loss = 0.19451329
Iteration 254, loss = 0.19850156
Iteration 255, loss = 0.19650188
Iteration 256, loss = 0.19958238
Iteration 257, loss = 0.19620646
Iteration 258, loss = 0.19681634
Iteration 259, loss = 0.18635136
Iteration 260, loss = 0.18812533
Iteration 261, loss = 0.20586506
Iteration 262, loss = 0.19023871
Iteration 263, loss = 0.18514261
Iteration 264, loss = 0.18707327
Iteration 265, loss = 0.19089679
Iteration 266, loss = 0.19738924
Iteration 267, loss = 0.18885154
Iteration 268, loss = 0.19145376
Iteration 269, loss = 0.18707152
Iteration 270, loss = 0.19419644
Iteration 271, loss = 0.19666825
Iteration 272, loss = 0.19023688
Iteration 273, loss = 0.18899819
Iteration 274, loss = 0.18869794
Iteration 275, loss = 0.18832924
Iteration 276, loss = 0.19439021
Iteration 277, loss = 0.18614702
Iteration 278, loss = 0.19308898
Iteration 279, loss = 0.19164372
Iteration 280, loss = 0.19129593
Iteration 281, loss = 0.18455426
Iteration 282, loss = 0.18807016
Iteration 283, loss = 0.19483193
Iteration 284, loss = 0.18761641
Iteration 285, loss = 0.18837059
Iteration 286, loss = 0.18579349
Iteration 287, loss = 0.18558348
Iteration 288, loss = 0.18356479
Iteration 289, loss = 0.19415725
Iteration 290, loss = 0.18661555
Iteration 291, loss = 0.18892060
Iteration 292, loss = 0.18618854
Iteration 293, loss = 0.18663038
Iteration 294, loss = 0.18993678
Iteration 295, loss = 0.18146866
Iteration 296, loss = 0.18393797
Iteration 297, loss = 0.18531141
Iteration 298, loss = 0.18586640
Iteration 299, loss = 0.18740226
Iteration 300, loss = 0.18219595
Iteration 301, loss = 0.19057448
Iteration 302, loss = 0.18670716
Iteration 303, loss = 0.19407153
Iteration 304, loss = 0.18914209
Iteration 305, loss = 0.18777138
Iteration 306, loss = 0.18685216
Iteration 307, loss = 0.18460226
Iteration 308, loss = 0.18603454
Iteration 309, loss = 0.18448033
Iteration 310, loss = 0.18458505
Iteration 311, loss = 0.18846637
Iteration 312, loss = 0.18681644
Iteration 313, loss = 0.18956747
Iteration 314, loss = 0.18209283
Iteration 315, loss = 0.18553742
Iteration 316, loss = 0.18184461
Iteration 317, loss = 0.18925087
Iteration 318, loss = 0.18219907
Iteration 319, loss = 0.19296109
Iteration 320, loss = 0.18921682
Iteration 321, loss = 0.18882292
Iteration 322, loss = 0.18590733
Iteration 323, loss = 0.18200790
Iteration 324, loss = 0.19106328
Iteration 325, loss = 0.18223097
Iteration 326, loss = 0.18984427
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 4/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(24, 24);, score=0.896 total time= 1.6min
Iteration 1, loss = 2.22817694
Iteration 2, loss = 1.03053643
Iteration 3, loss = 0.88951624
Iteration 4, loss = 0.83461281
Iteration 5, loss = 0.82311923
Iteration 6, loss = 0.78071840
Iteration 7, loss = 0.73493588
Iteration 8, loss = 0.84012630
Iteration 9, loss = 0.82322822
Iteration 10, loss = 0.74361741
Iteration 11, loss = 0.78059985
Iteration 12, loss = 0.73930776
Iteration 13, loss = 0.76640010
Iteration 14, loss = 0.73351297
Iteration 15, loss = 0.69866773
Iteration 16, loss = 0.76980512
Iteration 17, loss = 0.74367070
Iteration 18, loss = 0.71184706
Iteration 19, loss = 0.73139432
Iteration 20, loss = 0.70836120
Iteration 21, loss = 0.72793740
Iteration 22, loss = 0.71420501
Iteration 23, loss = 0.70983669
Iteration 24, loss = 0.71856084
Iteration 25, loss = 0.68562457
Iteration 26, loss = 0.72583459
Iteration 27, loss = 0.68493415
Iteration 28, loss = 0.69684278
Iteration 29, loss = 0.70462952
Iteration 30, loss = 0.66670058
Iteration 31, loss = 0.70459779
Iteration 32, loss = 0.68750561
Iteration 33, loss = 0.68038578
Iteration 34, loss = 0.68516327
Iteration 35, loss = 0.68423430
Iteration 36, loss = 0.66843173
Iteration 37, loss = 0.69019315
Iteration 38, loss = 0.67701881
Iteration 39, loss = 0.67032628
Iteration 40, loss = 0.66661581
Iteration 41, loss = 0.67851395
Iteration 42, loss = 0.67208852
Iteration 43, loss = 0.66127693
Iteration 44, loss = 0.65760850
Iteration 45, loss = 0.68307490
Iteration 46, loss = 0.64148747
Iteration 47, loss = 0.65975527
Iteration 48, loss = 0.65586744
Iteration 49, loss = 0.66470960
Iteration 50, loss = 0.65719996
Iteration 51, loss = 0.65625390
Iteration 52, loss = 0.65286044
Iteration 53, loss = 0.64290770
Iteration 54, loss = 0.65092023
Iteration 55, loss = 0.64994671
Iteration 56, loss = 0.64427427
Iteration 57, loss = 0.64284261
Iteration 58, loss = 0.64215816
Iteration 59, loss = 0.63923015
Iteration 60, loss = 0.64647598
Iteration 61, loss = 0.63196799
Iteration 62, loss = 0.63275880
Iteration 63, loss = 0.63780001
Iteration 64, loss = 0.62897917
Iteration 65, loss = 0.63794154
Iteration 66, loss = 0.63147397
Iteration 67, loss = 0.63267139
Iteration 68, loss = 0.63760122
Iteration 69, loss = 0.62415909
Iteration 70, loss = 0.62888928
Iteration 71, loss = 0.62435869
Iteration 72, loss = 0.62789289
Iteration 73, loss = 0.62846762
Iteration 74, loss = 0.62703808
Iteration 75, loss = 0.61963735
Iteration 76, loss = 0.62067642
Iteration 77, loss = 0.62438567
Iteration 78, loss = 0.61920338
Iteration 79, loss = 0.62157176
Iteration 80, loss = 0.62414304
Iteration 81, loss = 0.62302520
Iteration 82, loss = 0.62016852
Iteration 83, loss = 0.61994207
Iteration 84, loss = 0.61904421
Iteration 85, loss = 0.61696446
Iteration 86, loss = 0.62239540
Iteration 87, loss = 0.61210861
Iteration 88, loss = 0.61549049
Iteration 89, loss = 0.61578735
Iteration 90, loss = 0.61660513
Iteration 91, loss = 0.61093935
Iteration 92, loss = 0.62236046
Iteration 93, loss = 0.61127660
Iteration 94, loss = 0.61752443
Iteration 95, loss = 0.61410928
Iteration 96, loss = 0.61862012
Iteration 97, loss = 0.61024208
Iteration 98, loss = 0.61436197
Iteration 99, loss = 0.61303444
Iteration 100, loss = 0.61132947
Iteration 101, loss = 0.61337971
Iteration 102, loss = 0.61352100
Iteration 103, loss = 0.61146428
Iteration 104, loss = 0.60863304
Iteration 105, loss = 0.61050258
Iteration 106, loss = 0.61269360
Iteration 107, loss = 0.60971344
Iteration 108, loss = 0.61017480
Iteration 109, loss = 0.60946744
Iteration 110, loss = 0.60976828
Iteration 111, loss = 0.61047424
Iteration 112, loss = 0.60880977
Iteration 113, loss = 0.61084413
Iteration 114, loss = 0.60641005
Iteration 115, loss = 0.60741227
Iteration 116, loss = 0.60745794
Iteration 117, loss = 0.60808360
Iteration 118, loss = 0.60447152
Iteration 119, loss = 0.60421497
Iteration 120, loss = 0.60248861
Iteration 121, loss = 0.59984633
Iteration 122, loss = 0.60055175
Iteration 123, loss = 0.59949487
Iteration 124, loss = 0.60080135
Iteration 125, loss = 0.60144388
Iteration 126, loss = 0.59676220
Iteration 127, loss = 0.59761269
Iteration 128, loss = 0.59861919
Iteration 129, loss = 0.59466920
Iteration 130, loss = 0.59602949
Iteration 131, loss = 0.59546327
Iteration 132, loss = 0.59447179
Iteration 133, loss = 0.58818232
Iteration 134, loss = 0.58913169
Iteration 135, loss = 0.58525761
Iteration 136, loss = 0.58282665
Iteration 137, loss = 0.58337788
Iteration 138, loss = 0.58183692
Iteration 139, loss = 0.57881793
Iteration 140, loss = 0.57585856

Iteration 141, loss = 0.57619787
Iteration 142, loss = 0.57651474
Iteration 143, loss = 0.57773812
Iteration 144, loss = 0.57526764
Iteration 145, loss = 0.57449989
Iteration 146, loss = 0.57680447
Iteration 147, loss = 0.57395281
Iteration 148, loss = 0.57280679
Iteration 149, loss = 0.57328775
Iteration 150, loss = 0.57179081
Iteration 151, loss = 0.57406390
Iteration 152, loss = 0.57092013
Iteration 153, loss = 0.56727760
Iteration 154, loss = 0.55496101
Iteration 155, loss = 0.51696811
Iteration 156, loss = 0.44538411
Iteration 157, loss = 0.37437811
Iteration 158, loss = 0.33178703
Iteration 159, loss = 0.29885120
Iteration 160, loss = 0.26864429
Iteration 161, loss = 0.25382534
Iteration 162, loss = 0.24657925
Iteration 163, loss = 0.22723396
Iteration 164, loss = 0.23029401
Iteration 165, loss = 0.21836324
Iteration 166, loss = 0.21201454
Iteration 167, loss = 0.20471096
Iteration 168, loss = 0.21173077
Iteration 169, loss = 0.19671258
Iteration 170, loss = 0.19793722
Iteration 171, loss = 0.18923057
Iteration 172, loss = 0.19664017
Iteration 173, loss = 0.20154235
Iteration 174, loss = 0.18081779
Iteration 175, loss = 0.18502116
Iteration 176, loss = 0.18865042
Iteration 177, loss = 0.19321595
Iteration 178, loss = 0.19379273
Iteration 179, loss = 0.19050059
Iteration 180, loss = 0.17837113
Iteration 181, loss = 0.18845211
Iteration 182, loss = 0.17955260
Iteration 183, loss = 0.18310120
Iteration 184, loss = 0.18407903
Iteration 185, loss = 0.18440315
Iteration 186, loss = 0.16982317
Iteration 187, loss = 0.18202292
Iteration 188, loss = 0.18526872
Iteration 189, loss = 0.17027613
Iteration 190, loss = 0.18817533
Iteration 191, loss = 0.17174913
Iteration 192, loss = 0.16463874
Iteration 193, loss = 0.17551689
Iteration 194, loss = 0.16061500
Iteration 195, loss = 0.16731618
Iteration 196, loss = 0.15677008
Iteration 197, loss = 0.16832240
Iteration 198, loss = 0.15551042
Iteration 199, loss = 0.16321149
Iteration 200, loss = 0.15062222
Iteration 201, loss = 0.16813613
Iteration 202, loss = 0.14783487
Iteration 203, loss = 0.15793716
Iteration 204, loss = 0.13843349
Iteration 205, loss = 0.14856091
Iteration 206, loss = 0.14537821
Iteration 207, loss = 0.13683034
Iteration 208, loss = 0.13553667
Iteration 209, loss = 0.13846200
Iteration 210, loss = 0.15556057
Iteration 211, loss = 0.12882797
Iteration 212, loss = 0.13575073
Iteration 213, loss = 0.14491804
Iteration 214, loss = 0.12629310
Iteration 215, loss = 0.12199475
Iteration 216, loss = 0.14466076
Iteration 217, loss = 0.11342349
Iteration 218, loss = 0.30159984
Iteration 219, loss = 0.49902261
Iteration 220, loss = 0.47574832
Iteration 221, loss = 0.43275215
Iteration 222, loss = 0.37607509
Iteration 223, loss = 0.32145241
Iteration 224, loss = 0.27970781
Iteration 225, loss = 0.25813961
Iteration 226, loss = 0.23616038
Iteration 227, loss = 0.24273025
Iteration 228, loss = 0.22919896
Iteration 229, loss = 0.23378281
Iteration 230, loss = 0.22062737
Iteration 231, loss = 0.22028454
Iteration 232, loss = 0.22482948
Iteration 233, loss = 0.21860487
Iteration 234, loss = 0.22676030
Iteration 235, loss = 0.21323279
Iteration 236, loss = 0.21341234
Iteration 237, loss = 0.21746064
Iteration 238, loss = 0.21959581
Iteration 239, loss = 0.20490596
Iteration 240, loss = 0.24150031
Iteration 241, loss = 0.21294636
Iteration 242, loss = 0.20893778
Iteration 243, loss = 0.20142885
Iteration 244, loss = 0.21045574
Iteration 245, loss = 0.20108451
Iteration 246, loss = 0.20293448
Iteration 247, loss = 0.21220383
Iteration 248, loss = 0.21615893
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 5/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(24, 24);, score=0.933 total time= 1.4min
Iteration 1, loss = 1.36242819
Iteration 2, loss = 0.83527991
Iteration 3, loss = 0.73631479
Iteration 4, loss = 0.78829731
Iteration 5, loss = 0.71528615
Iteration 6, loss = 0.76051284
Iteration 7, loss = 0.70135659
Iteration 8, loss = 0.71836442
Iteration 9, loss = 0.70253974
Iteration 10, loss = 0.66077195
Iteration 11, loss = 0.68347278
Iteration 12, loss = 0.66292394
Iteration 13, loss = 0.66225893
Iteration 14, loss = 0.65031310
Iteration 15, loss = 0.66365062
Iteration 16, loss = 0.63814618
Iteration 17, loss = 0.65658033
Iteration 18, loss = 0.64288611
Iteration 19, loss = 0.63643991
Iteration 20, loss = 0.63743275
Iteration 21, loss = 0.63286356
Iteration 22, loss = 0.62533229
Iteration 23, loss = 0.63252775
Iteration 24, loss = 0.62967924
Iteration 25, loss = 0.63060165
Iteration 26, loss = 0.62831416
Iteration 27, loss = 0.62181752
Iteration 28, loss = 0.62258028
Iteration 29, loss = 0.62174725
Iteration 30, loss = 0.61877754
Iteration 31, loss = 0.61664632
Iteration 32, loss = 0.61693440
Iteration 33, loss = 0.61583891
Iteration 34, loss = 0.61529041
Iteration 35, loss = 0.61210056
Iteration 36, loss = 0.61452023
Iteration 37, loss = 0.61550594
Iteration 38, loss = 0.61419134
Iteration 39, loss = 0.61272261
Iteration 40, loss = 0.61359239
Iteration 41, loss = 0.60781449
Iteration 42, loss = 0.60967598
Iteration 43, loss = 0.60616621
Iteration 44, loss = 0.61331381
Iteration 45, loss = 0.60921173
Iteration 46, loss = 0.60988593
Iteration 47, loss = 0.61427650
Iteration 48, loss = 0.61039598
Iteration 49, loss = 0.61045195
Iteration 50, loss = 0.60882567
Iteration 51, loss = 0.60901718
Iteration 52, loss = 0.60752954
Iteration 53, loss = 0.60870940
Iteration 54, loss = 0.60908498
Iteration 55, loss = 0.60816572
Iteration 56, loss = 0.60328212
Iteration 57, loss = 0.60634648
Iteration 58, loss = 0.60489116
Iteration 59, loss = 0.60385005
Iteration 60, loss = 0.60654738
Iteration 61, loss = 0.60108317
Iteration 62, loss = 0.60250890
Iteration 63, loss = 0.60361606
Iteration 64, loss = 0.60737464
Iteration 65, loss = 0.60796250
Iteration 66, loss = 0.60478128
Iteration 67, loss = 0.60581916
Iteration 68, loss = 0.60936031
Iteration 69, loss = 0.61577590
Iteration 70, loss = 0.60827847
Iteration 71, loss = 0.60747687
Iteration 72, loss = 0.60237918
Iteration 73, loss = 0.59597709
Iteration 74, loss = 0.59233449
Iteration 75, loss = 0.61895009
Iteration 76, loss = 0.61665396
Iteration 77, loss = 0.61992557
Iteration 78, loss = 0.68179714
Iteration 79, loss = 0.68905732
Iteration 80, loss = 0.68904938
Iteration 81, loss = 0.68906114
Iteration 82, loss = 0.68907627
Iteration 83, loss = 0.68906219
Iteration 84, loss = 0.68908007
Iteration 85, loss = 0.68904731
Iteration 86, loss = 0.68904171
Iteration 87, loss = 0.68902835
Iteration 88, loss = 0.68906787
Iteration 89, loss = 0.68906138
Iteration 90, loss = 0.68904397
Iteration 91, loss = 0.68905647
Iteration 92, loss = 0.68905823
Iteration 93, loss = 0.68904686
Iteration 94, loss = 0.68915183
Iteration 95, loss = 0.68901712
Iteration 96, loss = 0.68910743
Iteration 97, loss = 0.68901604
Iteration 98, loss = 0.68898425
Iteration 99, loss = 0.68903986
Iteration 100, loss = 0.68896987
Iteration 101, loss = 0.68978415
Iteration 102, loss = 0.68896768
Iteration 103, loss = 0.68899002
Iteration 104, loss = 0.68896969
Iteration 105, loss = 0.68896819
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 1/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(24, 24, 24);, score=0.546 total time= 1.0min
Iteration 1, loss = 1.84741909
Iteration 2, loss = 0.80495168
Iteration 3, loss = 0.77247957
Iteration 4, loss = 0.73091815
Iteration 5, loss = 0.72773415
Iteration 6, loss = 0.69759889
Iteration 7, loss = 0.69171397
Iteration 8, loss = 0.68610711
Iteration 9, loss = 0.68180327
Iteration 10, loss = 0.66240822
Iteration 11, loss = 0.66274062
Iteration 12, loss = 0.65571384
Iteration 13, loss = 0.65186742
Iteration 14, loss = 0.65609475
Iteration 15, loss = 0.65313365
Iteration 16, loss = 0.63637536
Iteration 17, loss = 0.64922481
Iteration 18, loss = 0.64253952
Iteration 19, loss = 0.63252019
Iteration 20, loss = 0.64251566
Iteration 21, loss = 0.63426628
Iteration 22, loss = 0.62462343
Iteration 23, loss = 0.63295705
Iteration 24, loss = 0.63254442
Iteration 25, loss = 0.61898916
Iteration 26, loss = 0.62275761
Iteration 27, loss = 0.62390428
Iteration 28, loss = 0.61986197

Iteration 29, loss = 0.61613861
Iteration 30, loss = 0.62295107
Iteration 31, loss = 0.61889508
Iteration 32, loss = 0.61446027
Iteration 33, loss = 0.61389948
Iteration 34, loss = 0.61067023
Iteration 35, loss = 0.61596115
Iteration 36, loss = 0.61405880
Iteration 37, loss = 0.61217385
Iteration 38, loss = 0.61129964
Iteration 39, loss = 0.60741188
Iteration 40, loss = 0.60787470
Iteration 41, loss = 0.61317584
Iteration 42, loss = 0.60774873
Iteration 43, loss = 0.60380900
Iteration 44, loss = 0.60445044
Iteration 45, loss = 0.60367878
Iteration 46, loss = 0.60355036
Iteration 47, loss = 0.60113386
Iteration 48, loss = 0.60074590
Iteration 49, loss = 0.59757600
Iteration 50, loss = 0.59027760
Iteration 51, loss = 0.58415963
Iteration 52, loss = 0.58338780
Iteration 53, loss = 0.58481416
Iteration 54, loss = 0.58350512
Iteration 55, loss = 0.58240464
Iteration 56, loss = 0.58382095
Iteration 57, loss = 0.58242991
Iteration 58, loss = 0.58018641
Iteration 59, loss = 0.57780787
Iteration 60, loss = 0.57788069
Iteration 61, loss = 0.57856196
Iteration 62, loss = 0.57969640
Iteration 63, loss = 0.57666762
Iteration 64, loss = 0.57957735
Iteration 65, loss = 0.57615352
Iteration 66, loss = 0.57695694
Iteration 67, loss = 0.57611746
Iteration 68, loss = 0.56967859
Iteration 69, loss = 0.57410386
Iteration 70, loss = 0.56417281
Iteration 71, loss = 0.55147937
Iteration 72, loss = 0.52648825
Iteration 73, loss = 0.46520375
Iteration 74, loss = 0.54059835
Iteration 75, loss = 0.57048947
Iteration 76, loss = 0.57407443
Iteration 77, loss = 0.56828465
Iteration 78, loss = 0.56341995
Iteration 79, loss = 0.55465236
Iteration 80, loss = 0.54843777
Iteration 81, loss = 0.52599153
Iteration 82, loss = 0.49155571
Iteration 83, loss = 0.44156238
Iteration 84, loss = 0.38289534
Iteration 85, loss = 0.33062590
Iteration 86, loss = 0.31334522
Iteration 87, loss = 0.29205695
Iteration 88, loss = 0.32147445
Iteration 89, loss = 0.26823428
Iteration 90, loss = 0.27312790
Iteration 91, loss = 0.26150611
Iteration 92, loss = 0.26061905
Iteration 93, loss = 0.27334373
Iteration 94, loss = 0.26253082
Iteration 95, loss = 0.26882119
Iteration 96, loss = 0.25395767
Iteration 97, loss = 0.33097090
Iteration 98, loss = 0.31135038
Iteration 99, loss = 0.30921505
Iteration 100, loss = 0.29893260
Iteration 101, loss = 0.28868640
Iteration 102, loss = 0.30349884
Iteration 103, loss = 0.29651914
Iteration 104, loss = 0.29113466
Iteration 105, loss = 0.28636812
Iteration 106, loss = 0.28647812
Iteration 107, loss = 0.28905781
Iteration 108, loss = 0.29166566
Iteration 109, loss = 0.28349342
Iteration 110, loss = 0.29394677
Iteration 111, loss = 0.28885750
Iteration 112, loss = 0.27889217
Iteration 113, loss = 0.28984622
Iteration 114, loss = 0.29730678
Iteration 115, loss = 0.27894977
Iteration 116, loss = 0.27637467
Iteration 117, loss = 0.29274746
Iteration 118, loss = 0.27530331
Iteration 119, loss = 0.27287778
Iteration 120, loss = 0.28017240
Iteration 121, loss = 0.27404872
Iteration 122, loss = 0.27204887
Iteration 123, loss = 0.26774386
Iteration 124, loss = 0.26959028
Iteration 125, loss = 0.26815775
Iteration 126, loss = 0.26641203
Iteration 127, loss = 0.25827178
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 2/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(24, 24, 24);, score=0.926 total time= 1.1min
Iteration 1, loss = 1.70887056
Iteration 2, loss = 1.03771005
Iteration 3, loss = 0.95332719
Iteration 4, loss = 0.81723337
Iteration 5, loss = 0.78639033
Iteration 6, loss = 0.77692949
Iteration 7, loss = 0.74490902
Iteration 8, loss = 0.75065133
Iteration 9, loss = 0.75121216
Iteration 10, loss = 0.74703752
Iteration 11, loss = 0.72310881
Iteration 12, loss = 0.70319998
Iteration 13, loss = 0.68000503
Iteration 14, loss = 0.70265601
Iteration 15, loss = 0.70206559
Iteration 16, loss = 0.67235073
Iteration 17, loss = 0.68054573
Iteration 18, loss = 0.67054397
Iteration 19, loss = 0.64763908
Iteration 20, loss = 0.66122828
Iteration 21, loss = 0.65174367
Iteration 22, loss = 0.67596633
Iteration 23, loss = 0.63810452
Iteration 24, loss = 0.66059580
Iteration 25, loss = 0.64384765
Iteration 26, loss = 0.63318554
Iteration 27, loss = 0.64091058
Iteration 28, loss = 0.65029378
Iteration 29, loss = 0.64106662
Iteration 30, loss = 0.63060553
Iteration 31, loss = 0.64100841
Iteration 32, loss = 0.62876960
Iteration 33, loss = 0.63167315
Iteration 34, loss = 0.62161348
Iteration 35, loss = 0.62279295
Iteration 36, loss = 0.62341583
Iteration 37, loss = 0.60754292
Iteration 38, loss = 0.56940331
Iteration 39, loss = 0.51030813
Iteration 40, loss = 0.44483933
Iteration 41, loss = 0.39313696
Iteration 42, loss = 0.50544716
Iteration 43, loss = 0.61475746
Iteration 44, loss = 0.61974177
Iteration 45, loss = 0.61482141
Iteration 46, loss = 0.61240565
Iteration 47, loss = 0.61221576
Iteration 48, loss = 0.61365401
Iteration 49, loss = 0.61453231
Iteration 50, loss = 0.60556729
Iteration 51, loss = 0.56877107
Iteration 52, loss = 0.48824512
Iteration 53, loss = 0.39485911
Iteration 54, loss = 0.34359494
Iteration 55, loss = 0.31087869
Iteration 56, loss = 0.30687837
Iteration 57, loss = 0.30341773
Iteration 58, loss = 0.29804858
Iteration 59, loss = 0.29102053
Iteration 60, loss = 0.29456445
Iteration 61, loss = 0.29897608
Iteration 62, loss = 0.28189811
Iteration 63, loss = 0.28113094
Iteration 64, loss = 0.28007043
Iteration 65, loss = 0.28621350
Iteration 66, loss = 0.28796557
Iteration 67, loss = 0.27614092
Iteration 68, loss = 0.27976003
Iteration 69, loss = 0.27325619
Iteration 70, loss = 0.27582362
Iteration 71, loss = 0.26418694
Iteration 72, loss = 0.26934862
Iteration 73, loss = 0.28108525
Iteration 74, loss = 0.26082364
Iteration 75, loss = 0.26843673
Iteration 76, loss = 0.26605067
Iteration 77, loss = 0.27383373
Iteration 78, loss = 0.25929592
Iteration 79, loss = 0.24907013
Iteration 80, loss = 0.24751135
Iteration 81, loss = 0.23795816
Iteration 82, loss = 0.22878455
Iteration 83, loss = 0.23073733
Iteration 84, loss = 0.21760832
Iteration 85, loss = 0.22269039
Iteration 86, loss = 0.20315537
Iteration 87, loss = 0.19292073
Iteration 88, loss = 0.18431335
Iteration 89, loss = 0.19721426
Iteration 90, loss = 0.17988787
Iteration 91, loss = 0.18147357
Iteration 92, loss = 0.16407104
Iteration 93, loss = 0.15940805
Iteration 94, loss = 0.19176219
Iteration 95, loss = 0.22774820
Iteration 96, loss = 0.25953317
Iteration 97, loss = 0.24876863
Iteration 98, loss = 0.24053137
Iteration 99, loss = 0.17896352
Iteration 100, loss = 0.16749931
Iteration 101, loss = 0.16310604
Iteration 102, loss = 0.16050888
Iteration 103, loss = 0.17614578
Iteration 104, loss = 0.22807638
Iteration 105, loss = 0.24866130
Iteration 106, loss = 0.24505181
Iteration 107, loss = 0.24721330
Iteration 108, loss = 0.23484345
Iteration 109, loss = 0.25318565
Iteration 110, loss = 0.24810685
Iteration 111, loss = 0.23669655
Iteration 112, loss = 0.24751984
Iteration 113, loss = 0.24626300
Iteration 114, loss = 0.24960175
Iteration 115, loss = 0.24167883
Iteration 116, loss = 0.24051483
Iteration 117, loss = 0.24025299
Iteration 118, loss = 0.24479200
Iteration 119, loss = 0.23636284
Iteration 120, loss = 0.24039333
Iteration 121, loss = 0.25735559
Iteration 122, loss = 0.24192311
Iteration 123, loss = 0.24256935
Iteration 124, loss = 0.24327780
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 3/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(24, 24, 24);, score=0.911 total time= 1.1min
Iteration 1, loss = 2.60086934
Iteration 2, loss = 0.83604456
Iteration 3, loss = 0.81624970
Iteration 4, loss = 0.76894582
Iteration 5, loss = 0.79351445
Iteration 6, loss = 0.75980325
Iteration 7, loss = 0.73350581
Iteration 8, loss = 0.70716454
Iteration 9, loss = 0.68867098
Iteration 10, loss = 0.68129685
Iteration 11, loss = 0.66683362
Iteration 12, loss = 0.66060068
Iteration 13, loss = 0.64342459
Iteration 14, loss = 0.63955617
Iteration 15, loss = 0.64129533
Iteration 16, loss = 0.63953931
Iteration 17, loss = 0.63284107
Iteration 18, loss = 0.64154510
Iteration 19, loss = 0.63054950

Iteration 20, loss = 0.63861256
Iteration 21, loss = 0.63292946
Iteration 22, loss = 0.63096930
Iteration 23, loss = 0.62162450
Iteration 24, loss = 0.62569169
Iteration 25, loss = 0.62094546
Iteration 26, loss = 0.62493367
Iteration 27, loss = 0.61298443
Iteration 28, loss = 0.61712797
Iteration 29, loss = 0.61701814
Iteration 30, loss = 0.61734978
Iteration 31, loss = 0.61464893
Iteration 32, loss = 0.61654052
Iteration 33, loss = 0.61377218
Iteration 34, loss = 0.61342265
Iteration 35, loss = 0.61153724
Iteration 36, loss = 0.61370027
Iteration 37, loss = 0.60992862
Iteration 38, loss = 0.60778140
Iteration 39, loss = 0.60893401
Iteration 40, loss = 0.60888842
Iteration 41, loss = 0.60689115
Iteration 42, loss = 0.60687796
Iteration 43, loss = 0.60515961
Iteration 44, loss = 0.60398435
Iteration 45, loss = 0.60015302
Iteration 46, loss = 0.59956717
Iteration 47, loss = 0.59997512
Iteration 48, loss = 0.59712383
Iteration 49, loss = 0.59524314
Iteration 50, loss = 0.58675113
Iteration 51, loss = 0.57355912
Iteration 52, loss = 0.52609654
Iteration 53, loss = 0.43066876
Iteration 54, loss = 0.35613952
Iteration 55, loss = 0.32586671
Iteration 56, loss = 0.51491450
Iteration 57, loss = 0.35160264
Iteration 58, loss = 0.30571381
Iteration 59, loss = 0.29439308
Iteration 60, loss = 0.29266658
Iteration 61, loss = 0.29982089
Iteration 62, loss = 0.26294257
Iteration 63, loss = 0.26269295
Iteration 64, loss = 0.25878391
Iteration 65, loss = 0.25823382
Iteration 66, loss = 0.25194177
Iteration 67, loss = 0.24302165
Iteration 68, loss = 0.24050865
Iteration 69, loss = 0.24561287
Iteration 70, loss = 0.23165968
Iteration 71, loss = 0.24359006
Iteration 72, loss = 0.24024133
Iteration 73, loss = 0.22893413
Iteration 74, loss = 0.23064469
Iteration 75, loss = 0.23608023
Iteration 76, loss = 0.22978557
Iteration 77, loss = 0.22882624
Iteration 78, loss = 0.26007957
Iteration 79, loss = 0.23551503
Iteration 80, loss = 0.21473959
Iteration 81, loss = 0.22219430
Iteration 82, loss = 0.22596966
Iteration 83, loss = 0.23190617
Iteration 84, loss = 0.23933070
Iteration 85, loss = 0.20979646
Iteration 86, loss = 0.20068255
Iteration 87, loss = 0.21393937
Iteration 88, loss = 0.20413049
Iteration 89, loss = 0.20745612
Iteration 90, loss = 0.19976796
Iteration 91, loss = 0.21436862
Iteration 92, loss = 0.20151064
Iteration 93, loss = 0.21675742
Iteration 94, loss = 0.19099475
Iteration 95, loss = 0.18804769
Iteration 96, loss = 0.19179196
Iteration 97, loss = 0.20287161
Iteration 98, loss = 0.17817886
Iteration 99, loss = 0.18663703
Iteration 100, loss = 0.20440452
Iteration 101, loss = 0.19074316
Iteration 102, loss = 0.17510393
Iteration 103, loss = 0.16784631
Iteration 104, loss = 0.17423820
Iteration 105, loss = 0.17876523
Iteration 106, loss = 0.19243391
Iteration 107, loss = 0.16311461
Iteration 108, loss = 0.23437133
Iteration 109, loss = 0.19129149
Iteration 110, loss = 0.22219118
Iteration 111, loss = 0.17207296
Iteration 112, loss = 0.18498839
Iteration 113, loss = 0.19469739
Iteration 114, loss = 0.18206816
Iteration 115, loss = 0.21268436
Iteration 116, loss = 0.16945746
Iteration 117, loss = 0.18748459
Iteration 118, loss = 0.16248219
Iteration 119, loss = 0.18708552
Iteration 120, loss = 0.16935203
Iteration 121, loss = 0.18435057
Iteration 122, loss = 0.15432816
Iteration 123, loss = 0.17606005
Iteration 124, loss = 0.18241033
Iteration 125, loss = 0.17772871
Iteration 126, loss = 0.16603756
Iteration 127, loss = 0.18075507
Iteration 128, loss = 0.16950531
Iteration 129, loss = 0.18413772
Iteration 130, loss = 0.15833508
Iteration 131, loss = 0.22884469
Iteration 132, loss = 0.18087862
Iteration 133, loss = 0.18180022
Iteration 134, loss = 0.17406016
Iteration 135, loss = 0.17063692
Iteration 136, loss = 0.19647773
Iteration 137, loss = 0.19605049
Iteration 138, loss = 0.20818686
Iteration 139, loss = 0.23639196
Iteration 140, loss = 0.20021185
Iteration 141, loss = 0.24069712
Iteration 142, loss = 0.15308281
Iteration 143, loss = 0.22094702
Iteration 144, loss = 0.23699207
Iteration 145, loss = 0.19617591
Iteration 146, loss = 0.17946131
Iteration 147, loss = 0.20169285
Iteration 148, loss = 0.23197721
Iteration 149, loss = 0.18744819
Iteration 150, loss = 0.18491509
Iteration 151, loss = 0.16642273
Iteration 152, loss = 0.22291777
Iteration 153, loss = 0.21116113
Iteration 154, loss = 0.15132697
Iteration 155, loss = 0.26412040
Iteration 156, loss = 0.17429482
Iteration 157, loss = 0.17719358
Iteration 158, loss = 0.24362125
Iteration 159, loss = 0.20613079
Iteration 160, loss = 0.17454828
Iteration 161, loss = 0.22239316
Iteration 162, loss = 0.19127237
Iteration 163, loss = 0.16884980
Iteration 164, loss = 0.17606818
Iteration 165, loss = 0.17866413
Iteration 166, loss = 0.19456231
Iteration 167, loss = 0.18661647
Iteration 168, loss = 0.16186775
Iteration 169, loss = 0.18402572
Iteration 170, loss = 0.18534124
Iteration 171, loss = 0.17567397
Iteration 172, loss = 0.19764548
Iteration 173, loss = 0.19240585
Iteration 174, loss = 0.18380828
Iteration 175, loss = 0.14990898
Iteration 176, loss = 0.15085660
Iteration 177, loss = 0.14962190
Iteration 178, loss = 0.18594629
Iteration 179, loss = 0.26528200
Iteration 180, loss = 0.17180853
Iteration 181, loss = 0.15783674
Iteration 182, loss = 0.19543045
Iteration 183, loss = 0.18585796
Iteration 184, loss = 0.17598021
Iteration 185, loss = 0.17001324
Iteration 186, loss = 0.14897644
Iteration 187, loss = 0.15326212
Iteration 188, loss = 0.17990065
Iteration 189, loss = 0.15384603
Iteration 190, loss = 0.27354486
Iteration 191, loss = 0.22137112
Iteration 192, loss = 0.17382080
Iteration 193, loss = 0.16281271
Iteration 194, loss = 0.19168352
Iteration 195, loss = 0.17472448
Iteration 196, loss = 0.16229701
Iteration 197, loss = 0.22515102
Iteration 198, loss = 0.19885058
Iteration 199, loss = 0.17786411
Iteration 200, loss = 0.17351677
Iteration 201, loss = 0.19471090
Iteration 202, loss = 0.22437490
Iteration 203, loss = 0.20491709
Iteration 204, loss = 0.20539183
Iteration 205, loss = 0.17204616
Iteration 206, loss = 0.18859133
Iteration 207, loss = 0.20367284
Iteration 208, loss = 0.14908168
Iteration 209, loss = 0.26043156
Iteration 210, loss = 0.23599759
Iteration 211, loss = 0.18188102
Iteration 212, loss = 0.17677607
Iteration 213, loss = 0.15051805
Iteration 214, loss = 0.53193664
Iteration 215, loss = 0.35402022
Iteration 216, loss = 0.23738759
Iteration 217, loss = 0.22044909
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 4/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(24, 24, 24);, score=0.942 total time= 1.5min
Iteration 1, loss = 1.90351687
Iteration 2, loss = 0.95074631
Iteration 3, loss = 0.84348199
Iteration 4, loss = 0.81005552
Iteration 5, loss = 0.74251741
Iteration 6, loss = 0.79118671
Iteration 7, loss = 0.73262698
Iteration 8, loss = 0.71932972
Iteration 9, loss = 0.73863497
Iteration 10, loss = 0.68478322
Iteration 11, loss = 0.68661639
Iteration 12, loss = 0.70483116
Iteration 13, loss = 0.68233710
Iteration 14, loss = 0.67694672
Iteration 15, loss = 0.66818661
Iteration 16, loss = 0.65205255
Iteration 17, loss = 0.68509182
Iteration 18, loss = 0.66204887
Iteration 19, loss = 0.65039814
Iteration 20, loss = 0.64163649
Iteration 21, loss = 0.63628790
Iteration 22, loss = 0.65250647
Iteration 23, loss = 0.64663412
Iteration 24, loss = 0.63081441
Iteration 25, loss = 0.63685193
Iteration 26, loss = 0.63179283
Iteration 27, loss = 0.63097022
Iteration 28, loss = 0.62607751
Iteration 29, loss = 0.62881133
Iteration 30, loss = 0.63019462
Iteration 31, loss = 0.62467314
Iteration 32, loss = 0.62405270
Iteration 33, loss = 0.62390662
Iteration 34, loss = 0.62478024
Iteration 35, loss = 0.62353913
Iteration 36, loss = 0.61789655
Iteration 37, loss = 0.62053913
Iteration 38, loss = 0.61570879
Iteration 39, loss = 0.62293998
Iteration 40, loss = 0.61693464
Iteration 41, loss = 0.61280577
Iteration 42, loss = 0.61706619
Iteration 43, loss = 0.61498218
Iteration 44, loss = 0.61322136
Iteration 45, loss = 0.61277680
Iteration 46, loss = 0.61210677
Iteration 47, loss = 0.61119420
Iteration 48, loss = 0.60909705
Iteration 49, loss = 0.60881879

Iteration 50, loss = 0.60914413
Iteration 51, loss = 0.61041492
Iteration 52, loss = 0.60331245
Iteration 53, loss = 0.60750979
Iteration 54, loss = 0.60309715
Iteration 55, loss = 0.60263575
Iteration 56, loss = 0.60186126
Iteration 57, loss = 0.59973779
Iteration 58, loss = 0.59552172
Iteration 59, loss = 0.59560137
Iteration 60, loss = 0.58784737
Iteration 61, loss = 0.59119160
Iteration 62, loss = 0.58867631
Iteration 63, loss = 0.58704933
Iteration 64, loss = 0.58101222
Iteration 65, loss = 0.57938146
Iteration 66, loss = 0.58929339
Iteration 67, loss = 0.59918287
Iteration 68, loss = 0.59858652
Iteration 69, loss = 0.59870620
Iteration 70, loss = 0.59583404
Iteration 71, loss = 0.58775091
Iteration 72, loss = 0.58153104
Iteration 73, loss = 0.58295914
Iteration 74, loss = 0.57689532
Iteration 75, loss = 0.58290301
Iteration 76, loss = 0.58564517
Iteration 77, loss = 0.57383729
Iteration 78, loss = 0.57852789
Iteration 79, loss = 0.57772479
Iteration 80, loss = 0.57604200
Iteration 81, loss = 0.57608546
Iteration 82, loss = 0.60941888
Iteration 83, loss = 0.60956279
Iteration 84, loss = 0.60656289
Iteration 85, loss = 0.60241827
Iteration 86, loss = 0.59921076
Iteration 87, loss = 0.59766697
Iteration 88, loss = 0.59837923
Iteration 89, loss = 0.59623665
Iteration 90, loss = 0.59708130
Iteration 91, loss = 0.59679491
Iteration 92, loss = 0.59553051
Iteration 93, loss = 0.59782881
Iteration 94, loss = 0.59515942
Iteration 95, loss = 0.59342535
Iteration 96, loss = 0.59607233
Iteration 97, loss = 0.59820275
Iteration 98, loss = 0.59796268
Iteration 99, loss = 0.59516570
Iteration 100, loss = 0.59531235
Iteration 101, loss = 0.59588084
Iteration 102, loss = 0.59459174
Iteration 103, loss = 0.59281275
Iteration 104, loss = 0.59528799
Iteration 105, loss = 0.59390108
Iteration 106, loss = 0.59774085
Iteration 107, loss = 0.59351596
Iteration 108, loss = 0.59452105
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 5/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(24, 24, 24);, score=0.677 total time= 1.1min
Iteration 1, loss = 2.12086685
Iteration 2, loss = 1.00724009
Iteration 3, loss = 0.94852158
Iteration 4, loss = 0.75780056
Iteration 5, loss = 1.09606361
Iteration 6, loss = 0.88014485
Iteration 7, loss = 0.77823489
Iteration 8, loss = 0.81806084
Iteration 9, loss = 0.74874647
Iteration 10, loss = 0.72398173
Iteration 11, loss = 0.76417262
Iteration 12, loss = 0.69546797
Iteration 13, loss = 0.88942429
Iteration 14, loss = 0.74988800
Iteration 15, loss = 0.75285790
Iteration 16, loss = 0.68703532
Iteration 17, loss = 0.80178672
Iteration 18, loss = 0.74196267
Iteration 19, loss = 0.77327978
Iteration 20, loss = 0.70750157
Iteration 21, loss = 0.75962883
Iteration 22, loss = 0.72290063
Iteration 23, loss = 0.73303157
Iteration 24, loss = 0.70073800
Iteration 25, loss = 0.71566342
Iteration 26, loss = 0.69535690
Iteration 27, loss = 0.69771061
Iteration 28, loss = 0.72932392
Iteration 29, loss = 0.72154711
Iteration 30, loss = 0.66261982
Iteration 31, loss = 0.66664933
Iteration 32, loss = 0.72166770
Iteration 33, loss = 0.68791267
Iteration 34, loss = 0.68865997
Iteration 35, loss = 0.68507232
Iteration 36, loss = 0.70479335
Iteration 37, loss = 0.69321299
Iteration 38, loss = 0.68516019
Iteration 39, loss = 0.66656689
Iteration 40, loss = 0.70386463
Iteration 41, loss = 0.67525193
Iteration 42, loss = 0.66227765
Iteration 43, loss = 0.66971813
Iteration 44, loss = 0.67977891
Iteration 45, loss = 0.66304669
Iteration 46, loss = 0.68567319
Iteration 47, loss = 0.65844134
Iteration 48, loss = 0.65635527
Iteration 49, loss = 0.67872829
Iteration 50, loss = 0.65676253
Iteration 51, loss = 0.66857662
Iteration 52, loss = 0.65978594
Iteration 53, loss = 0.66095974
Iteration 54, loss = 0.65652720
Iteration 55, loss = 0.65241963
Iteration 56, loss = 0.66089644
Iteration 57, loss = 0.66856530
Iteration 58, loss = 0.67687869
Iteration 59, loss = 0.65167634
Iteration 60, loss = 0.65069835
Iteration 61, loss = 0.64751607
Iteration 62, loss = 0.66169808
Iteration 63, loss = 0.64351239
Iteration 64, loss = 0.64859751
Iteration 65, loss = 0.64019978
Iteration 66, loss = 0.66031169
Iteration 67, loss = 0.65050295
Iteration 68, loss = 0.65922805
Iteration 69, loss = 0.63990266
Iteration 70, loss = 0.64433131
Iteration 71, loss = 0.64136157
Iteration 72, loss = 0.64339712
Iteration 73, loss = 0.63872271
Iteration 74, loss = 0.65324482
Iteration 75, loss = 0.63997460
Iteration 76, loss = 0.63030226
Iteration 77, loss = 0.64054608
Iteration 78, loss = 0.64001051
Iteration 79, loss = 0.63873495
Iteration 80, loss = 0.63402245
Iteration 81, loss = 0.64076465
Iteration 82, loss = 0.63846674
Iteration 83, loss = 0.63829863
Iteration 84, loss = 0.63658116
Iteration 85, loss = 0.63495362
Iteration 86, loss = 0.63419381
Iteration 87, loss = 0.63879372
Iteration 88, loss = 0.64322721
Iteration 89, loss = 0.62810640
Iteration 90, loss = 0.62595400
Iteration 91, loss = 0.63254612
Iteration 92, loss = 0.63703816
Iteration 93, loss = 0.63394599
Iteration 94, loss = 0.62832280
Iteration 95, loss = 0.63971058
Iteration 96, loss = 0.63318832
Iteration 97, loss = 0.63331512
Iteration 98, loss = 0.62961932
Iteration 99, loss = 0.62072847
Iteration 100, loss = 0.63068840
Iteration 101, loss = 0.62393234
Iteration 102, loss = 0.62810277
Iteration 103, loss = 0.61913706
Iteration 104, loss = 0.62602074
Iteration 105, loss = 0.61926029
Iteration 106, loss = 0.62959058
Iteration 107, loss = 0.62770424
Iteration 108, loss = 0.62529634
Iteration 109, loss = 0.62386952
Iteration 110, loss = 0.62402057
Iteration 111, loss = 0.61938768
Iteration 112, loss = 0.62150002
Iteration 113, loss = 0.62819712
Iteration 114, loss = 0.62037566
Iteration 115, loss = 0.61711977
Iteration 116, loss = 0.61871803
Iteration 117, loss = 0.63314457
Iteration 118, loss = 0.62186164
Iteration 119, loss = 0.62047084
Iteration 120, loss = 0.62314387
Iteration 121, loss = 0.61747637
Iteration 122, loss = 0.62020409
Iteration 123, loss = 0.61746958
Iteration 124, loss = 0.61534052
Iteration 125, loss = 0.62329530
Iteration 126, loss = 0.62311687
Iteration 127, loss = 0.61444676
Iteration 128, loss = 0.61939046
Iteration 129, loss = 0.61912051
Iteration 130, loss = 0.61562133
Iteration 131, loss = 0.61488210
Iteration 132, loss = 0.61625249
Iteration 133, loss = 0.61673233
Iteration 134, loss = 0.61618710
Iteration 135, loss = 0.62283095
Iteration 136, loss = 0.61720856
Iteration 137, loss = 0.61656127
Iteration 138, loss = 0.61055070
Iteration 139, loss = 0.61967564
Iteration 140, loss = 0.61170568
Iteration 141, loss = 0.61488650
Iteration 142, loss = 0.61371295
Iteration 143, loss = 0.61232059
Iteration 144, loss = 0.61774824
Iteration 145, loss = 0.61120995
Iteration 146, loss = 0.61317506
Iteration 147, loss = 0.61246933
Iteration 148, loss = 0.61300899
Iteration 149, loss = 0.61152755
Iteration 150, loss = 0.61062630
Iteration 151, loss = 0.61444506
Iteration 152, loss = 0.61247542
Iteration 153, loss = 0.60842699
Iteration 154, loss = 0.60988269
Iteration 155, loss = 0.61583245
Iteration 156, loss = 0.61034770
Iteration 157, loss = 0.60998924
Iteration 158, loss = 0.61150656
Iteration 159, loss = 0.60827535
Iteration 160, loss = 0.61023108
Iteration 161, loss = 0.61269738
Iteration 162, loss = 0.60909988
Iteration 163, loss = 0.60777463
Iteration 164, loss = 0.60691797
Iteration 165, loss = 0.61001455
Iteration 166, loss = 0.61028856
Iteration 167, loss = 0.60822565
Iteration 168, loss = 0.60709942
Iteration 169, loss = 0.60849776
Iteration 170, loss = 0.61115519
Iteration 171, loss = 0.60675898
Iteration 172, loss = 0.60976417
Iteration 173, loss = 0.60481409
Iteration 174, loss = 0.60708544
Iteration 175, loss = 0.60682817
Iteration 176, loss = 0.60641233
Iteration 177, loss = 0.60682190
Iteration 178, loss = 0.61023459
Iteration 179, loss = 0.60925743
Iteration 180, loss = 0.60543881
Iteration 181, loss = 0.60369379
Iteration 182, loss = 0.60673846
Iteration 183, loss = 0.60531383
Iteration 184, loss = 0.60373825
Iteration 185, loss = 0.59615394
Iteration 186, loss = 0.59052390
Iteration 187, loss = 0.58675179
Iteration 188, loss = 0.58102764

Iteration 189, loss = 0.57072494
Iteration 190, loss = 0.55993202
Iteration 191, loss = 0.54648698
Iteration 192, loss = 0.53172897
Iteration 193, loss = 0.50753400
Iteration 194, loss = 0.49252053
Iteration 195, loss = 0.46443462
Iteration 196, loss = 0.43987773
Iteration 197, loss = 0.40974701
Iteration 198, loss = 0.37944204
Iteration 199, loss = 0.35610636
Iteration 200, loss = 0.33200872
Iteration 201, loss = 0.31186961
Iteration 202, loss = 0.30285347
Iteration 203, loss = 0.28860109
Iteration 204, loss = 0.27393173
Iteration 205, loss = 0.26784785
Iteration 206, loss = 0.27455159
Iteration 207, loss = 0.26087918
Iteration 208, loss = 0.25230335
Iteration 209, loss = 0.25310467
Iteration 210, loss = 0.25217189
Iteration 211, loss = 0.24948770
Iteration 212, loss = 0.24302869
Iteration 213, loss = 0.25649904
Iteration 214, loss = 0.24825843
Iteration 215, loss = 0.25264451
Iteration 216, loss = 0.24307183
Iteration 217, loss = 0.24257429
Iteration 218, loss = 0.24686994
Iteration 219, loss = 0.24436556
Iteration 220, loss = 0.24766795
Iteration 221, loss = 0.24448545
Iteration 222, loss = 0.24372273
Iteration 223, loss = 0.23995989
Iteration 224, loss = 0.24014349
Iteration 225, loss = 0.23904102
Iteration 226, loss = 0.25230481
Iteration 227, loss = 0.24142028
Iteration 228, loss = 0.25242424
Iteration 229, loss = 0.24187776
Iteration 230, loss = 0.23931024
Iteration 231, loss = 0.23796753
Iteration 232, loss = 0.24703756
Iteration 233, loss = 0.24144117
Iteration 234, loss = 0.24652374
Iteration 235, loss = 0.24188640
Iteration 236, loss = 0.23871760
Iteration 237, loss = 0.23907554
Iteration 238, loss = 0.24238201
Iteration 239, loss = 0.23844998
Iteration 240, loss = 0.24151328
Iteration 241, loss = 0.24108629
Iteration 242, loss = 0.24488463
Iteration 243, loss = 0.23599593
Iteration 244, loss = 0.24713029
Iteration 245, loss = 0.24000160
Iteration 246, loss = 0.24160604
Iteration 247, loss = 0.24662940
Iteration 248, loss = 0.23405140
Iteration 249, loss = 0.24779399
Iteration 250, loss = 0.23990736
Iteration 251, loss = 0.24086322
Iteration 252, loss = 0.23859946
Iteration 253, loss = 0.24094378
Iteration 254, loss = 0.23826564
Iteration 255, loss = 0.23963356
Iteration 256, loss = 0.23986987
Iteration 257, loss = 0.24090926
Iteration 258, loss = 0.23819518
Iteration 259, loss = 0.24256984
Iteration 260, loss = 0.23721876
Iteration 261, loss = 0.24288915
Iteration 262, loss = 0.24623697
Iteration 263, loss = 0.23957119
Iteration 264, loss = 0.24283039
Iteration 265, loss = 0.24426673
Iteration 266, loss = 0.23873264
Iteration 267, loss = 0.23362053
Iteration 268, loss = 0.23379925
Iteration 269, loss = 0.23757025
Iteration 270, loss = 0.24263734
Iteration 271, loss = 0.24329340
Iteration 272, loss = 0.23786885
Iteration 273, loss = 0.24023619
Iteration 274, loss = 0.24538668
Iteration 275, loss = 0.24295732
Iteration 276, loss = 0.23952531
Iteration 277, loss = 0.23465999
Iteration 278, loss = 0.23869285
Iteration 279, loss = 0.23719512
Iteration 280, loss = 0.23999719
Iteration 281, loss = 0.24329095
Iteration 282, loss = 0.23780772
Iteration 283, loss = 0.23883942
Iteration 284, loss = 0.24104003
Iteration 285, loss = 0.24372687
Iteration 286, loss = 0.24258016
Iteration 287, loss = 0.23416097
Iteration 288, loss = 0.23903814
Iteration 289, loss = 0.24600599
Iteration 290, loss = 0.23320146
Iteration 291, loss = 0.24410721
Iteration 292, loss = 0.24524356
Iteration 293, loss = 0.23657052
Iteration 294, loss = 0.24200776
Iteration 295, loss = 0.23952142
Iteration 296, loss = 0.23586987
Iteration 297, loss = 0.24138670
Iteration 298, loss = 0.23681677
Iteration 299, loss = 0.23454606
Iteration 300, loss = 0.23515564
Iteration 301, loss = 0.24249427
Iteration 302, loss = 0.23566264
Iteration 303, loss = 0.24112288
Iteration 304, loss = 0.23926606
Iteration 305, loss = 0.23398735
Iteration 306, loss = 0.23726151
Iteration 307, loss = 0.23960647
Iteration 308, loss = 0.24266750
Iteration 309, loss = 0.24022761
Iteration 310, loss = 0.23207924
Iteration 311, loss = 0.23379969
Iteration 312, loss = 0.23690848
Iteration 313, loss = 0.24017856
Iteration 314, loss = 0.23917279
Iteration 315, loss = 0.23647944
Iteration 316, loss = 0.23061140
Iteration 317, loss = 0.23649390
Iteration 318, loss = 0.23334181
Iteration 319, loss = 0.23111252
Iteration 320, loss = 0.23367361
Iteration 321, loss = 0.23134820
Iteration 322, loss = 0.23056552
Iteration 323, loss = 0.23096276
Iteration 324, loss = 0.22772858
Iteration 325, loss = 0.22343293
Iteration 326, loss = 0.22375139
Iteration 327, loss = 0.22556511
Iteration 328, loss = 0.22112361
Iteration 329, loss = 0.21613963
Iteration 330, loss = 0.22220812
Iteration 331, loss = 0.21639345
Iteration 332, loss = 0.21917455
Iteration 333, loss = 0.21545006
Iteration 334, loss = 0.21793302
Iteration 335, loss = 0.21467969
Iteration 336, loss = 0.21083962
Iteration 337, loss = 0.21522561
Iteration 338, loss = 0.20610565
Iteration 339, loss = 0.20552162
Iteration 340, loss = 0.20962237
Iteration 341, loss = 0.19997120
Iteration 342, loss = 0.20497297
Iteration 343, loss = 0.19802315
Iteration 344, loss = 0.20096589
Iteration 345, loss = 0.20706458
Iteration 346, loss = 0.19905187
Iteration 347, loss = 0.19407033
Iteration 348, loss = 0.19586597
Iteration 349, loss = 0.19444984
Iteration 350, loss = 0.19354962
Iteration 351, loss = 0.19535315
Iteration 352, loss = 0.19321479
Iteration 353, loss = 0.18938359
Iteration 354, loss = 0.19324096
Iteration 355, loss = 0.18485579
Iteration 356, loss = 0.18709399
Iteration 357, loss = 0.18952287
Iteration 358, loss = 0.18559410
Iteration 359, loss = 0.18550795
Iteration 360, loss = 0.18957070
Iteration 361, loss = 0.18802431
Iteration 362, loss = 0.17884104
Iteration 363, loss = 0.17835366
Iteration 364, loss = 0.18399709
Iteration 365, loss = 0.18023148
Iteration 366, loss = 0.18321234
Iteration 367, loss = 0.17759476
Iteration 368, loss = 0.17417092
Iteration 369, loss = 0.17421169
Iteration 370, loss = 0.17293281
Iteration 371, loss = 0.17741932
Iteration 372, loss = 0.17374833
Iteration 373, loss = 0.17437372
Iteration 374, loss = 0.17322255
Iteration 375, loss = 0.17379554
Iteration 376, loss = 0.17820973
Iteration 377, loss = 0.16805331
Iteration 378, loss = 0.16883275
Iteration 379, loss = 0.16232688
Iteration 380, loss = 0.17105459
Iteration 381, loss = 0.16464206
Iteration 382, loss = 0.16774647
Iteration 383, loss = 0.17402870
Iteration 384, loss = 0.16628693
Iteration 385, loss = 0.16575665
Iteration 386, loss = 0.16758325
Iteration 387, loss = 0.16454553
Iteration 388, loss = 0.16025664
Iteration 389, loss = 0.16707986
Iteration 390, loss = 0.16130768
Iteration 391, loss = 0.16687877
Iteration 392, loss = 0.16584614
Iteration 393, loss = 0.16189940
Iteration 394, loss = 0.15819011
Iteration 395, loss = 0.16299349
Iteration 396, loss = 0.16204587
Iteration 397, loss = 0.16124738
Iteration 398, loss = 0.16231001
Iteration 399, loss = 0.16011428
Iteration 400, loss = 0.16306132
Iteration 401, loss = 0.15941125
Iteration 402, loss = 0.15732104
Iteration 403, loss = 0.16324383
Iteration 404, loss = 0.16085958
Iteration 405, loss = 0.16215433
Iteration 406, loss = 0.15879109
Iteration 407, loss = 0.16183339
Iteration 408, loss = 0.16087890
Iteration 409, loss = 0.15346593
Iteration 410, loss = 0.15552825
Iteration 411, loss = 0.15761960
Iteration 412, loss = 0.16086371
Iteration 413, loss = 0.15465857
Iteration 414, loss = 0.16385364
Iteration 415, loss = 0.15370722
Iteration 416, loss = 0.16017800
Iteration 417, loss = 0.15233504
Iteration 418, loss = 0.15585140
Iteration 419, loss = 0.15322533
Iteration 420, loss = 0.15963567
Iteration 421, loss = 0.15459401
Iteration 422, loss = 0.16659149
Iteration 423, loss = 0.15044061
Iteration 424, loss = 0.15693722
Iteration 425, loss = 0.15006729
Iteration 426, loss = 0.15245635
Iteration 427, loss = 0.16127878
Iteration 428, loss = 0.15531460
Iteration 429, loss = 0.15417469
Iteration 430, loss = 0.14967509
Iteration 431, loss = 0.15414281
Iteration 432, loss = 0.15085105
Iteration 433, loss = 0.15603104
Iteration 434, loss = 0.15122964
Iteration 435, loss = 0.14860273
Iteration 436, loss = 0.16156412
Iteration 437, loss = 0.16031324

Iteration 438, loss = 0.14313353
Iteration 439, loss = 0.15244691
Iteration 440, loss = 0.14980282
Iteration 441, loss = 0.15113937
Iteration 442, loss = 0.15623304
Iteration 443, loss = 0.14822952
Iteration 444, loss = 0.14910949
Iteration 445, loss = 0.16019785
Iteration 446, loss = 0.15426553
Iteration 447, loss = 0.14924446
Iteration 448, loss = 0.14754713
Iteration 449, loss = 0.15265043
Iteration 450, loss = 0.14328233
Iteration 451, loss = 0.15160434
Iteration 452, loss = 0.15678617
Iteration 453, loss = 0.14446425
Iteration 454, loss = 0.15535283
Iteration 455, loss = 0.14756680
Iteration 456, loss = 0.14656538
Iteration 457, loss = 0.14534912
Iteration 458, loss = 0.15406604
Iteration 459, loss = 0.15121637
Iteration 460, loss = 0.15241872
Iteration 461, loss = 0.14942337
Iteration 462, loss = 0.14457423
Iteration 463, loss = 0.15065951
Iteration 464, loss = 0.15103321
Iteration 465, loss = 0.14588237
Iteration 466, loss = 0.14471477
Iteration 467, loss = 0.15065220
Iteration 468, loss = 0.15007860
Iteration 469, loss = 0.14369594
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 1/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(32, 32);, score=0.973 total time= 2.3min
Iteration 1, loss = 2.72880773
Iteration 2, loss = 1.14934238
Iteration 3, loss = 0.84268047
Iteration 4, loss = 0.95446648
Iteration 5, loss = 0.84220996
Iteration 6, loss = 0.81108319
Iteration 7, loss = 0.80826466
Iteration 8, loss = 0.84251991
Iteration 9, loss = 0.75257906
Iteration 10, loss = 0.76738144
Iteration 11, loss = 0.79815696
Iteration 12, loss = 0.75670622
Iteration 13, loss = 0.76203576
Iteration 14, loss = 0.75908611
Iteration 15, loss = 0.75921844
Iteration 16, loss = 0.70752218
Iteration 17, loss = 0.73961114
Iteration 18, loss = 0.73770146
Iteration 19, loss = 0.72467675
Iteration 20, loss = 0.73522353
Iteration 21, loss = 0.70326592
Iteration 22, loss = 0.70594867
Iteration 23, loss = 0.70313136
Iteration 24, loss = 0.72257129
Iteration 25, loss = 0.70068943
Iteration 26, loss = 0.69117639
Iteration 27, loss = 0.70359073
Iteration 28, loss = 0.69006244
Iteration 29, loss = 0.67496747
Iteration 30, loss = 0.67157916
Iteration 31, loss = 0.66547030
Iteration 32, loss = 0.68237001
Iteration 33, loss = 0.67195524
Iteration 34, loss = 0.67306302
Iteration 35, loss = 0.67022732
Iteration 36, loss = 0.67408825
Iteration 37, loss = 0.66764951
Iteration 38, loss = 0.65693423
Iteration 39, loss = 0.65383309
Iteration 40, loss = 0.66565173
Iteration 41, loss = 0.65216834
Iteration 42, loss = 0.66108231
Iteration 43, loss = 0.64935154
Iteration 44, loss = 0.64676804
Iteration 45, loss = 0.64989143
Iteration 46, loss = 0.64067565
Iteration 47, loss = 0.65939326
Iteration 48, loss = 0.65290822
Iteration 49, loss = 0.63863378
Iteration 50, loss = 0.64144933
Iteration 51, loss = 0.64034299
Iteration 52, loss = 0.64189783
Iteration 53, loss = 0.63277110
Iteration 54, loss = 0.64630600
Iteration 55, loss = 0.63537010
Iteration 56, loss = 0.63343724
Iteration 57, loss = 0.63200176
Iteration 58, loss = 0.63981153
Iteration 59, loss = 0.62661654
Iteration 60, loss = 0.62808306
Iteration 61, loss = 0.62682820
Iteration 62, loss = 0.62258372
Iteration 63, loss = 0.62244892
Iteration 64, loss = 0.62710675
Iteration 65, loss = 0.62163198
Iteration 66, loss = 0.62270756
Iteration 67, loss = 0.61808837
Iteration 68, loss = 0.62263334
Iteration 69, loss = 0.62312391
Iteration 70, loss = 0.61937828
Iteration 71, loss = 0.61663521
Iteration 72, loss = 0.62147664
Iteration 73, loss = 0.61667009
Iteration 74, loss = 0.61770872
Iteration 75, loss = 0.61351498
Iteration 76, loss = 0.61408797
Iteration 77, loss = 0.61586410
Iteration 78, loss = 0.61230456
Iteration 79, loss = 0.61096336
Iteration 80, loss = 0.60928133
Iteration 81, loss = 0.61380006
Iteration 82, loss = 0.61020027
Iteration 83, loss = 0.61310532
Iteration 84, loss = 0.60868838
Iteration 85, loss = 0.61002271
Iteration 86, loss = 0.60811746
Iteration 87, loss = 0.60653741
Iteration 88, loss = 0.60925766
Iteration 89, loss = 0.60832823
Iteration 90, loss = 0.60640103
Iteration 91, loss = 0.60309107
Iteration 92, loss = 0.60331729
Iteration 93, loss = 0.60432850
Iteration 94, loss = 0.60051528
Iteration 95, loss = 0.60075227
Iteration 96, loss = 0.59859521
Iteration 97, loss = 0.60027421
Iteration 98, loss = 0.59878329
Iteration 99, loss = 0.59875147
Iteration 100, loss = 0.59450974
Iteration 101, loss = 0.59297951
Iteration 102, loss = 0.59482649
Iteration 103, loss = 0.58512701
Iteration 104, loss = 0.56635721
Iteration 105, loss = 0.56973710
Iteration 106, loss = 0.55612027
Iteration 107, loss = 0.55409271
Iteration 108, loss = 0.50287459
Iteration 109, loss = 0.40194760
Iteration 110, loss = 0.34491079
Iteration 111, loss = 0.31122514
Iteration 112, loss = 0.29498454
Iteration 113, loss = 0.29124540
Iteration 114, loss = 0.28267476
Iteration 115, loss = 0.27488198
Iteration 116, loss = 0.28162564
Iteration 117, loss = 0.28894598
Iteration 118, loss = 0.27304218
Iteration 119, loss = 0.26560559
Iteration 120, loss = 0.25964114
Iteration 121, loss = 0.26366986
Iteration 122, loss = 0.27477823
Iteration 123, loss = 0.25499519
Iteration 124, loss = 0.26825502
Iteration 125, loss = 0.25464453
Iteration 126, loss = 0.26928009
Iteration 127, loss = 0.26942953
Iteration 128, loss = 0.25700679
Iteration 129, loss = 0.26212972
Iteration 130, loss = 0.25454414
Iteration 131, loss = 0.24536633
Iteration 132, loss = 0.25320888
Iteration 133, loss = 0.25152738
Iteration 134, loss = 0.23864999
Iteration 135, loss = 0.25776567
Iteration 136, loss = 0.24534385
Iteration 137, loss = 0.24847362
Iteration 138, loss = 0.24758752
Iteration 139, loss = 0.24531541
Iteration 140, loss = 0.23928403
Iteration 141, loss = 0.23729401
Iteration 142, loss = 0.24304712
Iteration 143, loss = 0.24520154
Iteration 144, loss = 0.25055677
Iteration 145, loss = 0.24554411
Iteration 146, loss = 0.24633933
Iteration 147, loss = 0.23361743
Iteration 148, loss = 0.24501348
Iteration 149, loss = 0.23101968
Iteration 150, loss = 0.23180676
Iteration 151, loss = 0.22394748
Iteration 152, loss = 0.24097580
Iteration 153, loss = 0.23488593
Iteration 154, loss = 0.23792166
Iteration 155, loss = 0.21389403
Iteration 156, loss = 0.21978628
Iteration 157, loss = 0.24377918
Iteration 158, loss = 0.22824029
Iteration 159, loss = 0.20942000
Iteration 160, loss = 0.22285083
Iteration 161, loss = 0.22472764
Iteration 162, loss = 0.20035085
Iteration 163, loss = 0.21481053
Iteration 164, loss = 0.21659073
Iteration 165, loss = 0.21096797
Iteration 166, loss = 0.20110878
Iteration 167, loss = 0.19478392
Iteration 168, loss = 0.19471346
Iteration 169, loss = 0.19029718
Iteration 170, loss = 0.20270264
Iteration 171, loss = 0.18580936
Iteration 172, loss = 0.23508157
Iteration 173, loss = 0.22218114
Iteration 174, loss = 0.21924986
Iteration 175, loss = 0.21035793
Iteration 176, loss = 0.21804203
Iteration 177, loss = 0.20906757
Iteration 178, loss = 0.21036998
Iteration 179, loss = 0.21768924
Iteration 180, loss = 0.21057053
Iteration 181, loss = 0.21286745
Iteration 182, loss = 0.20431865
Iteration 183, loss = 0.21286305
Iteration 184, loss = 0.22539422
Iteration 185, loss = 0.20142730
Iteration 186, loss = 0.20833868
Iteration 187, loss = 0.20286806
Iteration 188, loss = 0.22055278
Iteration 189, loss = 0.19753785
Iteration 190, loss = 0.21246880
Iteration 191, loss = 0.20445171
Iteration 192, loss = 0.20427523
Iteration 193, loss = 0.20700850
Iteration 194, loss = 0.21307846
Iteration 195, loss = 0.19981475
Iteration 196, loss = 0.23226392
Iteration 197, loss = 0.22837468
Iteration 198, loss = 0.22953563
Iteration 199, loss = 0.23638034
Iteration 200, loss = 0.23253379
Iteration 201, loss = 0.22697430
Iteration 202, loss = 0.23497981
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 2/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(32, 32);, score=0.937 total time= 1.3min
Iteration 1, loss = 3.36278738
Iteration 2, loss = 1.20912785
Iteration 3, loss = 1.28429151
Iteration 4, loss = 0.90608449
Iteration 5, loss = 0.97785823
Iteration 6, loss = 0.78388865

Iteration 7, loss = 0.88526651
Iteration 8, loss = 0.83761252
Iteration 9, loss = 0.96179412
Iteration 10, loss = 0.85104128
Iteration 11, loss = 0.88816469
Iteration 12, loss = 0.84264291
Iteration 13, loss = 0.77598537
Iteration 14, loss = 0.79249298
Iteration 15, loss = 0.77846513
Iteration 16, loss = 0.79544270
Iteration 17, loss = 0.74700642
Iteration 18, loss = 0.78380245
Iteration 19, loss = 0.72754818
Iteration 20, loss = 0.74730216
Iteration 21, loss = 0.71601270
Iteration 22, loss = 0.73364297
Iteration 23, loss = 0.73573602
Iteration 24, loss = 0.74093561
Iteration 25, loss = 0.76887009
Iteration 26, loss = 0.73675599
Iteration 27, loss = 0.73222201
Iteration 28, loss = 0.66321134
Iteration 29, loss = 0.73240334
Iteration 30, loss = 0.69062639
Iteration 31, loss = 0.70605086
Iteration 32, loss = 0.69539598
Iteration 33, loss = 0.71894634
Iteration 34, loss = 0.68449778
Iteration 35, loss = 0.70634427
Iteration 36, loss = 0.66343996
Iteration 37, loss = 0.68113275
Iteration 38, loss = 0.67255807
Iteration 39, loss = 0.67117070
Iteration 40, loss = 0.69549207
Iteration 41, loss = 0.70854972
Iteration 42, loss = 0.67258828
Iteration 43, loss = 0.66341914
Iteration 44, loss = 0.68246790
Iteration 45, loss = 0.67902674
Iteration 46, loss = 0.67266827
Iteration 47, loss = 0.67450594
Iteration 48, loss = 0.66185167
Iteration 49, loss = 0.67840300
Iteration 50, loss = 0.66017354
Iteration 51, loss = 0.66589457
Iteration 52, loss = 0.66352395
Iteration 53, loss = 0.65291538
Iteration 54, loss = 0.67254710
Iteration 55, loss = 0.65172362
Iteration 56, loss = 0.65847501
Iteration 57, loss = 0.65311533
Iteration 58, loss = 0.65937469
Iteration 59, loss = 0.65997184
Iteration 60, loss = 0.65264915
Iteration 61, loss = 0.64129771
Iteration 62, loss = 0.65949050
Iteration 63, loss = 0.64022622
Iteration 64, loss = 0.66537449
Iteration 65, loss = 0.64936064
Iteration 66, loss = 0.63797900
Iteration 67, loss = 0.63632684
Iteration 68, loss = 0.63163695
Iteration 69, loss = 0.65133436
Iteration 70, loss = 0.65285856
Iteration 71, loss = 0.64518819
Iteration 72, loss = 0.63701009
Iteration 73, loss = 0.63321656
Iteration 74, loss = 0.64451645
Iteration 75, loss = 0.63323229
Iteration 76, loss = 0.63772994
Iteration 77, loss = 0.63007223
Iteration 78, loss = 0.62946700
Iteration 79, loss = 0.64764895
Iteration 80, loss = 0.63061268
Iteration 81, loss = 0.63045395
Iteration 82, loss = 0.63565414
Iteration 83, loss = 0.63290615
Iteration 84, loss = 0.62534060
Iteration 85, loss = 0.63706004
Iteration 86, loss = 0.62319400
Iteration 87, loss = 0.62893003
Iteration 88, loss = 0.62159205
Iteration 89, loss = 0.62391799
Iteration 90, loss = 0.62328865
Iteration 91, loss = 0.63558220
Iteration 92, loss = 0.62330829
Iteration 93, loss = 0.62572696
Iteration 94, loss = 0.62766223
Iteration 95, loss = 0.62461169
Iteration 96, loss = 0.62596666
Iteration 97, loss = 0.61870646
Iteration 98, loss = 0.62686756
Iteration 99, loss = 0.62945412
Iteration 100, loss = 0.62175914
Iteration 101, loss = 0.61775259
Iteration 102, loss = 0.61603539
Iteration 103, loss = 0.61828649
Iteration 104, loss = 0.61713851
Iteration 105, loss = 0.62212376
Iteration 106, loss = 0.62213094
Iteration 107, loss = 0.61668613
Iteration 108, loss = 0.62084411
Iteration 109, loss = 0.62137720
Iteration 110, loss = 0.62003448
Iteration 111, loss = 0.61381584
Iteration 112, loss = 0.61512956
Iteration 113, loss = 0.61796786
Iteration 114, loss = 0.61442393
Iteration 115, loss = 0.62139605
Iteration 116, loss = 0.61400744
Iteration 117, loss = 0.61213717
Iteration 118, loss = 0.61656016
Iteration 119, loss = 0.61202948
Iteration 120, loss = 0.61520040
Iteration 121, loss = 0.61235122
Iteration 122, loss = 0.60938041
Iteration 123, loss = 0.61149859
Iteration 124, loss = 0.61279353
Iteration 125, loss = 0.60935122
Iteration 126, loss = 0.61298482
Iteration 127, loss = 0.61157966
Iteration 128, loss = 0.61263096
Iteration 129, loss = 0.61116328
Iteration 130, loss = 0.61140093
Iteration 131, loss = 0.61029599
Iteration 132, loss = 0.61052170
Iteration 133, loss = 0.61136835
Iteration 134, loss = 0.61068320
Iteration 135, loss = 0.61001677
Iteration 136, loss = 0.60859603
Iteration 137, loss = 0.61030880
Iteration 138, loss = 0.60836407
Iteration 139, loss = 0.60773614
Iteration 140, loss = 0.60628640
Iteration 141, loss = 0.60725211
Iteration 142, loss = 0.60918440
Iteration 143, loss = 0.60487136
Iteration 144, loss = 0.60611902
Iteration 145, loss = 0.60656189
Iteration 146, loss = 0.60997372
Iteration 147, loss = 0.60625873
Iteration 148, loss = 0.60892575
Iteration 149, loss = 0.60484285
Iteration 150, loss = 0.60741557
Iteration 151, loss = 0.60518717
Iteration 152, loss = 0.60733392
Iteration 153, loss = 0.60541127
Iteration 154, loss = 0.60428342
Iteration 155, loss = 0.60527653
Iteration 156, loss = 0.60566022
Iteration 157, loss = 0.60265846
Iteration 158, loss = 0.60477578
Iteration 159, loss = 0.60440662
Iteration 160, loss = 0.60409804
Iteration 161, loss = 0.60511102
Iteration 162, loss = 0.60560288
Iteration 163, loss = 0.60461952
Iteration 164, loss = 0.60425568
Iteration 165, loss = 0.60343065
Iteration 166, loss = 0.60358855
Iteration 167, loss = 0.60161165
Iteration 168, loss = 0.60189099
Iteration 169, loss = 0.60297473
Iteration 170, loss = 0.60147629
Iteration 171, loss = 0.60141597
Iteration 172, loss = 0.60315131
Iteration 173, loss = 0.60002286
Iteration 174, loss = 0.60181699
Iteration 175, loss = 0.59993444
Iteration 176, loss = 0.60235701
Iteration 177, loss = 0.59991711
Iteration 178, loss = 0.60298669
Iteration 179, loss = 0.59885834
Iteration 180, loss = 0.59920703
Iteration 181, loss = 0.59957585
Iteration 182, loss = 0.59686482
Iteration 183, loss = 0.59823423
Iteration 184, loss = 0.59406420
Iteration 185, loss = 0.58430329
Iteration 186, loss = 0.57316168
Iteration 187, loss = 0.55198169
Iteration 188, loss = 0.52712324
Iteration 189, loss = 0.49469851
Iteration 190, loss = 0.45211214
Iteration 191, loss = 0.41669457
Iteration 192, loss = 0.38734355
Iteration 193, loss = 0.36043762
Iteration 194, loss = 0.34811788
Iteration 195, loss = 0.34029588
Iteration 196, loss = 0.32096826
Iteration 197, loss = 0.31711252
Iteration 198, loss = 0.30757786
Iteration 199, loss = 0.30139412
Iteration 200, loss = 0.29347709
Iteration 201, loss = 0.27619463
Iteration 202, loss = 0.26901092
Iteration 203, loss = 0.26068808
Iteration 204, loss = 0.25174749
Iteration 205, loss = 0.25526019
Iteration 206, loss = 0.24801047
Iteration 207, loss = 0.25247504
Iteration 208, loss = 0.24690910
Iteration 209, loss = 0.24305009
Iteration 210, loss = 0.23840548
Iteration 211, loss = 0.24412730
Iteration 212, loss = 0.23950691
Iteration 213, loss = 0.24009510
Iteration 214, loss = 0.23899300
Iteration 215, loss = 0.23909254
Iteration 216, loss = 0.23397176
Iteration 217, loss = 0.24007161
Iteration 218, loss = 0.23778019
Iteration 219, loss = 0.22870540
Iteration 220, loss = 0.23204920
Iteration 221, loss = 0.23511221
Iteration 222, loss = 0.23791815
Iteration 223, loss = 0.23877840
Iteration 224, loss = 0.23081148
Iteration 225, loss = 0.23220613
Iteration 226, loss = 0.24026898
Iteration 227, loss = 0.23457845
Iteration 228, loss = 0.22892312
Iteration 229, loss = 0.23453249
Iteration 230, loss = 0.23718297
Iteration 231, loss = 0.22662002
Iteration 232, loss = 0.23332375
Iteration 233, loss = 0.23100370
Iteration 234, loss = 0.23423991
Iteration 235, loss = 0.23314437
Iteration 236, loss = 0.23529215
Iteration 237, loss = 0.23628143
Iteration 238, loss = 0.22883451
Iteration 239, loss = 0.22943615
Iteration 240, loss = 0.23015406
Iteration 241, loss = 0.23296790
Iteration 242, loss = 0.22921492
Iteration 243, loss = 0.22960259
Iteration 244, loss = 0.23337499
Iteration 245, loss = 0.22863753
Iteration 246, loss = 0.23025809
Iteration 247, loss = 0.22504681
Iteration 248, loss = 0.22924692
Iteration 249, loss = 0.22885514
Iteration 250, loss = 0.23858999
Iteration 251, loss = 0.22584195
Iteration 252, loss = 0.23099544
Iteration 253, loss = 0.22978264
Iteration 254, loss = 0.22397454
Iteration 255, loss = 0.22738865
Iteration 256, loss = 0.22794948
Iteration 257, loss = 0.22906629
Iteration 258, loss = 0.23058930

Iteration 259, loss = 0.22788687
Iteration 260, loss = 0.23287955
Iteration 261, loss = 0.23224992
Iteration 262, loss = 0.23018666
Iteration 263, loss = 0.23345063
Iteration 264, loss = 0.23019565
Iteration 265, loss = 0.22635662
Iteration 266, loss = 0.23588768
Iteration 267, loss = 0.22672287
Iteration 268, loss = 0.23076358
Iteration 269, loss = 0.23213242
Iteration 270, loss = 0.22550701
Iteration 271, loss = 0.22627177
Iteration 272, loss = 0.22074885
Iteration 273, loss = 0.23909792
Iteration 274, loss = 0.22883126
Iteration 275, loss = 0.22644214
Iteration 276, loss = 0.22520894
Iteration 277, loss = 0.22303136
Iteration 278, loss = 0.22920056
Iteration 279, loss = 0.22460049
Iteration 280, loss = 0.23564728
Iteration 281, loss = 0.22591678
Iteration 282, loss = 0.23381920
Iteration 283, loss = 0.22726199
Iteration 284, loss = 0.22550177
Iteration 285, loss = 0.22638934
Iteration 286, loss = 0.22903315
Iteration 287, loss = 0.22519689
Iteration 288, loss = 0.21999930
Iteration 289, loss = 0.22867425
Iteration 290, loss = 0.22345288
Iteration 291, loss = 0.22403443
Iteration 292, loss = 0.22863774
Iteration 293, loss = 0.22505824
Iteration 294, loss = 0.22854553
Iteration 295, loss = 0.22714982
Iteration 296, loss = 0.22352968
Iteration 297, loss = 0.22680802
Iteration 298, loss = 0.22911148
Iteration 299, loss = 0.22238867
Iteration 300, loss = 0.22789357
Iteration 301, loss = 0.22487334
Iteration 302, loss = 0.22697467
Iteration 303, loss = 0.22438102
Iteration 304, loss = 0.22206006
Iteration 305, loss = 0.22414157
Iteration 306, loss = 0.22653100
Iteration 307, loss = 0.22202214
Iteration 308, loss = 0.22168147
Iteration 309, loss = 0.22693290
Iteration 310, loss = 0.23004952
Iteration 311, loss = 0.22909031
Iteration 312, loss = 0.21970260
Iteration 313, loss = 0.22398515
Iteration 314, loss = 0.22495471
Iteration 315, loss = 0.23057818
Iteration 316, loss = 0.21882942
Iteration 317, loss = 0.22300430
Iteration 318, loss = 0.22381968
Iteration 319, loss = 0.23180443
Iteration 320, loss = 0.22391183
Iteration 321, loss = 0.22187340
Iteration 322, loss = 0.22699482
Iteration 323, loss = 0.22484294
Iteration 324, loss = 0.22492064
Iteration 325, loss = 0.22902457
Iteration 326, loss = 0.22251772
Iteration 327, loss = 0.22201102
Iteration 328, loss = 0.22288771
Iteration 329, loss = 0.22438275
Iteration 330, loss = 0.22044878
Iteration 331, loss = 0.22983127
Iteration 332, loss = 0.22913150
Iteration 333, loss = 0.22575886
Iteration 334, loss = 0.22545499
Iteration 335, loss = 0.22436064
Iteration 336, loss = 0.22420488
Iteration 337, loss = 0.21896433
Iteration 338, loss = 0.22659006
Iteration 339, loss = 0.22471091
Iteration 340, loss = 0.22145196
Iteration 341, loss = 0.21905084
Iteration 342, loss = 0.23119602
Iteration 343, loss = 0.22073095
Iteration 344, loss = 0.22130623
Iteration 345, loss = 0.22445606
Iteration 346, loss = 0.22059518
Iteration 347, loss = 0.22347194
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 3/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(32, 32);, score=0.949 total time= 1.9min
Iteration 1, loss = 1.82998883
Iteration 2, loss = 0.86881873
Iteration 3, loss = 0.77847606
Iteration 4, loss = 0.78854999
Iteration 5, loss = 0.70858321
Iteration 6, loss = 0.70878447
Iteration 7, loss = 0.73157457
Iteration 8, loss = 0.70795111
Iteration 9, loss = 0.69511479
Iteration 10, loss = 0.68203747
Iteration 11, loss = 0.72961902
Iteration 12, loss = 0.67037024
Iteration 13, loss = 0.71064212
Iteration 14, loss = 0.68506140
Iteration 15, loss = 0.68738912
Iteration 16, loss = 0.69187177
Iteration 17, loss = 0.68625437
Iteration 18, loss = 0.66223855
Iteration 19, loss = 0.68487865
Iteration 20, loss = 0.69262949
Iteration 21, loss = 0.64287197
Iteration 22, loss = 0.66631195
Iteration 23, loss = 0.69038848
Iteration 24, loss = 0.65594635
Iteration 25, loss = 0.67716029
Iteration 26, loss = 0.65809722
Iteration 27, loss = 0.65715072
Iteration 28, loss = 0.65030825
Iteration 29, loss = 0.65354859
Iteration 30, loss = 0.66116005
Iteration 31, loss = 0.65460507
Iteration 32, loss = 0.65252153
Iteration 33, loss = 0.65389701
Iteration 34, loss = 0.63252917
Iteration 35, loss = 0.63712264
Iteration 36, loss = 0.66413898
Iteration 37, loss = 0.63918114
Iteration 38, loss = 0.64261953
Iteration 39, loss = 0.64640787
Iteration 40, loss = 0.64799092
Iteration 41, loss = 0.64296758
Iteration 42, loss = 0.65668882
Iteration 43, loss = 0.64143554
Iteration 44, loss = 0.63418080
Iteration 45, loss = 0.62563914
Iteration 46, loss = 0.63522767
Iteration 47, loss = 0.63832464
Iteration 48, loss = 0.63386320
Iteration 49, loss = 0.62964707
Iteration 50, loss = 0.62725555
Iteration 51, loss = 0.62885284
Iteration 52, loss = 0.63009843
Iteration 53, loss = 0.62539245
Iteration 54, loss = 0.62469711
Iteration 55, loss = 0.62194536
Iteration 56, loss = 0.62728085
Iteration 57, loss = 0.63317353
Iteration 58, loss = 0.61955201
Iteration 59, loss = 0.61889952
Iteration 60, loss = 0.62542603
Iteration 61, loss = 0.61711767
Iteration 62, loss = 0.61980412
Iteration 63, loss = 0.61822612
Iteration 64, loss = 0.62297291
Iteration 65, loss = 0.61742450
Iteration 66, loss = 0.61727407
Iteration 67, loss = 0.61488862
Iteration 68, loss = 0.61569019
Iteration 69, loss = 0.61569164
Iteration 70, loss = 0.61099757
Iteration 71, loss = 0.61447719
Iteration 72, loss = 0.61578446
Iteration 73, loss = 0.61181630
Iteration 74, loss = 0.61254058
Iteration 75, loss = 0.61339567
Iteration 76, loss = 0.60944507
Iteration 77, loss = 0.60853298
Iteration 78, loss = 0.61069708
Iteration 79, loss = 0.61173371
Iteration 80, loss = 0.61222958
Iteration 81, loss = 0.60962105
Iteration 82, loss = 0.60712370
Iteration 83, loss = 0.61023669
Iteration 84, loss = 0.60466122
Iteration 85, loss = 0.60502155
Iteration 86, loss = 0.60116159
Iteration 87, loss = 0.60068544
Iteration 88, loss = 0.60330137
Iteration 89, loss = 0.59944153
Iteration 90, loss = 0.59982834
Iteration 91, loss = 0.59713148
Iteration 92, loss = 0.59552426
Iteration 93, loss = 0.60548751
Iteration 94, loss = 0.60821505
Iteration 95, loss = 0.60325588
Iteration 96, loss = 0.60224556
Iteration 97, loss = 0.60337464
Iteration 98, loss = 0.60225349
Iteration 99, loss = 0.59821602
Iteration 100, loss = 0.59888873
Iteration 101, loss = 0.60226063
Iteration 102, loss = 0.60043178
Iteration 103, loss = 0.59845570
Iteration 104, loss = 0.59982750
Iteration 105, loss = 0.60221180
Iteration 106, loss = 0.60011433
Iteration 107, loss = 0.60026173
Iteration 108, loss = 0.59689351
Iteration 109, loss = 0.59528280
Iteration 110, loss = 0.59936569
Iteration 111, loss = 0.59769176
Iteration 112, loss = 0.59778727
Iteration 113, loss = 0.59117627
Iteration 114, loss = 0.59353263
Iteration 115, loss = 0.58519962
Iteration 116, loss = 0.58489634
Iteration 117, loss = 0.58277014
Iteration 118, loss = 0.58349369
Iteration 119, loss = 0.57707992
Iteration 120, loss = 0.57742104
Iteration 121, loss = 0.57725190
Iteration 122, loss = 0.57832239
Iteration 123, loss = 0.57691942
Iteration 124, loss = 0.57418956
Iteration 125, loss = 0.57655117
Iteration 126, loss = 0.57450279
Iteration 127, loss = 0.57292829
Iteration 128, loss = 0.57252347
Iteration 129, loss = 0.57410855
Iteration 130, loss = 0.57245880
Iteration 131, loss = 0.57470442
Iteration 132, loss = 0.57253361
Iteration 133, loss = 0.57376012
Iteration 134, loss = 0.57214075
Iteration 135, loss = 0.57003023
Iteration 136, loss = 0.57475503
Iteration 137, loss = 0.57224946
Iteration 138, loss = 0.57259585
Iteration 139, loss = 0.57251614
Iteration 140, loss = 0.57202278
Iteration 141, loss = 0.57256197
Iteration 142, loss = 0.57283948
Iteration 143, loss = 0.57174158
Iteration 144, loss = 0.57137579
Iteration 145, loss = 0.57059150
Iteration 146, loss = 0.57138459
Iteration 147, loss = 0.57250983
Iteration 148, loss = 0.57005898
Iteration 149, loss = 0.57137093
Iteration 150, loss = 0.56974860
Iteration 151, loss = 0.57094200
Iteration 152, loss = 0.57137557
Iteration 153, loss = 0.57038382
Iteration 154, loss = 0.57124954
Iteration 155, loss = 0.56935855
Iteration 156, loss = 0.56985154
Iteration 157, loss = 0.56953622

Iteration 158, loss = 0.56940638
Iteration 159, loss = 0.57107303
Iteration 160, loss = 0.56896931
Iteration 161, loss = 0.57219282
Iteration 162, loss = 0.56804443
Iteration 163, loss = 0.56962917
Iteration 164, loss = 0.56876141
Iteration 165, loss = 0.57061961
Iteration 166, loss = 0.57021627
Iteration 167, loss = 0.56871792
Iteration 168, loss = 0.57057935
Iteration 169, loss = 0.56953947
Iteration 170, loss = 0.56991836
Iteration 171, loss = 0.56944775
Iteration 172, loss = 0.56902302
Iteration 173, loss = 0.56817812
Iteration 174, loss = 0.56851329
Iteration 175, loss = 0.56764765
Iteration 176, loss = 0.56796997
Iteration 177, loss = 0.56554774
Iteration 178, loss = 0.56611199
Iteration 179, loss = 0.56050197
Iteration 180, loss = 0.54262690
Iteration 181, loss = 0.45041932
Iteration 182, loss = 0.36014867
Iteration 183, loss = 0.31248167
Iteration 184, loss = 0.29384132
Iteration 185, loss = 0.28403850
Iteration 186, loss = 0.27198507
Iteration 187, loss = 0.27783151
Iteration 188, loss = 0.26690229
Iteration 189, loss = 0.26705366
Iteration 190, loss = 0.26790701
Iteration 191, loss = 0.26968274
Iteration 192, loss = 0.26408441
Iteration 193, loss = 0.25755519
Iteration 194, loss = 0.25313656
Iteration 195, loss = 0.25134603
Iteration 196, loss = 0.25456900
Iteration 197, loss = 0.25262027
Iteration 198, loss = 0.24917592
Iteration 199, loss = 0.24385661
Iteration 200, loss = 0.24436198
Iteration 201, loss = 0.25406748
Iteration 202, loss = 0.24863962
Iteration 203, loss = 0.24769487
Iteration 204, loss = 0.23140795
Iteration 205, loss = 0.23361061
Iteration 206, loss = 0.25163339
Iteration 207, loss = 0.22598577
Iteration 208, loss = 0.23536010
Iteration 209, loss = 0.22504603
Iteration 210, loss = 0.23561849
Iteration 211, loss = 0.23709418
Iteration 212, loss = 0.22062605
Iteration 213, loss = 0.22575417
Iteration 214, loss = 0.22110142
Iteration 215, loss = 0.22351328
Iteration 216, loss = 0.22222504
Iteration 217, loss = 0.21501852
Iteration 218, loss = 0.22616254
Iteration 219, loss = 0.21422918
Iteration 220, loss = 0.21174618
Iteration 221, loss = 0.22887031
Iteration 222, loss = 0.21291361
Iteration 223, loss = 0.21018304
Iteration 224, loss = 0.20919194
Iteration 225, loss = 0.21933997
Iteration 226, loss = 0.22066028
Iteration 227, loss = 0.21969086
Iteration 228, loss = 0.21489701
Iteration 229, loss = 0.21699513
Iteration 230, loss = 0.21309220
Iteration 231, loss = 0.21529880
Iteration 232, loss = 0.20615103
Iteration 233, loss = 0.21943869
Iteration 234, loss = 0.21391849
Iteration 235, loss = 0.21674093
Iteration 236, loss = 0.21640284
Iteration 237, loss = 0.20568455
Iteration 238, loss = 0.20535761
Iteration 239, loss = 0.22199822
Iteration 240, loss = 0.20557688
Iteration 241, loss = 0.20966429
Iteration 242, loss = 0.20601334
Iteration 243, loss = 0.21156493
Iteration 244, loss = 0.20226078
Iteration 245, loss = 0.20506689
Iteration 246, loss = 0.21120082
Iteration 247, loss = 0.20413311
Iteration 248, loss = 0.20648950
Iteration 249, loss = 0.20452392
Iteration 250, loss = 0.19815930
Iteration 251, loss = 0.20807823
Iteration 252, loss = 0.21033184
Iteration 253, loss = 0.20345298
Iteration 254, loss = 0.19894269
Iteration 255, loss = 0.19670516
Iteration 256, loss = 0.20086751
Iteration 257, loss = 0.20378387
Iteration 258, loss = 0.19983427
Iteration 259, loss = 0.20773267
Iteration 260, loss = 0.19534536
Iteration 261, loss = 0.19696649
Iteration 262, loss = 0.20739616
Iteration 263, loss = 0.19798383
Iteration 264, loss = 0.20325560
Iteration 265, loss = 0.20020248
Iteration 266, loss = 0.19318875
Iteration 267, loss = 0.19457924
Iteration 268, loss = 0.20071972
Iteration 269, loss = 0.19472437
Iteration 270, loss = 0.19173719
Iteration 271, loss = 0.21313966
Iteration 272, loss = 0.19699098
Iteration 273, loss = 0.19328569
Iteration 274, loss = 0.19444096
Iteration 275, loss = 0.19905805
Iteration 276, loss = 0.19245142
Iteration 277, loss = 0.20645134
Iteration 278, loss = 0.19027299
Iteration 279, loss = 0.18728894
Iteration 280, loss = 0.20056555
Iteration 281, loss = 0.19490266
Iteration 282, loss = 0.19078319
Iteration 283, loss = 0.19247026
Iteration 284, loss = 0.19633711
Iteration 285, loss = 0.18991981
Iteration 286, loss = 0.18479248
Iteration 287, loss = 0.19342426
Iteration 288, loss = 0.19004599
Iteration 289, loss = 0.17597173
Iteration 290, loss = 0.18659991
Iteration 291, loss = 0.17871041
Iteration 292, loss = 0.17660899
Iteration 293, loss = 0.17582689
Iteration 294, loss = 0.17668929
Iteration 295, loss = 0.18402752
Iteration 296, loss = 0.17651598
Iteration 297, loss = 0.17145661
Iteration 298, loss = 0.17229820
Iteration 299, loss = 0.16883747
Iteration 300, loss = 0.17365474
Iteration 301, loss = 0.18394309
Iteration 302, loss = 0.16741915
Iteration 303, loss = 0.17203999
Iteration 304, loss = 0.16242072
Iteration 305, loss = 0.17147939
Iteration 306, loss = 0.16587426
Iteration 307, loss = 0.16438653
Iteration 308, loss = 0.17002857
Iteration 309, loss = 0.17346822
Iteration 310, loss = 0.15786272
Iteration 311, loss = 0.16233709
Iteration 312, loss = 0.17344658
Iteration 313, loss = 0.16578303
Iteration 314, loss = 0.15688247
Iteration 315, loss = 0.17042704
Iteration 316, loss = 0.15591581
Iteration 317, loss = 0.16405887
Iteration 318, loss = 0.15656492
Iteration 319, loss = 0.17111818
Iteration 320, loss = 0.15680876
Iteration 321, loss = 0.16297057
Iteration 322, loss = 0.15594450
Iteration 323, loss = 0.14996757
Iteration 324, loss = 0.14890502
Iteration 325, loss = 0.15710555
Iteration 326, loss = 0.15233843
Iteration 327, loss = 0.15144807
Iteration 328, loss = 0.15220356
Iteration 329, loss = 0.14424683
Iteration 330, loss = 0.16094392
Iteration 331, loss = 0.15086567
Iteration 332, loss = 0.15136587
Iteration 333, loss = 0.15169833
Iteration 334, loss = 0.14825622
Iteration 335, loss = 0.14221562
Iteration 336, loss = 0.14522290
Iteration 337, loss = 0.14540238
Iteration 338, loss = 0.14946154
Iteration 339, loss = 0.13824443
Iteration 340, loss = 0.14282503
Iteration 341, loss = 0.14706104
Iteration 342, loss = 0.15036210
Iteration 343, loss = 0.13377787
Iteration 344, loss = 0.13864979
Iteration 345, loss = 0.15214869
Iteration 346, loss = 0.13618771
Iteration 347, loss = 0.13459051
Iteration 348, loss = 0.13924308
Iteration 349, loss = 0.14475313
Iteration 350, loss = 0.12854576
Iteration 351, loss = 0.21335216
Iteration 352, loss = 0.22772396
Iteration 353, loss = 0.22158442
Iteration 354, loss = 0.21721805
Iteration 355, loss = 0.20754839
Iteration 356, loss = 0.20962555
Iteration 357, loss = 0.20583918
Iteration 358, loss = 0.20232720
Iteration 359, loss = 0.19846976
Iteration 360, loss = 0.20001073
Iteration 361, loss = 0.19576685
Iteration 362, loss = 0.19759366
Iteration 363, loss = 0.19859002
Iteration 364, loss = 0.19523670
Iteration 365, loss = 0.19365439
Iteration 366, loss = 0.19496260
Iteration 367, loss = 0.19740935
Iteration 368, loss = 0.19667177
Iteration 369, loss = 0.19942940
Iteration 370, loss = 0.19040665
Iteration 371, loss = 0.19410438
Iteration 372, loss = 0.19067106
Iteration 373, loss = 0.18744936
Iteration 374, loss = 0.19697228
Iteration 375, loss = 0.19878559
Iteration 376, loss = 0.19387440
Iteration 377, loss = 0.18620835
Iteration 378, loss = 0.19466924
Iteration 379, loss = 0.19195604
Iteration 380, loss = 0.19687360
Iteration 381, loss = 0.18909993
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 4/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(32, 32);, score=0.950 total time= 2.0min
Iteration 1, loss = 3.57963414
Iteration 2, loss = 1.34608130
Iteration 3, loss = 1.34201421
Iteration 4, loss = 1.05150218
Iteration 5, loss = 0.94026668
Iteration 6, loss = 0.96512395
Iteration 7, loss = 1.28581594
Iteration 8, loss = 0.91595243
Iteration 9, loss = 0.83387764
Iteration 10, loss = 0.95624724
Iteration 11, loss = 0.89549188
Iteration 12, loss = 0.86711204
Iteration 13, loss = 0.86437283
Iteration 14, loss = 0.85857661
Iteration 15, loss = 0.87956227
Iteration 16, loss = 0.82239035
Iteration 17, loss = 0.76994782
Iteration 18, loss = 0.85663465
Iteration 19, loss = 0.89122930

Iteration 20, loss = 0.73542415
Iteration 21, loss = 0.87559962
Iteration 22, loss = 0.77994963
Iteration 23, loss = 0.86856898
Iteration 24, loss = 0.79533940
Iteration 25, loss = 0.73924796
Iteration 26, loss = 0.82555433
Iteration 27, loss = 0.78553254
Iteration 28, loss = 0.77388812
Iteration 29, loss = 0.76005128
Iteration 30, loss = 0.79395534
Iteration 31, loss = 0.70750573
Iteration 32, loss = 0.75249047
Iteration 33, loss = 0.75487679
Iteration 34, loss = 0.76039594
Iteration 35, loss = 0.74760057
Iteration 36, loss = 0.74779512
Iteration 37, loss = 0.70272443
Iteration 38, loss = 0.74618274
Iteration 39, loss = 0.72891008
Iteration 40, loss = 0.71564325
Iteration 41, loss = 0.76683886
Iteration 42, loss = 0.73649150
Iteration 43, loss = 0.72218659
Iteration 44, loss = 0.68827606
Iteration 45, loss = 0.68988005
Iteration 46, loss = 0.69083580
Iteration 47, loss = 0.71587044
Iteration 48, loss = 0.69680543
Iteration 49, loss = 0.70720411
Iteration 50, loss = 0.69240352
Iteration 51, loss = 0.70362757
Iteration 52, loss = 0.66704191
Iteration 53, loss = 0.68754110
Iteration 54, loss = 0.68391645
Iteration 55, loss = 0.67816752
Iteration 56, loss = 0.65373953
Iteration 57, loss = 0.67157172
Iteration 58, loss = 0.67871464
Iteration 59, loss = 0.66033565
Iteration 60, loss = 0.66562453
Iteration 61, loss = 0.67567367
Iteration 62, loss = 0.65266592
Iteration 63, loss = 0.67104615
Iteration 64, loss = 0.65037977
Iteration 65, loss = 0.65347537
Iteration 66, loss = 0.65220183
Iteration 67, loss = 0.64123507
Iteration 68, loss = 0.63355009
Iteration 69, loss = 0.65252843
Iteration 70, loss = 0.63162373
Iteration 71, loss = 0.64070939
Iteration 72, loss = 0.64274669
Iteration 73, loss = 0.64900018
Iteration 74, loss = 0.63212749
Iteration 75, loss = 0.64320287
Iteration 76, loss = 0.62646131
Iteration 77, loss = 0.63488959
Iteration 78, loss = 0.63450510
Iteration 79, loss = 0.63285971
Iteration 80, loss = 0.62258501
Iteration 81, loss = 0.62106524
Iteration 82, loss = 0.63272343
Iteration 83, loss = 0.61963328
Iteration 84, loss = 0.62755852
Iteration 85, loss = 0.61634106
Iteration 86, loss = 0.60932631
Iteration 87, loss = 0.62024749
Iteration 88, loss = 0.61836449
Iteration 89, loss = 0.61726992
Iteration 90, loss = 0.61138114
Iteration 91, loss = 0.61155697
Iteration 92, loss = 0.61181646
Iteration 93, loss = 0.60968224
Iteration 94, loss = 0.60493073
Iteration 95, loss = 0.61411854
Iteration 96, loss = 0.60978977
Iteration 97, loss = 0.60897304
Iteration 98, loss = 0.60909271
Iteration 99, loss = 0.60626956
Iteration 100, loss = 0.60848553
Iteration 101, loss = 0.61626961
Iteration 102, loss = 0.61139414
Iteration 103, loss = 0.61288833
Iteration 104, loss = 0.61477664
Iteration 105, loss = 0.61302788
Iteration 106, loss = 0.61642573
Iteration 107, loss = 0.61340116
Iteration 108, loss = 0.61057017
Iteration 109, loss = 0.61306689
Iteration 110, loss = 0.61177162
Iteration 111, loss = 0.61119917
Iteration 112, loss = 0.61082382
Iteration 113, loss = 0.61163737
Iteration 114, loss = 0.60891182
Iteration 115, loss = 0.61066092
Iteration 116, loss = 0.61582777
Iteration 117, loss = 0.60851598
Iteration 118, loss = 0.61058558
Iteration 119, loss = 0.61229110
Iteration 120, loss = 0.60764830
Iteration 121, loss = 0.61019036
Iteration 122, loss = 0.60897184
Iteration 123, loss = 0.60789783
Iteration 124, loss = 0.60432672
Iteration 125, loss = 0.59592431
Iteration 126, loss = 0.57091719
Iteration 127, loss = 0.51333586
Iteration 128, loss = 0.41911060
Iteration 129, loss = 0.33193529
Iteration 130, loss = 0.31000221
Iteration 131, loss = 0.31000026
Iteration 132, loss = 0.29352056
Iteration 133, loss = 0.27962405
Iteration 134, loss = 0.27372290
Iteration 135, loss = 0.26715764
Iteration 136, loss = 0.25652064
Iteration 137, loss = 0.25170376
Iteration 138, loss = 0.22964242
Iteration 139, loss = 0.23321388
Iteration 140, loss = 0.23049782
Iteration 141, loss = 0.21747062
Iteration 142, loss = 0.22359673
Iteration 143, loss = 0.21011911
Iteration 144, loss = 0.21232970
Iteration 145, loss = 0.22528369
Iteration 146, loss = 0.29085007
Iteration 147, loss = 0.30951675
Iteration 148, loss = 0.23720757
Iteration 149, loss = 0.27182076
Iteration 150, loss = 0.27852850
Iteration 151, loss = 0.24700221
Iteration 152, loss = 0.37116265
Iteration 153, loss = 0.35753867
Iteration 154, loss = 0.32398247
Iteration 155, loss = 0.28977006
Iteration 156, loss = 0.30298514
Iteration 157, loss = 0.27044664
Iteration 158, loss = 0.58007983
Iteration 159, loss = 0.68564789
Iteration 160, loss = 0.68243982
Iteration 161, loss = 0.67947954
Iteration 162, loss = 0.67521104
Iteration 163, loss = 0.66494063
Iteration 164, loss = 0.64251578
Iteration 165, loss = 0.61823508
Iteration 166, loss = 0.60593452
Iteration 167, loss = 0.60228237
Iteration 168, loss = 0.60227819
Iteration 169, loss = 0.60163923
Iteration 170, loss = 0.60091774
Iteration 171, loss = 0.60082316
Iteration 172, loss = 0.60011322
Iteration 173, loss = 0.60068258
Iteration 174, loss = 0.60036413
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 5/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(32, 32);, score=0.686 total time= 1.3min
Iteration 1, loss = 2.39059999
Iteration 2, loss = 1.51953417
Iteration 3, loss = 0.99407739
Iteration 4, loss = 1.10856468
Iteration 5, loss = 0.89627382
Iteration 6, loss = 0.84433958
Iteration 7, loss = 0.75323666
Iteration 8, loss = 0.89109794
Iteration 9, loss = 0.82013481
Iteration 10, loss = 0.74942733
Iteration 11, loss = 0.73553325
Iteration 12, loss = 0.74887130
Iteration 13, loss = 0.73914712
Iteration 14, loss = 0.71319804
Iteration 15, loss = 0.71220206
Iteration 16, loss = 0.66233094
Iteration 17, loss = 0.71218712
Iteration 18, loss = 0.70730567
Iteration 19, loss = 0.69318146
Iteration 20, loss = 0.68308822
Iteration 21, loss = 0.67061375
Iteration 22, loss = 0.68359460
Iteration 23, loss = 0.65674518
Iteration 24, loss = 0.65485366
Iteration 25, loss = 0.66535681
Iteration 26, loss = 0.65062737
Iteration 27, loss = 0.64635793
Iteration 28, loss = 0.64033744
Iteration 29, loss = 0.63060558
Iteration 30, loss = 0.64283690
Iteration 31, loss = 0.62498484
Iteration 32, loss = 0.63560374
Iteration 33, loss = 0.62803163
Iteration 34, loss = 0.62271084
Iteration 35, loss = 0.61892410
Iteration 36, loss = 0.62030139
Iteration 37, loss = 0.62078184
Iteration 38, loss = 0.61833763
Iteration 39, loss = 0.61610925
Iteration 40, loss = 0.61767694
Iteration 41, loss = 0.61384049
Iteration 42, loss = 0.61628126
Iteration 43, loss = 0.61878065
Iteration 44, loss = 0.61057978
Iteration 45, loss = 0.61473892
Iteration 46, loss = 0.61329121
Iteration 47, loss = 0.61102408
Iteration 48, loss = 0.60867100
Iteration 49, loss = 0.60975152
Iteration 50, loss = 0.60907706
Iteration 51, loss = 0.60486375
Iteration 52, loss = 0.60444956
Iteration 53, loss = 0.60653324
Iteration 54, loss = 0.60463376
Iteration 55, loss = 0.60240288
Iteration 56, loss = 0.60171008
Iteration 57, loss = 0.60445203
Iteration 58, loss = 0.60224459
Iteration 59, loss = 0.59982380
Iteration 60, loss = 0.59989566
Iteration 61, loss = 0.60269838
Iteration 62, loss = 0.60565586
Iteration 63, loss = 0.60354985
Iteration 64, loss = 0.60534580
Iteration 65, loss = 0.59973548
Iteration 66, loss = 0.60083187
Iteration 67, loss = 0.59112751
Iteration 68, loss = 0.59508222
Iteration 69, loss = 0.58761653
Iteration 70, loss = 0.59054889
Iteration 71, loss = 0.58638520
Iteration 72, loss = 0.58477022
Iteration 73, loss = 0.58885283
Iteration 74, loss = 0.59223900
Iteration 75, loss = 0.60687297
Iteration 76, loss = 0.60305774
Iteration 77, loss = 0.60461797
Iteration 78, loss = 0.60134529
Iteration 79, loss = 0.59988953
Iteration 80, loss = 0.59680955
Iteration 81, loss = 0.59914982
Iteration 82, loss = 0.59479405
Iteration 83, loss = 0.59912128
Iteration 84, loss = 0.59509774
Iteration 85, loss = 0.59322081
Iteration 86, loss = 0.59451299
Iteration 87, loss = 0.59640474
Iteration 88, loss = 0.59403298
Iteration 89, loss = 0.59201601
Iteration 90, loss = 0.59108377
Iteration 91, loss = 0.59191297
Iteration 92, loss = 0.59063539
Iteration 93, loss = 0.59686444

Iteration 94, loss = 0.60675832
Iteration 95, loss = 0.60233788
Iteration 96, loss = 0.60483771
Iteration 97, loss = 0.60120730
Iteration 98, loss = 0.60537120
Iteration 99, loss = 0.60320752
Iteration 100, loss = 0.60450135
Iteration 101, loss = 0.60267565
Iteration 102, loss = 0.60263749
Iteration 103, loss = 0.60025330
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 1/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(32, 32, 32);, score=0.689 total time= 1.1min
Iteration 1, loss = 1.67243286
Iteration 2, loss = 0.87585994
Iteration 3, loss = 0.96010076
Iteration 4, loss = 0.81025371
Iteration 5, loss = 0.75894584
Iteration 6, loss = 0.70081008
Iteration 7, loss = 0.78467151
Iteration 8, loss = 0.71381265
Iteration 9, loss = 0.70535793
Iteration 10, loss = 0.70739725
Iteration 11, loss = 0.67888302
Iteration 12, loss = 0.69012941
Iteration 13, loss = 0.67363677
Iteration 14, loss = 0.66981643
Iteration 15, loss = 0.66267724
Iteration 16, loss = 0.65718199
Iteration 17, loss = 0.65199014
Iteration 18, loss = 0.65162626
Iteration 19, loss = 0.64976071
Iteration 20, loss = 0.64166511
Iteration 21, loss = 0.63666693
Iteration 22, loss = 0.63676532
Iteration 23, loss = 0.64297389
Iteration 24, loss = 0.63744283
Iteration 25, loss = 0.64050683
Iteration 26, loss = 0.62963480
Iteration 27, loss = 0.62592752
Iteration 28, loss = 0.63523978
Iteration 29, loss = 0.62698411
Iteration 30, loss = 0.62321963
Iteration 31, loss = 0.62507602
Iteration 32, loss = 0.62320946
Iteration 33, loss = 0.61613455
Iteration 34, loss = 0.61679985
Iteration 35, loss = 0.61824030
Iteration 36, loss = 0.61692938
Iteration 37, loss = 0.61516971
Iteration 38, loss = 0.62036104
Iteration 39, loss = 0.61305852
Iteration 40, loss = 0.61141309
Iteration 41, loss = 0.60561945
Iteration 42, loss = 0.60969888
Iteration 43, loss = 0.60989833
Iteration 44, loss = 0.60739505
Iteration 45, loss = 0.60535059
Iteration 46, loss = 0.60930587
Iteration 47, loss = 0.60698889
Iteration 48, loss = 0.60763770
Iteration 49, loss = 0.60507875
Iteration 50, loss = 0.60225341
Iteration 51, loss = 0.60273515
Iteration 52, loss = 0.60059908
Iteration 53, loss = 0.60028880
Iteration 54, loss = 0.59713266
Iteration 55, loss = 0.59466976
Iteration 56, loss = 0.59355662
Iteration 57, loss = 0.58992345
Iteration 58, loss = 0.58462415
Iteration 59, loss = 0.58430842
Iteration 60, loss = 0.57935663
Iteration 61, loss = 0.58710183
Iteration 62, loss = 0.57772513
Iteration 63, loss = 0.57871464
Iteration 64, loss = 0.58217976
Iteration 65, loss = 0.57929606
Iteration 66, loss = 0.57811761
Iteration 67, loss = 0.57483343
Iteration 68, loss = 0.57768254
Iteration 69, loss = 0.57768007
Iteration 70, loss = 0.57847733
Iteration 71, loss = 0.59068210
Iteration 72, loss = 0.58883324
Iteration 73, loss = 0.58833535
Iteration 74, loss = 0.58575777
Iteration 75, loss = 0.59324258
Iteration 76, loss = 0.60915943
Iteration 77, loss = 0.61373987
Iteration 78, loss = 0.61176711
Iteration 79, loss = 0.60681790
Iteration 80, loss = 0.60361294
Iteration 81, loss = 0.60131066
Iteration 82, loss = 0.59973236
Iteration 83, loss = 0.60388910
Iteration 84, loss = 0.59970382
Iteration 85, loss = 0.60120129
Iteration 86, loss = 0.59834848
Iteration 87, loss = 0.59692534
Iteration 88, loss = 0.59665593
Iteration 89, loss = 0.59480848
Iteration 90, loss = 0.59300159
Iteration 91, loss = 0.59294568
Iteration 92, loss = 0.59275549
Iteration 93, loss = 0.59061898
Iteration 94, loss = 0.59171140
Iteration 95, loss = 0.58835815
Iteration 96, loss = 0.58898832
Iteration 97, loss = 0.58787730
Iteration 98, loss = 0.58650219
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 2/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(32, 32, 32);, score=0.679 total time= 1.1min
Iteration 1, loss = 1.62025649
Iteration 2, loss = 0.82327301
Iteration 3, loss = 0.74934927
Iteration 4, loss = 0.74443843
Iteration 5, loss = 0.69834746
Iteration 6, loss = 0.70205309
Iteration 7, loss = 0.68291064
Iteration 8, loss = 0.66827651
Iteration 9, loss = 0.66260598
Iteration 10, loss = 0.64848185
Iteration 11, loss = 0.64603044
Iteration 12, loss = 0.63249605
Iteration 13, loss = 0.62850048
Iteration 14, loss = 0.63043714
Iteration 15, loss = 0.62587436
Iteration 16, loss = 0.62139887
Iteration 17, loss = 0.61540309
Iteration 18, loss = 0.62190352
Iteration 19, loss = 0.62001661
Iteration 20, loss = 0.61712619
Iteration 21, loss = 0.61741225
Iteration 22, loss = 0.61295873
Iteration 23, loss = 0.61096537
Iteration 24, loss = 0.61569903
Iteration 25, loss = 0.60958869
Iteration 26, loss = 0.60968109
Iteration 27, loss = 0.60519467
Iteration 28, loss = 0.60834415
Iteration 29, loss = 0.60932350
Iteration 30, loss = 0.60848183
Iteration 31, loss = 0.60592715
Iteration 32, loss = 0.61212055
Iteration 33, loss = 0.60663598
Iteration 34, loss = 0.60568506
Iteration 35, loss = 0.60051865
Iteration 36, loss = 0.59402666
Iteration 37, loss = 0.58740319
Iteration 38, loss = 0.59613773
Iteration 39, loss = 0.60191006
Iteration 40, loss = 0.60008371
Iteration 41, loss = 0.59376720
Iteration 42, loss = 0.59831540
Iteration 43, loss = 0.60213397
Iteration 44, loss = 0.59762772
Iteration 45, loss = 0.58971187
Iteration 46, loss = 0.59090324
Iteration 47, loss = 0.59209639
Iteration 48, loss = 0.58778296
Iteration 49, loss = 0.58343110
Iteration 50, loss = 0.58286229
Iteration 51, loss = 0.57770382
Iteration 52, loss = 0.58222393
Iteration 53, loss = 0.57677242
Iteration 54, loss = 0.57924125
Iteration 55, loss = 0.57729919
Iteration 56, loss = 0.58526270
Iteration 57, loss = 0.59822838
Iteration 58, loss = 0.59834237
Iteration 59, loss = 0.59878843
Iteration 60, loss = 0.59764106
Iteration 61, loss = 0.59810578
Iteration 62, loss = 0.59894327
Iteration 63, loss = 0.59821660
Iteration 64, loss = 0.59750264
Iteration 65, loss = 0.59894175
Iteration 66, loss = 0.59284334
Iteration 67, loss = 0.58568591
Iteration 68, loss = 0.58173209
Iteration 69, loss = 0.58064433
Iteration 70, loss = 0.57915059
Iteration 71, loss = 0.57861287
Iteration 72, loss = 0.57690880
Iteration 73, loss = 0.57653267
Iteration 74, loss = 0.57670375
Iteration 75, loss = 0.57328779
Iteration 76, loss = 0.57852181
Iteration 77, loss = 0.57362137
Iteration 78, loss = 0.57862605
Iteration 79, loss = 0.57343581
Iteration 80, loss = 0.57073125
Iteration 81, loss = 0.57404131
Iteration 82, loss = 0.57164787
Iteration 83, loss = 0.57799240
Iteration 84, loss = 0.57161539
Iteration 85, loss = 0.57242467
Iteration 86, loss = 0.57365034
Iteration 87, loss = 0.57399868
Iteration 88, loss = 0.57433573
Iteration 89, loss = 0.57164724
Iteration 90, loss = 0.57443266
Iteration 91, loss = 0.57480341
Iteration 92, loss = 0.57009072
Iteration 93, loss = 0.57191679
Iteration 94, loss = 0.57555786
Iteration 95, loss = 0.57146833
Iteration 96, loss = 0.57042446
Iteration 97, loss = 0.57474514
Iteration 98, loss = 0.56959200
Iteration 99, loss = 0.57002340
Iteration 100, loss = 0.57164822
Iteration 101, loss = 0.56890058
Iteration 102, loss = 0.56958648
Iteration 103, loss = 0.57337401
Iteration 104, loss = 0.56969975
Iteration 105, loss = 0.56939678
Iteration 106, loss = 0.57017231
Iteration 107, loss = 0.57041339
Iteration 108, loss = 0.57003344
Iteration 109, loss = 0.57336449
Iteration 110, loss = 0.56831713
Iteration 111, loss = 0.56871870
Iteration 112, loss = 0.56990678
Iteration 113, loss = 0.57277418
Iteration 114, loss = 0.59801263
Iteration 115, loss = 0.58677063
Iteration 116, loss = 0.58036862
Iteration 117, loss = 0.57725314
Iteration 118, loss = 0.57431369
Iteration 119, loss = 0.57280308
Iteration 120, loss = 0.57367302
Iteration 121, loss = 0.57158649
Iteration 122, loss = 0.57120838
Iteration 123, loss = 0.57225559
Iteration 124, loss = 0.57156670
Iteration 125, loss = 0.56994857
Iteration 126, loss = 0.57212352
Iteration 127, loss = 0.56991909
Iteration 128, loss = 0.57150186
Iteration 129, loss = 0.56937354
Iteration 130, loss = 0.57080239
Iteration 131, loss = 0.56970575
Iteration 132, loss = 0.57008601
Iteration 133, loss = 0.56917569
Iteration 134, loss = 0.56802428
Iteration 135, loss = 0.57124791

Iteration 136, loss = 0.56950146
Iteration 137, loss = 0.56844735
Iteration 138, loss = 0.56733199
Iteration 139, loss = 0.56939508
Iteration 140, loss = 0.56799007
Iteration 141, loss = 0.56891618
Iteration 142, loss = 0.56842709
Iteration 143, loss = 0.56787947
Iteration 144, loss = 0.56852243
Iteration 145, loss = 0.57032658
Iteration 146, loss = 0.56855672
Iteration 147, loss = 0.57060577
Iteration 148, loss = 0.56849217
Iteration 149, loss = 0.56876060
Iteration 150, loss = 0.56835823
Iteration 151, loss = 0.56812359
Iteration 152, loss = 0.56795017
Iteration 153, loss = 0.56800981
Iteration 154, loss = 0.56801207
Iteration 155, loss = 0.56759724
Iteration 156, loss = 0.56723751
Iteration 157, loss = 0.57037999
Iteration 158, loss = 0.56492475
Iteration 159, loss = 0.56782053
Iteration 160, loss = 0.56658653
Iteration 161, loss = 0.56617452
Iteration 162, loss = 0.56645443
Iteration 163, loss = 0.56696626
Iteration 164, loss = 0.56547870
Iteration 165, loss = 0.56730884
Iteration 166, loss = 0.56591663
Iteration 167, loss = 0.56704424
Iteration 168, loss = 0.56917662
Iteration 169, loss = 0.56864983
Iteration 170, loss = 0.56740511
Iteration 171, loss = 0.56801652
Iteration 172, loss = 0.56752061
Iteration 173, loss = 0.56533893
Iteration 174, loss = 0.56738979
Iteration 175, loss = 0.56708766
Iteration 176, loss = 0.56803218
Iteration 177, loss = 0.60064109
Iteration 178, loss = 0.60902485
Iteration 179, loss = 0.60413748
Iteration 180, loss = 0.60014677
Iteration 181, loss = 0.59858254
Iteration 182, loss = 0.59771412
Iteration 183, loss = 0.59566573
Iteration 184, loss = 0.59413574
Iteration 185, loss = 0.59422253
Iteration 186, loss = 0.59270340
Iteration 187, loss = 0.59161292
Iteration 188, loss = 0.59236904
Iteration 189, loss = 0.59163959
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 3/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(32, 32, 32);, score=0.686 total time= 1.6min
Iteration 1, loss = 2.54901694
Iteration 2, loss = 1.35827035
Iteration 3, loss = 0.94615396
Iteration 4, loss = 1.04763231
Iteration 5, loss = 0.79943965
Iteration 6, loss = 0.80100602
Iteration 7, loss = 0.73958888
Iteration 8, loss = 0.77305636
Iteration 9, loss = 0.76873494
Iteration 10, loss = 0.72270065
Iteration 11, loss = 0.70445048
Iteration 12, loss = 0.70358365
Iteration 13, loss = 0.67280253
Iteration 14, loss = 0.68703620
Iteration 15, loss = 0.66953956
Iteration 16, loss = 0.67277993
Iteration 17, loss = 0.65156998
Iteration 18, loss = 0.66026690
Iteration 19, loss = 0.63618830
Iteration 20, loss = 0.64896601
Iteration 21, loss = 0.63629625
Iteration 22, loss = 0.63273867
Iteration 23, loss = 0.63501869
Iteration 24, loss = 0.63191836
Iteration 25, loss = 0.62819137
Iteration 26, loss = 0.62929566
Iteration 27, loss = 0.62130074
Iteration 28, loss = 0.62463495
Iteration 29, loss = 0.62027819
Iteration 30, loss = 0.61701291
Iteration 31, loss = 0.61535561
Iteration 32, loss = 0.61999487
Iteration 33, loss = 0.61137882
Iteration 34, loss = 0.61327604
Iteration 35, loss = 0.61718502
Iteration 36, loss = 0.61459857
Iteration 37, loss = 0.61630117
Iteration 38, loss = 0.61233815
Iteration 39, loss = 0.61152978
Iteration 40, loss = 0.61594557
Iteration 41, loss = 0.61033851
Iteration 42, loss = 0.61101238
Iteration 43, loss = 0.60542843
Iteration 44, loss = 0.61286667
Iteration 45, loss = 0.61234677
Iteration 46, loss = 0.61084908
Iteration 47, loss = 0.60595467
Iteration 48, loss = 0.60753139
Iteration 49, loss = 0.60855421
Iteration 50, loss = 0.60546625
Iteration 51, loss = 0.60913060
Iteration 52, loss = 0.60489665
Iteration 53, loss = 0.60915055
Iteration 54, loss = 0.60240963
Iteration 55, loss = 0.60390976
Iteration 56, loss = 0.60137139
Iteration 57, loss = 0.60060787
Iteration 58, loss = 0.60917385
Iteration 59, loss = 0.60016499
Iteration 60, loss = 0.59909257
Iteration 61, loss = 0.60120923
Iteration 62, loss = 0.60288876
Iteration 63, loss = 0.59886532
Iteration 64, loss = 0.59868553
Iteration 65, loss = 0.59679713
Iteration 66, loss = 0.59758941
Iteration 67, loss = 0.60157474
Iteration 68, loss = 0.59946750
Iteration 69, loss = 0.59546074
Iteration 70, loss = 0.59438018
Iteration 71, loss = 0.59057399
Iteration 72, loss = 0.59023416
Iteration 73, loss = 0.58905824
Iteration 74, loss = 0.59833556
Iteration 75, loss = 0.60253732
Iteration 76, loss = 0.60162252
Iteration 77, loss = 0.60006299
Iteration 78, loss = 0.60052139
Iteration 79, loss = 0.60271377
Iteration 80, loss = 0.59752709
Iteration 81, loss = 0.59984920
Iteration 82, loss = 0.59692365
Iteration 83, loss = 0.59995740
Iteration 84, loss = 0.59802914
Iteration 85, loss = 0.59819068
Iteration 86, loss = 0.59778660
Iteration 87, loss = 0.60002597
Iteration 88, loss = 0.59930581
Iteration 89, loss = 0.59739807
Iteration 90, loss = 0.59925353
Iteration 91, loss = 0.59870868
Iteration 92, loss = 0.59689758
Iteration 93, loss = 0.59828970
Iteration 94, loss = 0.59863727
Iteration 95, loss = 0.59687941
Iteration 96, loss = 0.59825587
Iteration 97, loss = 0.59728158
Iteration 98, loss = 0.59756139
Iteration 99, loss = 0.59578941
Iteration 100, loss = 0.59722995
Iteration 101, loss = 0.59823412
Iteration 102, loss = 0.59683115
Iteration 103, loss = 0.59593625
Iteration 104, loss = 0.60153485
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 4/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(32, 32, 32);, score=0.685 total time= 1.2min
Iteration 1, loss = 2.26811120
Iteration 2, loss = 0.94908931
Iteration 3, loss = 0.77679175
Iteration 4, loss = 0.78123373
Iteration 5, loss = 0.72775637
Iteration 6, loss = 0.76613229
Iteration 7, loss = 0.72070182
Iteration 8, loss = 0.70955963
Iteration 9, loss = 0.68940718
Iteration 10, loss = 0.68661223
Iteration 11, loss = 0.69705440
Iteration 12, loss = 0.67028996
Iteration 13, loss = 0.65435426
Iteration 14, loss = 0.65201157
Iteration 15, loss = 0.67255598
Iteration 16, loss = 0.65296580
Iteration 17, loss = 0.64990648
Iteration 18, loss = 0.64392081
Iteration 19, loss = 0.64529828
Iteration 20, loss = 0.63635097
Iteration 21, loss = 0.63713670
Iteration 22, loss = 0.63567338
Iteration 23, loss = 0.62742776
Iteration 24, loss = 0.63777324
Iteration 25, loss = 0.63513918
Iteration 26, loss = 0.62529941
Iteration 27, loss = 0.61965956
Iteration 28, loss = 0.61949685
Iteration 29, loss = 0.61843286
Iteration 30, loss = 0.62794693
Iteration 31, loss = 0.61908587
Iteration 32, loss = 0.61359611
Iteration 33, loss = 0.61653268
Iteration 34, loss = 0.61668611
Iteration 35, loss = 0.61404611
Iteration 36, loss = 0.61434838
Iteration 37, loss = 0.61130649
Iteration 38, loss = 0.60897365
Iteration 39, loss = 0.61057497
Iteration 40, loss = 0.60780393
Iteration 41, loss = 0.60976645
Iteration 42, loss = 0.60692739
Iteration 43, loss = 0.60449040
Iteration 44, loss = 0.60465968
Iteration 45, loss = 0.60465543
Iteration 46, loss = 0.60532465
Iteration 47, loss = 0.60542451
Iteration 48, loss = 0.60407918
Iteration 49, loss = 0.60077023
Iteration 50, loss = 0.60085003
Iteration 51, loss = 0.59675546
Iteration 52, loss = 0.59339356
Iteration 53, loss = 0.59141818
Iteration 54, loss = 0.57584666
Iteration 55, loss = 0.56252541
Iteration 56, loss = 0.56482738
Iteration 57, loss = 0.55058966
Iteration 58, loss = 0.48242812
Iteration 59, loss = 0.44277802
Iteration 60, loss = 0.38548901
Iteration 61, loss = 0.34973306
Iteration 62, loss = 0.30357461
Iteration 63, loss = 0.30808467
Iteration 64, loss = 0.29850986
Iteration 65, loss = 0.29771185
Iteration 66, loss = 0.28746220
Iteration 67, loss = 0.29107752
Iteration 68, loss = 0.29165030
Iteration 69, loss = 0.28220997
Iteration 70, loss = 0.29077581
Iteration 71, loss = 0.27113225
Iteration 72, loss = 0.26956226
Iteration 73, loss = 0.25704668
Iteration 74, loss = 0.26754305
Iteration 75, loss = 0.24924910
Iteration 76, loss = 0.25080789
Iteration 77, loss = 0.25859585
Iteration 78, loss = 0.24904291
Iteration 79, loss = 0.23226969
Iteration 80, loss = 0.24034614
Iteration 81, loss = 0.24730000
Iteration 82, loss = 0.25287813
Iteration 83, loss = 0.23403838
Iteration 84, loss = 0.22894905

Iteration 85, loss = 0.23244082
Iteration 86, loss = 0.23730404
Iteration 87, loss = 0.22322518
Iteration 88, loss = 0.22167689
Iteration 89, loss = 0.22079867
Iteration 90, loss = 0.22430505
Iteration 91, loss = 0.21724623
Iteration 92, loss = 0.21220396
Iteration 93, loss = 0.21190975
Iteration 94, loss = 0.22655079
Iteration 95, loss = 0.20576434
Iteration 96, loss = 0.22271503
Iteration 97, loss = 0.20346067
Iteration 98, loss = 0.21258297
Iteration 99, loss = 0.20931727
Iteration 100, loss = 0.20765326
Iteration 101, loss = 0.21014432
Iteration 102, loss = 0.20757667
Iteration 103, loss = 0.20759308
Iteration 104, loss = 0.19671377
Iteration 105, loss = 0.20392506
Iteration 106, loss = 0.21438826
Iteration 107, loss = 0.19880837
Iteration 108, loss = 0.19623946
Iteration 109, loss = 0.20172276
Iteration 110, loss = 0.18887565
Iteration 111, loss = 0.19495255
Iteration 112, loss = 0.18370163
Iteration 113, loss = 0.19510930
Iteration 114, loss = 0.19490333
Iteration 115, loss = 0.17350309
Iteration 116, loss = 0.18000054
Iteration 117, loss = 0.15837725
Iteration 118, loss = 0.17396594
Iteration 119, loss = 0.16812221
Iteration 120, loss = 0.18221848
Iteration 121, loss = 0.16196005
Iteration 122, loss = 0.16402266
Iteration 123, loss = 0.18599926
Iteration 124, loss = 0.18835955
Iteration 125, loss = 0.21572559
Iteration 126, loss = 0.18490068
Iteration 127, loss = 0.19022157
Iteration 128, loss = 0.16498552
Iteration 129, loss = 0.19907700
Iteration 130, loss = 0.16125850
Iteration 131, loss = 0.33624475
Iteration 132, loss = 0.39312520
Iteration 133, loss = 0.28631333
Iteration 134, loss = 0.26243873
Iteration 135, loss = 0.25363462
Iteration 136, loss = 0.25345698
Iteration 137, loss = 0.24986230
Iteration 138, loss = 0.24845148
Iteration 139, loss = 0.25525200
Iteration 140, loss = 0.24773376
Iteration 141, loss = 0.24902370
Iteration 142, loss = 0.24760670
Iteration 143, loss = 0.24684728
Iteration 144, loss = 0.24556316
Iteration 145, loss = 0.24482680
Iteration 146, loss = 0.24342844
Iteration 147, loss = 0.25465178
Iteration 148, loss = 0.24792493
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 5/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(32, 32, 32);, score=0.948 total time= 1.4min
Iteration 1, loss = 3.30231867
Iteration 2, loss = 1.40157452
Iteration 3, loss = 1.28360941
Iteration 4, loss = 1.19577429
Iteration 5, loss = 1.35003834
Iteration 6, loss = 1.49486152
Iteration 7, loss = 1.23350984
Iteration 8, loss = 1.24797686
Iteration 9, loss = 1.31977886
Iteration 10, loss = 1.52211399
Iteration 11, loss = 1.14839568
Iteration 12, loss = 1.19273003
Iteration 13, loss = 1.34904193
Iteration 14, loss = 1.04142988
Iteration 15, loss = 1.05383915
Iteration 16, loss = 1.18175955
Iteration 17, loss = 1.24828168
Iteration 18, loss = 1.12215127
Iteration 19, loss = 1.09013824
Iteration 20, loss = 1.12952572
Iteration 21, loss = 1.01212605
Iteration 22, loss = 1.74040812
Iteration 23, loss = 1.04694932
Iteration 24, loss = 1.04885493
Iteration 25, loss = 1.28399327
Iteration 26, loss = 0.90241348
Iteration 27, loss = 1.30262606
Iteration 28, loss = 1.10192843
Iteration 29, loss = 1.13624288
Iteration 30, loss = 0.90259008
Iteration 31, loss = 0.95419502
Iteration 32, loss = 1.20998145
Iteration 33, loss = 1.36404720
Iteration 34, loss = 0.92835110
Iteration 35, loss = 1.18058061
Iteration 36, loss = 1.03753538
Iteration 37, loss = 1.11627217
Iteration 38, loss = 1.08472482
Iteration 39, loss = 0.88264135
Iteration 40, loss = 1.03954446
Iteration 41, loss = 1.03242167
Iteration 42, loss = 1.01436396
Iteration 43, loss = 0.93513472
Iteration 44, loss = 1.13948050
Iteration 45, loss = 0.96380424
Iteration 46, loss = 0.98077451
Iteration 47, loss = 1.04248190
Iteration 48, loss = 1.03619675
Iteration 49, loss = 0.88773139
Iteration 50, loss = 1.29252239
Iteration 51, loss = 0.75498950
Iteration 52, loss = 1.00238502
Iteration 53, loss = 1.04494467
Iteration 54, loss = 0.96388290
Iteration 55, loss = 1.02190320
Iteration 56, loss = 0.98225673
Iteration 57, loss = 0.86656440
Iteration 58, loss = 0.86333575
Iteration 59, loss = 1.07190328
Iteration 60, loss = 0.87013860
Iteration 61, loss = 1.01340324
Iteration 62, loss = 0.96496776
Iteration 63, loss = 0.89636543
Iteration 64, loss = 0.81152346
Iteration 65, loss = 0.88095535
Iteration 66, loss = 0.97878297
Iteration 67, loss = 0.92646966
Iteration 68, loss = 0.93213836
Iteration 69, loss = 0.87379243
Iteration 70, loss = 0.88660924
Iteration 71, loss = 0.89068231
Iteration 72, loss = 0.82402956
Iteration 73, loss = 0.76882302
Iteration 74, loss = 0.94343663
Iteration 75, loss = 0.86453506
Iteration 76, loss = 0.81639712
Iteration 77, loss = 0.88183944
Iteration 78, loss = 0.80924413
Iteration 79, loss = 0.80016601
Iteration 80, loss = 0.77183487
Iteration 81, loss = 0.94760100
Iteration 82, loss = 0.79745916
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 1/5] END estimator__activation=relu, estimator__hidden_layer_sizes=100;, score=0.546 total time= 1.7min
Iteration 1, loss = 2.21836957
Iteration 2, loss = 1.22712857
Iteration 3, loss = 1.06789829
Iteration 4, loss = 0.99945923
Iteration 5, loss = 0.88362749
Iteration 6, loss = 1.22597889
Iteration 7, loss = 1.09050903
Iteration 8, loss = 0.84280601
Iteration 9, loss = 1.03420396
Iteration 10, loss = 0.97174202
Iteration 11, loss = 0.88464829
Iteration 12, loss = 0.89374458
Iteration 13, loss = 0.97283245
Iteration 14, loss = 0.93639254
Iteration 15, loss = 1.18085561
Iteration 16, loss = 0.93840593
Iteration 17, loss = 0.87195988
Iteration 18, loss = 0.90570201
Iteration 19, loss = 0.79573585
Iteration 20, loss = 1.10499399
Iteration 21, loss = 0.75131763
Iteration 22, loss = 1.01054533
Iteration 23, loss = 0.88998591
Iteration 24, loss = 0.92819555
Iteration 25, loss = 0.74573980
Iteration 26, loss = 0.86245452
Iteration 27, loss = 0.92209704
Iteration 28, loss = 0.91317071
Iteration 29, loss = 0.84515337
Iteration 30, loss = 0.87769580
Iteration 31, loss = 0.87028445
Iteration 32, loss = 0.88706461
Iteration 33, loss = 0.86778154
Iteration 34, loss = 0.88730078
Iteration 35, loss = 0.82637627
Iteration 36, loss = 0.79770695
Iteration 37, loss = 0.79901925
Iteration 38, loss = 0.80407828
Iteration 39, loss = 0.89016332
Iteration 40, loss = 0.70421285
Iteration 41, loss = 0.89899959
Iteration 42, loss = 0.82292119
Iteration 43, loss = 0.76927212
Iteration 44, loss = 0.81639120
Iteration 45, loss = 1.04121330
Iteration 46, loss = 0.75643766
Iteration 47, loss = 0.77851429
Iteration 48, loss = 0.72062946
Iteration 49, loss = 0.89813553
Iteration 50, loss = 0.71857401
Iteration 51, loss = 0.76027698
Iteration 52, loss = 0.86603165
Iteration 53, loss = 0.81432958
Iteration 54, loss = 0.76925876
Iteration 55, loss = 0.82429837
Iteration 56, loss = 0.78537392
Iteration 57, loss = 0.74697447
Iteration 58, loss = 0.77993122
Iteration 59, loss = 0.70322810
Iteration 60, loss = 0.80764283
Iteration 61, loss = 0.78663460
Iteration 62, loss = 0.76198317
Iteration 63, loss = 0.75498599
Iteration 64, loss = 0.72959139
Iteration 65, loss = 0.70391844
Iteration 66, loss = 0.74482000
Iteration 67, loss = 0.79800175
Iteration 68, loss = 0.72666396
Iteration 69, loss = 0.77681083
Iteration 70, loss = 0.72569505
Iteration 71, loss = 0.73883759
Iteration 72, loss = 0.81638697
Iteration 73, loss = 0.69908307
Iteration 74, loss = 0.75571378
Iteration 75, loss = 0.73171177
Iteration 76, loss = 0.72770385
Iteration 77, loss = 0.72622957
Iteration 78, loss = 0.71557520
Iteration 79, loss = 0.77384025
Iteration 80, loss = 0.66150502
Iteration 81, loss = 0.74408273
Iteration 82, loss = 0.74546007
Iteration 83, loss = 0.70511720
Iteration 84, loss = 0.69808178
Iteration 85, loss = 0.66043673
Iteration 86, loss = 0.76621402
Iteration 87, loss = 0.68188587
Iteration 88, loss = 0.71256411
Iteration 89, loss = 0.70541191
Iteration 90, loss = 0.69390172
Iteration 91, loss = 0.72022013
Iteration 92, loss = 0.70165802
Iteration 93, loss = 0.69086763
Iteration 94, loss = 0.71719977
Iteration 95, loss = 0.69234931
Iteration 96, loss = 0.66012382
Iteration 97, loss = 0.69670309

Iteration 98, loss = 0.70206924
Iteration 99, loss = 0.67206242
Iteration 100, loss = 0.69440780
Iteration 101, loss = 0.68449956
Iteration 102, loss = 0.67435196
Iteration 103, loss = 0.70012246
Iteration 104, loss = 0.67041967
Iteration 105, loss = 0.68001107
Iteration 106, loss = 0.69769360
Iteration 107, loss = 0.67653258
Iteration 108, loss = 0.66871487
Iteration 109, loss = 0.65381363
Iteration 110, loss = 0.67255686
Iteration 111, loss = 0.71325360
Iteration 112, loss = 0.66878417
Iteration 113, loss = 0.66723924
Iteration 114, loss = 0.66290250
Iteration 115, loss = 0.67900453
Iteration 116, loss = 0.68228732
Iteration 117, loss = 0.67481210
Iteration 118, loss = 0.64184582
Iteration 119, loss = 0.66587903
Iteration 120, loss = 0.65778941
Iteration 121, loss = 0.68144410
Iteration 122, loss = 0.65517828
Iteration 123, loss = 0.67054031
Iteration 124, loss = 0.65559073
Iteration 125, loss = 0.66765880
Iteration 126, loss = 0.66004017
Iteration 127, loss = 0.64022148
Iteration 128, loss = 0.65940510
Iteration 129, loss = 0.66287701
Iteration 130, loss = 0.66308383
Iteration 131, loss = 0.65118231
Iteration 132, loss = 0.65535019
Iteration 133, loss = 0.65274407
Iteration 134, loss = 0.65010837
Iteration 135, loss = 0.64487220
Iteration 136, loss = 0.64549775
Iteration 137, loss = 0.64163093
Iteration 138, loss = 0.64409542
Iteration 139, loss = 0.65940792
Iteration 140, loss = 0.64544529
Iteration 141, loss = 0.65323936
Iteration 142, loss = 0.64176621
Iteration 143, loss = 0.62854822
Iteration 144, loss = 0.65139997
Iteration 145, loss = 0.65066370
Iteration 146, loss = 0.63643647
Iteration 147, loss = 0.63061676
Iteration 148, loss = 0.62761776
Iteration 149, loss = 0.65307191
Iteration 150, loss = 0.63821410
Iteration 151, loss = 0.64078003
Iteration 152, loss = 0.63291781
Iteration 153, loss = 0.64772334
Iteration 154, loss = 0.62727199
Iteration 155, loss = 0.64971229
Iteration 156, loss = 0.62244172
Iteration 157, loss = 0.63353135
Iteration 158, loss = 0.62352645
Iteration 159, loss = 0.65121069
Iteration 160, loss = 0.63274802
Iteration 161, loss = 0.64528747
Iteration 162, loss = 0.63506454
Iteration 163, loss = 0.62607857
Iteration 164, loss = 0.63457308
Iteration 165, loss = 0.62411156
Iteration 166, loss = 0.63430100
Iteration 167, loss = 0.62662816
Iteration 168, loss = 0.63772103
Iteration 169, loss = 0.63311413
Iteration 170, loss = 0.61770675
Iteration 171, loss = 0.62024791
Iteration 172, loss = 0.63178216
Iteration 173, loss = 0.62239200
Iteration 174, loss = 0.62933763
Iteration 175, loss = 0.63422758
Iteration 176, loss = 0.62718264
Iteration 177, loss = 0.63797300
Iteration 178, loss = 0.63262777
Iteration 179, loss = 0.62116282
Iteration 180, loss = 0.62660918
Iteration 181, loss = 0.61802599
Iteration 182, loss = 0.61407327
Iteration 183, loss = 0.63243930
Iteration 184, loss = 0.61906543
Iteration 185, loss = 0.61872022
Iteration 186, loss = 0.61211982
Iteration 187, loss = 0.62449917
Iteration 188, loss = 0.62394162
Iteration 189, loss = 0.62018941
Iteration 190, loss = 0.63487499
Iteration 191, loss = 0.61232185
Iteration 192, loss = 0.62589060
Iteration 193, loss = 0.62840793
Iteration 194, loss = 0.61430600
Iteration 195, loss = 0.62483351
Iteration 196, loss = 0.62582371
Iteration 197, loss = 0.61641638
Iteration 198, loss = 0.61582844
Iteration 199, loss = 0.62046722
Iteration 200, loss = 0.61116121
Iteration 201, loss = 0.63608327
Iteration 202, loss = 0.61255024
Iteration 203, loss = 0.61061376
Iteration 204, loss = 0.61394054
Iteration 205, loss = 0.61527332
Iteration 206, loss = 0.62065518
Iteration 207, loss = 0.62926888
Iteration 208, loss = 0.60384879
Iteration 209, loss = 0.61975592
Iteration 210, loss = 0.63174657
Iteration 211, loss = 0.61410514
Iteration 212, loss = 0.61111268
Iteration 213, loss = 0.60788287
Iteration 214, loss = 0.61063863
Iteration 215, loss = 0.61518923
Iteration 216, loss = 0.62093009
Iteration 217, loss = 0.61409642
Iteration 218, loss = 0.62105346
Iteration 219, loss = 0.61656001
Iteration 220, loss = 0.61968205
Iteration 221, loss = 0.61799432
Iteration 222, loss = 0.59940369
Iteration 223, loss = 0.61776779
Iteration 224, loss = 0.61044613
Iteration 225, loss = 0.61882308
Iteration 226, loss = 0.61098710
Iteration 227, loss = 0.61657488
Iteration 228, loss = 0.61936865
Iteration 229, loss = 0.60869117
Iteration 230, loss = 0.60783742
Iteration 231, loss = 0.61938503
Iteration 232, loss = 0.60540476
Iteration 233, loss = 0.60516794
Iteration 234, loss = 0.61131156
Iteration 235, loss = 0.61156359
Iteration 236, loss = 0.61103321
Iteration 237, loss = 0.62085865
Iteration 238, loss = 0.60064605
Iteration 239, loss = 0.61729829
Iteration 240, loss = 0.60822544
Iteration 241, loss = 0.59890827
Iteration 242, loss = 0.60554642
Iteration 243, loss = 0.61010007
Iteration 244, loss = 0.61257668
Iteration 245, loss = 0.60165224
Iteration 246, loss = 0.61444379
Iteration 247, loss = 0.60563354
Iteration 248, loss = 0.60438573
Iteration 249, loss = 0.60800098
Iteration 250, loss = 0.61068480
Iteration 251, loss = 0.61226741
Iteration 252, loss = 0.59727745
Iteration 253, loss = 0.61060748
Iteration 254, loss = 0.60903206
Iteration 255, loss = 0.61420460
Iteration 256, loss = 0.59985717
Iteration 257, loss = 0.60826847
Iteration 258, loss = 0.60934823
Iteration 259, loss = 0.60471766
Iteration 260, loss = 0.60617271
Iteration 261, loss = 0.60963900
Iteration 262, loss = 0.60026023
Iteration 263, loss = 0.59902428
Iteration 264, loss = 0.60597308
Iteration 265, loss = 0.60566714
Iteration 266, loss = 0.60352444
Iteration 267, loss = 0.60106086
Iteration 268, loss = 0.60929074
Iteration 269, loss = 0.60519880
Iteration 270, loss = 0.60701350
Iteration 271, loss = 0.60489684
Iteration 272, loss = 0.60285084
Iteration 273, loss = 0.60531633
Iteration 274, loss = 0.60381618
Iteration 275, loss = 0.59961115
Iteration 276, loss = 0.61025959
Iteration 277, loss = 0.59717561
Iteration 278, loss = 0.60602083
Iteration 279, loss = 0.60685053
Iteration 280, loss = 0.60965499
Iteration 281, loss = 0.59506713
Iteration 282, loss = 0.60412115
Iteration 283, loss = 0.59836765
Iteration 284, loss = 0.60236668
Iteration 285, loss = 0.60147815
Iteration 286, loss = 0.60305295
Iteration 287, loss = 0.60249440
Iteration 288, loss = 0.59348401
Iteration 289, loss = 0.59972582
Iteration 290, loss = 0.60741195
Iteration 291, loss = 0.60522956
Iteration 292, loss = 0.59780882
Iteration 293, loss = 0.59906157
Iteration 294, loss = 0.60213728
Iteration 295, loss = 0.60074915
Iteration 296, loss = 0.60450206
Iteration 297, loss = 0.59982766
Iteration 298, loss = 0.59729000
Iteration 299, loss = 0.59695138
Iteration 300, loss = 0.60164772
Iteration 301, loss = 0.59319901
Iteration 302, loss = 0.59263608
Iteration 303, loss = 0.60005806
Iteration 304, loss = 0.59433625
Iteration 305, loss = 0.60396752
Iteration 306, loss = 0.59712113
Iteration 307, loss = 0.60188228
Iteration 308, loss = 0.59555432
Iteration 309, loss = 0.59975592
Iteration 310, loss = 0.59373043
Iteration 311, loss = 0.59787624
Iteration 312, loss = 0.59656276
Iteration 313, loss = 0.59815676
Iteration 314, loss = 0.59036655
Iteration 315, loss = 0.59611271
Iteration 316, loss = 0.59700768
Iteration 317, loss = 0.59538904
Iteration 318, loss = 0.59153578
Iteration 319, loss = 0.59222268
Iteration 320, loss = 0.59415793
Iteration 321, loss = 0.59215897
Iteration 322, loss = 0.59397088
Iteration 323, loss = 0.59766422
Iteration 324, loss = 0.59666740
Iteration 325, loss = 0.59019632
Iteration 326, loss = 0.59672459
Iteration 327, loss = 0.59730044
Iteration 328, loss = 0.60095604
Iteration 329, loss = 0.59625993
Iteration 330, loss = 0.60060779
Iteration 331, loss = 0.59097687
Iteration 332, loss = 0.58808741
Iteration 333, loss = 0.59495240
Iteration 334, loss = 0.59205550
Iteration 335, loss = 0.59784286
Iteration 336, loss = 0.59449986
Iteration 337, loss = 0.59304328
Iteration 338, loss = 0.59315373
Iteration 339, loss = 0.59666515
Iteration 340, loss = 0.59407172
Iteration 341, loss = 0.59043815
Iteration 342, loss = 0.59214545
Iteration 343, loss = 0.59572310
Iteration 344, loss = 0.59499136
Iteration 345, loss = 0.59335548
Iteration 346, loss = 0.59325517

Iteration 347, loss = 0.60373443
Iteration 348, loss = 0.58985496
Iteration 349, loss = 0.58776779
Iteration 350, loss = 0.59464793
Iteration 351, loss = 0.59344754
Iteration 352, loss = 0.59080162
Iteration 353, loss = 0.59561642
Iteration 354, loss = 0.59757026
Iteration 355, loss = 0.59426496
Iteration 356, loss = 0.59231380
Iteration 357, loss = 0.59092135
Iteration 358, loss = 0.59364847
Iteration 359, loss = 0.58987349
Iteration 360, loss = 0.58760941
Iteration 361, loss = 0.59696089
Iteration 362, loss = 0.59564216
Iteration 363, loss = 0.59000599
Iteration 364, loss = 0.58985644
Iteration 365, loss = 0.59142414
Iteration 366, loss = 0.59390359
Iteration 367, loss = 0.59227387
Iteration 368, loss = 0.58373621
Iteration 369, loss = 0.56146490
Iteration 370, loss = 0.50273096
Iteration 371, loss = 0.43015259
Iteration 372, loss = 0.37699738
Iteration 373, loss = 0.34160457
Iteration 374, loss = 0.31748045
Iteration 375, loss = 0.31248786
Iteration 376, loss = 0.29872635
Iteration 377, loss = 0.30246524
Iteration 378, loss = 0.29878637
Iteration 379, loss = 0.29278475
Iteration 380, loss = 0.29793803
Iteration 381, loss = 0.28937995
Iteration 382, loss = 0.29587421
Iteration 383, loss = 0.29013935
Iteration 384, loss = 0.29251220
Iteration 385, loss = 0.29319898
Iteration 386, loss = 0.28803102
Iteration 387, loss = 0.29215897
Iteration 388, loss = 0.28534043
Iteration 389, loss = 0.29455892
Iteration 390, loss = 0.29545747
Iteration 391, loss = 0.28634929
Iteration 392, loss = 0.29231432
Iteration 393, loss = 0.29413909
Iteration 394, loss = 0.27556152
Iteration 395, loss = 0.28574686
Iteration 396, loss = 0.28349810
Iteration 397, loss = 0.29704132
Iteration 398, loss = 0.29423065
Iteration 399, loss = 0.28054333
Iteration 400, loss = 0.28310671
Iteration 401, loss = 0.28251981
Iteration 402, loss = 0.29033714
Iteration 403, loss = 0.28588486
Iteration 404, loss = 0.28914722
Iteration 405, loss = 0.28579733
Iteration 406, loss = 0.28682577
Iteration 407, loss = 0.28240098
Iteration 408, loss = 0.28561912
Iteration 409, loss = 0.28062258
Iteration 410, loss = 0.28687913
Iteration 411, loss = 0.28819259
Iteration 412, loss = 0.28475671
Iteration 413, loss = 0.28909859
Iteration 414, loss = 0.28878188
Iteration 415, loss = 0.28499683
Iteration 416, loss = 0.29458277
Iteration 417, loss = 0.27962803
Iteration 418, loss = 0.28674441
Iteration 419, loss = 0.28816183
Iteration 420, loss = 0.28855528
Iteration 421, loss = 0.28737833
Iteration 422, loss = 0.28255246
Iteration 423, loss = 0.28808134
Iteration 424, loss = 0.28138850
Iteration 425, loss = 0.28252619
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 2/5] END estimator__activation=relu, estimator__hidden_layer_sizes=100;, score=0.874 total time= 6.2min
Iteration 1, loss = 3.11097485
Iteration 2, loss = 1.23650397
Iteration 3, loss = 1.06331923
Iteration 4, loss = 1.09375265
Iteration 5, loss = 1.22464645
Iteration 6, loss = 1.03575612
Iteration 7, loss = 1.07012169
Iteration 8, loss = 1.07427259
Iteration 9, loss = 1.00268715
Iteration 10, loss = 1.23881794
Iteration 11, loss = 1.27660490
Iteration 12, loss = 1.06125299
Iteration 13, loss = 0.98419389
Iteration 14, loss = 1.03789454
Iteration 15, loss = 1.11292729
Iteration 16, loss = 1.02508996
Iteration 17, loss = 1.07109748
Iteration 18, loss = 1.15141430
Iteration 19, loss = 1.09362424
Iteration 20, loss = 0.98550762
Iteration 21, loss = 1.00086190
Iteration 22, loss = 0.93412231
Iteration 23, loss = 1.15639528
Iteration 24, loss = 1.02240207
Iteration 25, loss = 0.95134102
Iteration 26, loss = 1.05592132
Iteration 27, loss = 0.99086803
Iteration 28, loss = 0.86542391
Iteration 29, loss = 0.92819816
Iteration 30, loss = 1.07529326
Iteration 31, loss = 0.90207557
Iteration 32, loss = 1.13997629
Iteration 33, loss = 1.09191659
Iteration 34, loss = 0.85476791
Iteration 35, loss = 0.90385236
Iteration 36, loss = 0.91670145
Iteration 37, loss = 1.06872493
Iteration 38, loss = 0.87441728
Iteration 39, loss = 1.03661427
Iteration 40, loss = 0.87871236
Iteration 41, loss = 0.86852975
Iteration 42, loss = 0.88462694
Iteration 43, loss = 1.10591103
Iteration 44, loss = 0.90889524
Iteration 45, loss = 0.84502675
Iteration 46, loss = 1.05773924
Iteration 47, loss = 0.82221015
Iteration 48, loss = 0.92047986
Iteration 49, loss = 0.88371899
Iteration 50, loss = 0.78915977
Iteration 51, loss = 1.00572277
Iteration 52, loss = 0.93301542
Iteration 53, loss = 0.82928971
Iteration 54, loss = 0.81937932
Iteration 55, loss = 0.88980593
Iteration 56, loss = 0.84967524
Iteration 57, loss = 0.80572882
Iteration 58, loss = 0.89565066
Iteration 59, loss = 0.86341627
Iteration 60, loss = 0.76401204
Iteration 61, loss = 0.93123023
Iteration 62, loss = 0.88413269
Iteration 63, loss = 0.80195226
Iteration 64, loss = 0.79853638
Iteration 65, loss = 0.91197293
Iteration 66, loss = 0.80230011
Iteration 67, loss = 0.76520843
Iteration 68, loss = 0.93239518
Iteration 69, loss = 0.78117764
Iteration 70, loss = 0.84841981
Iteration 71, loss = 0.81110301
Iteration 72, loss = 0.86936579
Iteration 73, loss = 0.79780224
Iteration 74, loss = 0.76449272
Iteration 75, loss = 0.83177977
Iteration 76, loss = 0.79572714
Iteration 77, loss = 0.84473446
Iteration 78, loss = 0.85689490
Iteration 79, loss = 0.76196189
Iteration 80, loss = 0.75481741
Iteration 81, loss = 0.75929058
Iteration 82, loss = 0.75360674
Iteration 83, loss = 0.84428489
Iteration 84, loss = 0.81914612
Iteration 85, loss = 0.77113910
Iteration 86, loss = 0.78835594
Iteration 87, loss = 0.78995107
Iteration 88, loss = 0.79375697
Iteration 89, loss = 0.79962067
Iteration 90, loss = 0.77977123
Iteration 91, loss = 0.80265873
Iteration 92, loss = 0.77055924
Iteration 93, loss = 0.78974610
Iteration 94, loss = 0.81463449
Iteration 95, loss = 0.74554816
Iteration 96, loss = 0.73723456
Iteration 97, loss = 0.70531082
Iteration 98, loss = 0.82203212
Iteration 99, loss = 0.75960995
Iteration 100, loss = 0.81357522
Iteration 101, loss = 0.71570013
Iteration 102, loss = 0.72039152
Iteration 103, loss = 0.79100344
Iteration 104, loss = 0.71879449
Iteration 105, loss = 0.77000527
Iteration 106, loss = 0.72538430
Iteration 107, loss = 0.76261501
Iteration 108, loss = 0.72424320
Iteration 109, loss = 0.74758848
Iteration 110, loss = 0.73756241
Iteration 111, loss = 0.71428734
Iteration 112, loss = 0.69242108
Iteration 113, loss = 0.76354129
Iteration 114, loss = 0.72022116
Iteration 115, loss = 0.77771265
Iteration 116, loss = 0.69026996
Iteration 117, loss = 0.74028089
Iteration 118, loss = 0.76074221
Iteration 119, loss = 0.73940979
Iteration 120, loss = 0.69032958
Iteration 121, loss = 0.73687864
Iteration 122, loss = 0.73594319
Iteration 123, loss = 0.69240440
Iteration 124, loss = 0.73714779
Iteration 125, loss = 0.73022248
Iteration 126, loss = 0.70794405
Iteration 127, loss = 0.67706966
Iteration 128, loss = 0.75491910
Iteration 129, loss = 0.71420883
Iteration 130, loss = 0.69916535
Iteration 131, loss = 0.71670046
Iteration 132, loss = 0.72536211
Iteration 133, loss = 0.69514167
Iteration 134, loss = 0.66830106
Iteration 135, loss = 0.70216822
Iteration 136, loss = 0.69200664
Iteration 137, loss = 0.69586586
Iteration 138, loss = 0.73500926
Iteration 139, loss = 0.68163805
Iteration 140, loss = 0.70763938
Iteration 141, loss = 0.68336754
Iteration 142, loss = 0.70786754
Iteration 143, loss = 0.69333610
Iteration 144, loss = 0.70554325
Iteration 145, loss = 0.68089844
Iteration 146, loss = 0.67189997
Iteration 147, loss = 0.70289090
Iteration 148, loss = 0.69000536
Iteration 149, loss = 0.68248615
Iteration 150, loss = 0.69425809
Iteration 151, loss = 0.67952098
Iteration 152, loss = 0.69651094
Iteration 153, loss = 0.67838925
Iteration 154, loss = 0.69828514
Iteration 155, loss = 0.68294462
Iteration 156, loss = 0.68658293
Iteration 157, loss = 0.67527710
Iteration 158, loss = 0.69043143
Iteration 159, loss = 0.69685291
Iteration 160, loss = 0.68009712
Iteration 161, loss = 0.68892818
Iteration 162, loss = 0.66615067
Iteration 163, loss = 0.68856738
Iteration 164, loss = 0.68364577
Iteration 165, loss = 0.66081846
Iteration 166, loss = 0.67764779
Iteration 167, loss = 0.70330545

Iteration 168, loss = 0.65273446
Iteration 169, loss = 0.63430259
Iteration 170, loss = 0.59872728
Iteration 171, loss = 0.57748454
Iteration 172, loss = 0.54698120
Iteration 173, loss = 0.54266167
Iteration 174, loss = 0.51548179
Iteration 175, loss = 0.50764161
Iteration 176, loss = 0.48920412
Iteration 177, loss = 0.46610985
Iteration 178, loss = 0.47278671
Iteration 179, loss = 0.44120137
Iteration 180, loss = 0.44810643
Iteration 181, loss = 0.45480911
Iteration 182, loss = 0.41476103
Iteration 183, loss = 0.40772570
Iteration 184, loss = 0.42048853
Iteration 185, loss = 0.41545332
Iteration 186, loss = 0.39243820
Iteration 187, loss = 0.38987491
Iteration 188, loss = 0.43116106
Iteration 189, loss = 0.38049928
Iteration 190, loss = 0.37565420
Iteration 191, loss = 0.39080001
Iteration 192, loss = 0.36641467
Iteration 193, loss = 0.36827399
Iteration 194, loss = 0.37484549
Iteration 195, loss = 0.36267807
Iteration 196, loss = 0.36952979
Iteration 197, loss = 0.34972225
Iteration 198, loss = 0.36847399
Iteration 199, loss = 0.36188009
Iteration 200, loss = 0.34568207
Iteration 201, loss = 0.34337448
Iteration 202, loss = 0.37651271
Iteration 203, loss = 0.34973921
Iteration 204, loss = 0.34950130
Iteration 205, loss = 0.34974187
Iteration 206, loss = 0.34073467
Iteration 207, loss = 0.34734110
Iteration 208, loss = 0.33460343
Iteration 209, loss = 0.34241086
Iteration 210, loss = 0.34809614
Iteration 211, loss = 0.33915782
Iteration 212, loss = 0.33967356
Iteration 213, loss = 0.33900294
Iteration 214, loss = 0.33060714
Iteration 215, loss = 0.34271344
Iteration 216, loss = 0.33861400
Iteration 217, loss = 0.32694142
Iteration 218, loss = 0.34351137
Iteration 219, loss = 0.33803032
Iteration 220, loss = 0.32850582
Iteration 221, loss = 0.33478408
Iteration 222, loss = 0.33032592
Iteration 223, loss = 0.33387170
Iteration 224, loss = 0.32660534
Iteration 225, loss = 0.33393875
Iteration 226, loss = 0.32181467
Iteration 227, loss = 0.31586565
Iteration 228, loss = 0.34151350
Iteration 229, loss = 0.32113617
Iteration 230, loss = 0.33985166
Iteration 231, loss = 0.31822297
Iteration 232, loss = 0.33277610
Iteration 233, loss = 0.31416585
Iteration 234, loss = 0.32139649
Iteration 235, loss = 0.32034852
Iteration 236, loss = 0.32699229
Iteration 237, loss = 0.32458474
Iteration 238, loss = 0.33820344
Iteration 239, loss = 0.30927557
Iteration 240, loss = 0.31857970
Iteration 241, loss = 0.32934361
Iteration 242, loss = 0.32221031
Iteration 243, loss = 0.30509978
Iteration 244, loss = 0.31540525
Iteration 245, loss = 0.32523563
Iteration 246, loss = 0.31204979
Iteration 247, loss = 0.33242883
Iteration 248, loss = 0.31888102
Iteration 249, loss = 0.30708473
Iteration 250, loss = 0.30983758
Iteration 251, loss = 0.32494073
Iteration 252, loss = 0.33601752
Iteration 253, loss = 0.32060862
Iteration 254, loss = 0.31886459
Iteration 255, loss = 0.32175502
Iteration 256, loss = 0.30729923
Iteration 257, loss = 0.30940986
Iteration 258, loss = 0.31314945
Iteration 259, loss = 0.31652262
Iteration 260, loss = 0.32294732
Iteration 261, loss = 0.32849347
Iteration 262, loss = 0.31167315
Iteration 263, loss = 0.30722452
Iteration 264, loss = 0.32371803
Iteration 265, loss = 0.31544371
Iteration 266, loss = 0.33285589
Iteration 267, loss = 0.30181563
Iteration 268, loss = 0.31421942
Iteration 269, loss = 0.31861531
Iteration 270, loss = 0.31360052
Iteration 271, loss = 0.32068560
Iteration 272, loss = 0.31026249
Iteration 273, loss = 0.30140218
Iteration 274, loss = 0.31892390
Iteration 275, loss = 0.32235966
Iteration 276, loss = 0.30972681
Iteration 277, loss = 0.31339495
Iteration 278, loss = 0.30841979
Iteration 279, loss = 0.30848732
Iteration 280, loss = 0.31780770
Iteration 281, loss = 0.30237386
Iteration 282, loss = 0.29979859
Iteration 283, loss = 0.32213724
Iteration 284, loss = 0.31246668
Iteration 285, loss = 0.32336909
Iteration 286, loss = 0.30911678
Iteration 287, loss = 0.31667187
Iteration 288, loss = 0.32253863
Iteration 289, loss = 0.30759503
Iteration 290, loss = 0.31341492
Iteration 291, loss = 0.31366673
Iteration 292, loss = 0.31979518
Iteration 293, loss = 0.31311716
Iteration 294, loss = 0.29657097
Iteration 295, loss = 0.32026951
Iteration 296, loss = 0.31506509
Iteration 297, loss = 0.31066987
Iteration 298, loss = 0.30959639
Iteration 299, loss = 0.31156116
Iteration 300, loss = 0.30173739
Iteration 301, loss = 0.30622440
Iteration 302, loss = 0.30847766
Iteration 303, loss = 0.31044171
Iteration 304, loss = 0.30412626
Iteration 305, loss = 0.32189806
Iteration 306, loss = 0.32494437
Iteration 307, loss = 0.30001603
Iteration 308, loss = 0.30456088
Iteration 309, loss = 0.30914896
Iteration 310, loss = 0.31186152
Iteration 311, loss = 0.29940372
Iteration 312, loss = 0.30009254
Iteration 313, loss = 0.31682744
Iteration 314, loss = 0.31775779
Iteration 315, loss = 0.30333940
Iteration 316, loss = 0.31465780
Iteration 317, loss = 0.30846921
Iteration 318, loss = 0.31002566
Iteration 319, loss = 0.31161309
Iteration 320, loss = 0.31105411
Iteration 321, loss = 0.29940510
Iteration 322, loss = 0.31698563
Iteration 323, loss = 0.30374494
Iteration 324, loss = 0.31020127
Iteration 325, loss = 0.29528703
Iteration 326, loss = 0.30862399
Iteration 327, loss = 0.30440497
Iteration 328, loss = 0.30590702
Iteration 329, loss = 0.30159063
Iteration 330, loss = 0.30702622
Iteration 331, loss = 0.29449676
Iteration 332, loss = 0.30194305
Iteration 333, loss = 0.30457503
Iteration 334, loss = 0.29330760
Iteration 335, loss = 0.30504810
Iteration 336, loss = 0.29555050
Iteration 337, loss = 0.30828289
Iteration 338, loss = 0.30585190
Iteration 339, loss = 0.29563527
Iteration 340, loss = 0.31199145
Iteration 341, loss = 0.30547812
Iteration 342, loss = 0.29248617
Iteration 343, loss = 0.30808248
Iteration 344, loss = 0.30697886
Iteration 345, loss = 0.29908647
Iteration 346, loss = 0.31080986
Iteration 347, loss = 0.30519057
Iteration 348, loss = 0.29758644
Iteration 349, loss = 0.28930193
Iteration 350, loss = 0.29267543
Iteration 351, loss = 0.31715763
Iteration 352, loss = 0.29488446
Iteration 353, loss = 0.30912020
Iteration 354, loss = 0.30738782
Iteration 355, loss = 0.29578987
Iteration 356, loss = 0.29807648
Iteration 357, loss = 0.29746357
Iteration 358, loss = 0.30531294
Iteration 359, loss = 0.29942878
Iteration 360, loss = 0.30057005
Iteration 361, loss = 0.30651126
Iteration 362, loss = 0.30112726
Iteration 363, loss = 0.29442959
Iteration 364, loss = 0.29618277
Iteration 365, loss = 0.31430067
Iteration 366, loss = 0.30357606
Iteration 367, loss = 0.30109479
Iteration 368, loss = 0.29718544
Iteration 369, loss = 0.29842029
Iteration 370, loss = 0.30330500
Iteration 371, loss = 0.30951882
Iteration 372, loss = 0.29610080
Iteration 373, loss = 0.29462287
Iteration 374, loss = 0.30492040
Iteration 375, loss = 0.29161859
Iteration 376, loss = 0.30879372
Iteration 377, loss = 0.29243976
Iteration 378, loss = 0.29551391
Iteration 379, loss = 0.29836004
Iteration 380, loss = 0.29755960
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 3/5] END estimator__activation=relu, estimator__hidden_layer_sizes=100;, score=0.902 total time= 5.3min
Iteration 1, loss = 2.84270313
Iteration 2, loss = 1.95500782
Iteration 3, loss = 1.40249348
Iteration 4, loss = 1.32959867
Iteration 5, loss = 1.55533314
Iteration 6, loss = 1.58064599
Iteration 7, loss = 1.11980914
Iteration 8, loss = 1.52041139
Iteration 9, loss = 1.44847113
Iteration 10, loss = 1.42596489
Iteration 11, loss = 1.23076441
Iteration 12, loss = 1.09718451
Iteration 13, loss = 1.57690404
Iteration 14, loss = 1.49326070
Iteration 15, loss = 0.97529831
Iteration 16, loss = 1.28950323
Iteration 17, loss = 1.21710519
Iteration 18, loss = 1.16972769
Iteration 19, loss = 1.38794936
Iteration 20, loss = 1.28842044
Iteration 21, loss = 1.22308599
Iteration 22, loss = 1.74438912
Iteration 23, loss = 1.20557176
Iteration 24, loss = 1.31449358
Iteration 25, loss = 1.00357227
Iteration 26, loss = 1.27311252
Iteration 27, loss = 1.28012829
Iteration 28, loss = 1.12457582
Iteration 29, loss = 1.06364327
Iteration 30, loss = 1.12937156
Iteration 31, loss = 1.23363649

Iteration 32, loss = 1.06702881
Iteration 33, loss = 1.08445411
Iteration 34, loss = 1.15331228
Iteration 35, loss = 1.24974865
Iteration 36, loss = 1.15174203
Iteration 37, loss = 1.05779345
Iteration 38, loss = 1.09367580
Iteration 39, loss = 1.23410091
Iteration 40, loss = 1.04892301
Iteration 41, loss = 1.31948560
Iteration 42, loss = 1.20112613
Iteration 43, loss = 1.13502756
Iteration 44, loss = 0.96468276
Iteration 45, loss = 1.05998701
Iteration 46, loss = 1.16662204
Iteration 47, loss = 0.89812852
Iteration 48, loss = 1.02134888
Iteration 49, loss = 1.04336322
Iteration 50, loss = 1.08726370
Iteration 51, loss = 0.84612687
Iteration 52, loss = 0.97084839
Iteration 53, loss = 1.21729462
Iteration 54, loss = 1.03200524
Iteration 55, loss = 0.94796119
Iteration 56, loss = 0.94524475
Iteration 57, loss = 1.00530881
Iteration 58, loss = 0.94035322
Iteration 59, loss = 0.91917035
Iteration 60, loss = 1.08093643
Iteration 61, loss = 0.91127969
Iteration 62, loss = 0.81167862
Iteration 63, loss = 0.89716275
Iteration 64, loss = 0.94430236
Iteration 65, loss = 0.97213865
Iteration 66, loss = 0.86364262
Iteration 67, loss = 1.03969456
Iteration 68, loss = 0.85083732
Iteration 69, loss = 0.95076867
Iteration 70, loss = 0.99136638
Iteration 71, loss = 0.74664902
Iteration 72, loss = 0.87776559
Iteration 73, loss = 1.02061997
Iteration 74, loss = 0.83150841
Iteration 75, loss = 0.89440432
Iteration 76, loss = 0.95904896
Iteration 77, loss = 0.95880224
Iteration 78, loss = 0.82956100
Iteration 79, loss = 0.87303679
Iteration 80, loss = 0.85551653
Iteration 81, loss = 0.77914387
Iteration 82, loss = 0.95668185
Iteration 83, loss = 0.85318491
Iteration 84, loss = 0.77024292
Iteration 85, loss = 0.81234089
Iteration 86, loss = 0.92138725
Iteration 87, loss = 0.89595836
Iteration 88, loss = 0.76299790
Iteration 89, loss = 0.90734657
Iteration 90, loss = 0.74149115
Iteration 91, loss = 0.81805112
Iteration 92, loss = 0.84331248
Iteration 93, loss = 0.83695966
Iteration 94, loss = 0.80921853
Iteration 95, loss = 0.82834370
Iteration 96, loss = 0.74525062
Iteration 97, loss = 0.79695469
Iteration 98, loss = 0.86281314
Iteration 99, loss = 0.80524450
Iteration 100, loss = 0.80258287
Iteration 101, loss = 0.78691039
Iteration 102, loss = 0.71418558
Iteration 103, loss = 0.81681145
Iteration 104, loss = 0.81362175
Iteration 105, loss = 0.76425169
Iteration 106, loss = 0.78196744
Iteration 107, loss = 0.82550489
Iteration 108, loss = 0.79756692
Iteration 109, loss = 0.79419127
Iteration 110, loss = 0.75398623
Iteration 111, loss = 0.77509946
Iteration 112, loss = 0.75176419
Iteration 113, loss = 0.81789640
Iteration 114, loss = 0.74522539
Iteration 115, loss = 0.77706152
Iteration 116, loss = 0.75268086
Iteration 117, loss = 0.80960414
Iteration 118, loss = 0.74002864
Iteration 119, loss = 0.72465423
Iteration 120, loss = 0.80742350
Iteration 121, loss = 0.77047896
Iteration 122, loss = 0.75163434
Iteration 123, loss = 0.74998497
Iteration 124, loss = 0.67597223
Iteration 125, loss = 0.72439361
Iteration 126, loss = 0.74191944
Iteration 127, loss = 0.80751708
Iteration 128, loss = 0.72441245
Iteration 129, loss = 0.76531150
Iteration 130, loss = 0.72673757
Iteration 131, loss = 0.73718884
Iteration 132, loss = 0.78494463
Iteration 133, loss = 0.70598233
Iteration 134, loss = 0.75646665
Iteration 135, loss = 0.73065609
Iteration 136, loss = 0.73401271
Iteration 137, loss = 0.73851092
Iteration 138, loss = 0.71518157
Iteration 139, loss = 0.75916407
Iteration 140, loss = 0.71618687
Iteration 141, loss = 0.73216132
Iteration 142, loss = 0.75106534
Iteration 143, loss = 0.74511120
Iteration 144, loss = 0.70813951
Iteration 145, loss = 0.73289466
Iteration 146, loss = 0.68855221
Iteration 147, loss = 0.74654349
Iteration 148, loss = 0.68107527
Iteration 149, loss = 0.73921035
Iteration 150, loss = 0.69958062
Iteration 151, loss = 0.67426879
Iteration 152, loss = 0.74164706
Iteration 153, loss = 0.70087579
Iteration 154, loss = 0.72810790
Iteration 155, loss = 0.72755486
Iteration 156, loss = 0.68146210
Iteration 157, loss = 0.74717247
Iteration 158, loss = 0.71679944
Iteration 159, loss = 0.75001656
Iteration 160, loss = 0.71418744
Iteration 161, loss = 0.68823020
Iteration 162, loss = 0.72410859
Iteration 163, loss = 0.71945990
Iteration 164, loss = 0.67682847
Iteration 165, loss = 0.71924720
Iteration 166, loss = 0.68823087
Iteration 167, loss = 0.70155590
Iteration 168, loss = 0.69198750
Iteration 169, loss = 0.72172454
Iteration 170, loss = 0.67082559
Iteration 171, loss = 0.71700370
Iteration 172, loss = 0.70168963
Iteration 173, loss = 0.69577874
Iteration 174, loss = 0.68003526
Iteration 175, loss = 0.69576750
Iteration 176, loss = 0.69570683
Iteration 177, loss = 0.70118328
Iteration 178, loss = 0.71815160
Iteration 179, loss = 0.67943083
Iteration 180, loss = 0.69234275
Iteration 181, loss = 0.71713448
Iteration 182, loss = 0.68469205
Iteration 183, loss = 0.68160214
Iteration 184, loss = 0.69674990
Iteration 185, loss = 0.68274571
Iteration 186, loss = 0.69236587
Iteration 187, loss = 0.67076087
Iteration 188, loss = 0.67328162
Iteration 189, loss = 0.71163517
Iteration 190, loss = 0.67034961
Iteration 191, loss = 0.67808709
Iteration 192, loss = 0.68248093
Iteration 193, loss = 0.69028609
Iteration 194, loss = 0.68063653
Iteration 195, loss = 0.68474223
Iteration 196, loss = 0.68496553
Iteration 197, loss = 0.65159067
Iteration 198, loss = 0.67689609
Iteration 199, loss = 0.67095169
Iteration 200, loss = 0.67420909
Iteration 201, loss = 0.68785010
Iteration 202, loss = 0.65806120
Iteration 203, loss = 0.66400282
Iteration 204, loss = 0.67815787
Iteration 205, loss = 0.66017580
Iteration 206, loss = 0.67652916
Iteration 207, loss = 0.65489407
Iteration 208, loss = 0.66425896
Iteration 209, loss = 0.69470167
Iteration 210, loss = 0.66262127
Iteration 211, loss = 0.66781411
Iteration 212, loss = 0.67293117
Iteration 213, loss = 0.64675016
Iteration 214, loss = 0.66346629
Iteration 215, loss = 0.66546649
Iteration 216, loss = 0.66300059
Iteration 217, loss = 0.66549315
Iteration 218, loss = 0.66134727
Iteration 219, loss = 0.66753937
Iteration 220, loss = 0.65719234
Iteration 221, loss = 0.65677827
Iteration 222, loss = 0.66556097
Iteration 223, loss = 0.64949513
Iteration 224, loss = 0.66285183
Iteration 225, loss = 0.65872137
Iteration 226, loss = 0.64323542
Iteration 227, loss = 0.65949499
Iteration 228, loss = 0.66018088
Iteration 229, loss = 0.64895126
Iteration 230, loss = 0.65656790
Iteration 231, loss = 0.65487736
Iteration 232, loss = 0.65920444
Iteration 233, loss = 0.64314389
Iteration 234, loss = 0.65014715
Iteration 235, loss = 0.64883932
Iteration 236, loss = 0.64563493
Iteration 237, loss = 0.67664421
Iteration 238, loss = 0.63556678
Iteration 239, loss = 0.64254439
Iteration 240, loss = 0.65100048
Iteration 241, loss = 0.65138544
Iteration 242, loss = 0.63671815
Iteration 243, loss = 0.64134592
Iteration 244, loss = 0.64767353
Iteration 245, loss = 0.64373790
Iteration 246, loss = 0.65308370
Iteration 247, loss = 0.64619479
Iteration 248, loss = 0.64119308
Iteration 249, loss = 0.65149997
Iteration 250, loss = 0.64646810
Iteration 251, loss = 0.65139462
Iteration 252, loss = 0.64081998
Iteration 253, loss = 0.64214689
Iteration 254, loss = 0.63717526
Iteration 255, loss = 0.65196719
Iteration 256, loss = 0.64307605
Iteration 257, loss = 0.63676930
Iteration 258, loss = 0.63753327
Iteration 259, loss = 0.64240490
Iteration 260, loss = 0.63537157
Iteration 261, loss = 0.64595539
Iteration 262, loss = 0.64406550
Iteration 263, loss = 0.63835608
Iteration 264, loss = 0.64091653
Iteration 265, loss = 0.63601885
Iteration 266, loss = 0.63520091
Iteration 267, loss = 0.63445123
Iteration 268, loss = 0.63629501
Iteration 269, loss = 0.62738775
Iteration 270, loss = 0.64016319
Iteration 271, loss = 0.62676605
Iteration 272, loss = 0.63027927
Iteration 273, loss = 0.64488254
Iteration 274, loss = 0.63378052
Iteration 275, loss = 0.65144901
Iteration 276, loss = 0.63461159
Iteration 277, loss = 0.65769929
Iteration 278, loss = 0.62978557
Iteration 279, loss = 0.63399292
Iteration 280, loss = 0.63933103
Iteration 281, loss = 0.62698076
Iteration 282, loss = 0.63204559

Iteration 283, loss = 0.63059230
Iteration 284, loss = 0.63904623
Iteration 285, loss = 0.62964300
Iteration 286, loss = 0.63381069
Iteration 287, loss = 0.63236912
Iteration 288, loss = 0.62099734
Iteration 289, loss = 0.62621113
Iteration 290, loss = 0.63088325
Iteration 291, loss = 0.62963717
Iteration 292, loss = 0.64093951
Iteration 293, loss = 0.62869752
Iteration 294, loss = 0.63660663
Iteration 295, loss = 0.62125150
Iteration 296, loss = 0.63616675
Iteration 297, loss = 0.62778530
Iteration 298, loss = 0.63208199
Iteration 299, loss = 0.63078378
Iteration 300, loss = 0.63242168
Iteration 301, loss = 0.63117421
Iteration 302, loss = 0.62703182
Iteration 303, loss = 0.62764117
Iteration 304, loss = 0.62352361
Iteration 305, loss = 0.62285905
Iteration 306, loss = 0.62707615
Iteration 307, loss = 0.62824751
Iteration 308, loss = 0.61984646
Iteration 309, loss = 0.64042838
Iteration 310, loss = 0.61649832
Iteration 311, loss = 0.62918622
Iteration 312, loss = 0.62156444
Iteration 313, loss = 0.62658836
Iteration 314, loss = 0.63068796
Iteration 315, loss = 0.62137095
Iteration 316, loss = 0.62263380
Iteration 317, loss = 0.62577758
Iteration 318, loss = 0.61840098
Iteration 319, loss = 0.62471641
Iteration 320, loss = 0.61970832
Iteration 321, loss = 0.61644160
Iteration 322, loss = 0.62136605
Iteration 323, loss = 0.62062892
Iteration 324, loss = 0.62090473
Iteration 325, loss = 0.62451476
Iteration 326, loss = 0.61660683
Iteration 327, loss = 0.62258874
Iteration 328, loss = 0.62207361
Iteration 329, loss = 0.61723959
Iteration 330, loss = 0.62594172
Iteration 331, loss = 0.61317391
Iteration 332, loss = 0.61506531
Iteration 333, loss = 0.62815322
Iteration 334, loss = 0.61777755
Iteration 335, loss = 0.61335562
Iteration 336, loss = 0.61866966
Iteration 337, loss = 0.62075332
Iteration 338, loss = 0.61960216
Iteration 339, loss = 0.61977894
Iteration 340, loss = 0.61667873
Iteration 341, loss = 0.61180893
Iteration 342, loss = 0.61974769
Iteration 343, loss = 0.62582309
Iteration 344, loss = 0.61598747
Iteration 345, loss = 0.61800976
Iteration 346, loss = 0.61659769
Iteration 347, loss = 0.61347036
Iteration 348, loss = 0.61413368
Iteration 349, loss = 0.61839467
Iteration 350, loss = 0.61203220
Iteration 351, loss = 0.61219832
Iteration 352, loss = 0.61410134
Iteration 353, loss = 0.61335145
Iteration 354, loss = 0.61585270
Iteration 355, loss = 0.61820510
Iteration 356, loss = 0.61581620
Iteration 357, loss = 0.61265001
Iteration 358, loss = 0.61016621
Iteration 359, loss = 0.61395528
Iteration 360, loss = 0.61202056
Iteration 361, loss = 0.61562173
Iteration 362, loss = 0.60906877
Iteration 363, loss = 0.61292277
Iteration 364, loss = 0.61259659
Iteration 365, loss = 0.60591800
Iteration 366, loss = 0.61615631
Iteration 367, loss = 0.60684760
Iteration 368, loss = 0.61212028
Iteration 369, loss = 0.60918526
Iteration 370, loss = 0.60731824
Iteration 371, loss = 0.60967075
Iteration 372, loss = 0.60914882
Iteration 373, loss = 0.60830731
Iteration 374, loss = 0.60675008
Iteration 375, loss = 0.61170184
Iteration 376, loss = 0.60878743
Iteration 377, loss = 0.60828883
Iteration 378, loss = 0.60862770
Iteration 379, loss = 0.60622446
Iteration 380, loss = 0.60741931
Iteration 381, loss = 0.60716378
Iteration 382, loss = 0.60735787
Iteration 383, loss = 0.60810093
Iteration 384, loss = 0.60682872
Iteration 385, loss = 0.60602759
Iteration 386, loss = 0.60646255
Iteration 387, loss = 0.60792833
Iteration 388, loss = 0.60657492
Iteration 389, loss = 0.60544976
Iteration 390, loss = 0.60423149
Iteration 391, loss = 0.60526639
Iteration 392, loss = 0.60676134
Iteration 393, loss = 0.60628805
Iteration 394, loss = 0.60790017
Iteration 395, loss = 0.60592759
Iteration 396, loss = 0.60456377
Iteration 397, loss = 0.60892340
Iteration 398, loss = 0.60289869
Iteration 399, loss = 0.60296144
Iteration 400, loss = 0.60427509
Iteration 401, loss = 0.60257542
Iteration 402, loss = 0.60629320
Iteration 403, loss = 0.60359359
Iteration 404, loss = 0.60505596
Iteration 405, loss = 0.60581136
Iteration 406, loss = 0.60249610
Iteration 407, loss = 0.60209259
Iteration 408, loss = 0.60635302
Iteration 409, loss = 0.60234345
Iteration 410, loss = 0.60438295
Iteration 411, loss = 0.60194560
Iteration 412, loss = 0.60320320
Iteration 413, loss = 0.60309149
Iteration 414, loss = 0.60390835
Iteration 415, loss = 0.60368994
Iteration 416, loss = 0.60398082
Iteration 417, loss = 0.60302195
Iteration 418, loss = 0.60208849
Iteration 419, loss = 0.60301513
Iteration 420, loss = 0.60084376
Iteration 421, loss = 0.60182962
Iteration 422, loss = 0.60168432
Iteration 423, loss = 0.60310580
Iteration 424, loss = 0.60333726
Iteration 425, loss = 0.60150970
Iteration 426, loss = 0.60158753
Iteration 427, loss = 0.59751688
Iteration 428, loss = 0.59698323
Iteration 429, loss = 0.59548242
Iteration 430, loss = 0.58769858
Iteration 431, loss = 0.57927375
Iteration 432, loss = 0.57444070
Iteration 433, loss = 0.56796817
Iteration 434, loss = 0.56023914
Iteration 435, loss = 0.55008323
Iteration 436, loss = 0.53911478
Iteration 437, loss = 0.53257722
Iteration 438, loss = 0.52027023
Iteration 439, loss = 0.50818766
Iteration 440, loss = 0.49515268
Iteration 441, loss = 0.48338362
Iteration 442, loss = 0.46938716
Iteration 443, loss = 0.46277366
Iteration 444, loss = 0.44537217
Iteration 445, loss = 0.43404531
Iteration 446, loss = 0.42339226
Iteration 447, loss = 0.41287950
Iteration 448, loss = 0.39742674
Iteration 449, loss = 0.39780912
Iteration 450, loss = 0.38927726
Iteration 451, loss = 0.37489235
Iteration 452, loss = 0.36899868
Iteration 453, loss = 0.36726729
Iteration 454, loss = 0.36000478
Iteration 455, loss = 0.35249008
Iteration 456, loss = 0.35190450
Iteration 457, loss = 0.34815574
Iteration 458, loss = 0.33859992
Iteration 459, loss = 0.33352606
Iteration 460, loss = 0.33144669
Iteration 461, loss = 0.33546003
Iteration 462, loss = 0.33017942
Iteration 463, loss = 0.33113664
Iteration 464, loss = 0.32254833
Iteration 465, loss = 0.33207761
Iteration 466, loss = 0.32216118
Iteration 467, loss = 0.32274357
Iteration 468, loss = 0.31730220
Iteration 469, loss = 0.31907537
Iteration 470, loss = 0.32430489
Iteration 471, loss = 0.31717592
Iteration 472, loss = 0.31797619
Iteration 473, loss = 0.31762422
Iteration 474, loss = 0.31617038
Iteration 475, loss = 0.30907133
Iteration 476, loss = 0.30685500
Iteration 477, loss = 0.31167345
Iteration 478, loss = 0.30955045
Iteration 479, loss = 0.30901320
Iteration 480, loss = 0.30630798
Iteration 481, loss = 0.30562526
Iteration 482, loss = 0.30509906
Iteration 483, loss = 0.30806047
Iteration 484, loss = 0.30359659
Iteration 485, loss = 0.30156122
Iteration 486, loss = 0.29917418
Iteration 487, loss = 0.30364565
Iteration 488, loss = 0.29913418
Iteration 489, loss = 0.30037585
Iteration 490, loss = 0.30482258
Iteration 491, loss = 0.30313629
Iteration 492, loss = 0.29701980
Iteration 493, loss = 0.30259066
Iteration 494, loss = 0.29388230
Iteration 495, loss = 0.30087240
Iteration 496, loss = 0.29723076
Iteration 497, loss = 0.29711152
Iteration 498, loss = 0.29871733
Iteration 499, loss = 0.29095069
Iteration 500, loss = 0.29562392
Iteration 501, loss = 0.29336022
Iteration 502, loss = 0.29540082
Iteration 503, loss = 0.29032381
Iteration 504, loss = 0.29347294
Iteration 505, loss = 0.30190182
Iteration 506, loss = 0.29219100
Iteration 507, loss = 0.29299083
Iteration 508, loss = 0.29525178
Iteration 509, loss = 0.28983986
Iteration 510, loss = 0.29171538
Iteration 511, loss = 0.29061874
Iteration 512, loss = 0.28904324
Iteration 513, loss = 0.29134113
Iteration 514, loss = 0.28666566
Iteration 515, loss = 0.28968956
Iteration 516, loss = 0.28429161
Iteration 517, loss = 0.28664199
Iteration 518, loss = 0.28374916
Iteration 519, loss = 0.29107197
Iteration 520, loss = 0.28996791
Iteration 521, loss = 0.29185240
Iteration 522, loss = 0.28699428
Iteration 523, loss = 0.28593661
Iteration 524, loss = 0.29469806
Iteration 525, loss = 0.29042153
Iteration 526, loss = 0.28586799
Iteration 527, loss = 0.28487821
Iteration 528, loss = 0.28664958
Iteration 529, loss = 0.28683543
Iteration 530, loss = 0.28770814
Iteration 531, loss = 0.28218490

Iteration 532, loss = 0.29534191
Iteration 533, loss = 0.28697183
Iteration 534, loss = 0.28642255
Iteration 535, loss = 0.29436903
Iteration 536, loss = 0.28654571
Iteration 537, loss = 0.28626024
Iteration 538, loss = 0.28256301
Iteration 539, loss = 0.28089158
Iteration 540, loss = 0.28456163
Iteration 541, loss = 0.28890111
Iteration 542, loss = 0.28459713
Iteration 543, loss = 0.28460186
Iteration 544, loss = 0.28763158
Iteration 545, loss = 0.28994109
Iteration 546, loss = 0.28672611
Iteration 547, loss = 0.28756013
Iteration 548, loss = 0.28749374
Iteration 549, loss = 0.28938657
Iteration 550, loss = 0.28134774
Iteration 551, loss = 0.28720875
Iteration 552, loss = 0.27957420
Iteration 553, loss = 0.28333886
Iteration 554, loss = 0.28577404
Iteration 555, loss = 0.28375197
Iteration 556, loss = 0.28734646
Iteration 557, loss = 0.29018793
Iteration 558, loss = 0.28317295
Iteration 559, loss = 0.28463201
Iteration 560, loss = 0.28426638
Iteration 561, loss = 0.28856422
Iteration 562, loss = 0.28195400
Iteration 563, loss = 0.28275117
Iteration 564, loss = 0.28755613
Iteration 565, loss = 0.28810039
Iteration 566, loss = 0.28879487
Iteration 567, loss = 0.27920152
Iteration 568, loss = 0.28646201
Iteration 569, loss = 0.28514799
Iteration 570, loss = 0.28290531
Iteration 571, loss = 0.27981665
Iteration 572, loss = 0.29390343
Iteration 573, loss = 0.28264909
Iteration 574, loss = 0.28457129
Iteration 575, loss = 0.27881394
Iteration 576, loss = 0.28462757
Iteration 577, loss = 0.27997543
Iteration 578, loss = 0.28737630
Iteration 579, loss = 0.28489860
Iteration 580, loss = 0.28393671
Iteration 581, loss = 0.28763726
Iteration 582, loss = 0.28333525
Iteration 583, loss = 0.27865883
Iteration 584, loss = 0.28683540
Iteration 585, loss = 0.28760588
Iteration 586, loss = 0.27794138
Iteration 587, loss = 0.28751890
Iteration 588, loss = 0.28032952
Iteration 589, loss = 0.28518576
Iteration 590, loss = 0.27819102
Iteration 591, loss = 0.27944508
Iteration 592, loss = 0.28075447
Iteration 593, loss = 0.28400545
Iteration 594, loss = 0.28366916
Iteration 595, loss = 0.28242606
Iteration 596, loss = 0.28685401
Iteration 597, loss = 0.28360416
Iteration 598, loss = 0.27695363
Iteration 599, loss = 0.28013466
Iteration 600, loss = 0.28113826
Iteration 601, loss = 0.28503186
Iteration 602, loss = 0.28542288
Iteration 603, loss = 0.27888900
Iteration 604, loss = 0.28552714
Iteration 605, loss = 0.28688059
Iteration 606, loss = 0.28195527
Iteration 607, loss = 0.27753800
Iteration 608, loss = 0.28153928
Iteration 609, loss = 0.28234720
Iteration 610, loss = 0.28234091
Iteration 611, loss = 0.28032720
Iteration 612, loss = 0.28432766
Iteration 613, loss = 0.28258529
Iteration 614, loss = 0.28069237
Iteration 615, loss = 0.28126870
Iteration 616, loss = 0.28064523
Iteration 617, loss = 0.28476535
Iteration 618, loss = 0.27753113
Iteration 619, loss = 0.27985092
Iteration 620, loss = 0.28416111
Iteration 621, loss = 0.27538929
Iteration 622, loss = 0.28843519
Iteration 623, loss = 0.28338541
Iteration 624, loss = 0.27883195
Iteration 625, loss = 0.28523722
Iteration 626, loss = 0.28390680
Iteration 627, loss = 0.28051785
Iteration 628, loss = 0.27706730
Iteration 629, loss = 0.28491848
Iteration 630, loss = 0.28017733
Iteration 631, loss = 0.28274071
Iteration 632, loss = 0.28621660
Iteration 633, loss = 0.27988987
Iteration 634, loss = 0.28030367
Iteration 635, loss = 0.28410927
Iteration 636, loss = 0.28302174
Iteration 637, loss = 0.27856769
Iteration 638, loss = 0.28116210
Iteration 639, loss = 0.28106739
Iteration 640, loss = 0.28225732
Iteration 641, loss = 0.27966416
Iteration 642, loss = 0.27506918
Iteration 643, loss = 0.27803186
Iteration 644, loss = 0.28346431
Iteration 645, loss = 0.28213924
Iteration 646, loss = 0.28691536
Iteration 647, loss = 0.27886050
Iteration 648, loss = 0.28118384
Iteration 649, loss = 0.28012209
Iteration 650, loss = 0.28242366
Iteration 651, loss = 0.27825383
Iteration 652, loss = 0.28647109
Iteration 653, loss = 0.28237112
Iteration 654, loss = 0.28653665
Iteration 655, loss = 0.27626064
Iteration 656, loss = 0.27882433
Iteration 657, loss = 0.28820354
Iteration 658, loss = 0.27499156
Iteration 659, loss = 0.28152501
Iteration 660, loss = 0.28158480
Iteration 661, loss = 0.28352116
Iteration 662, loss = 0.27743446
Iteration 663, loss = 0.27953066
Iteration 664, loss = 0.28152242
Iteration 665, loss = 0.27772195
Iteration 666, loss = 0.27928364
Iteration 667, loss = 0.28385005
Iteration 668, loss = 0.28088072
Iteration 669, loss = 0.27859796
Iteration 670, loss = 0.28061673
Iteration 671, loss = 0.28186447
Iteration 672, loss = 0.28114734
Iteration 673, loss = 0.28187963
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 4/5] END estimator__activation=relu, estimator__hidden_layer_sizes=100;, score=0.913 total time= 8.1min
Iteration 1, loss = 2.77101528
Iteration 2, loss = 1.30619453
Iteration 3, loss = 1.12139674
Iteration 4, loss = 1.23454640
Iteration 5, loss = 1.08531594
Iteration 6, loss = 1.00618404
Iteration 7, loss = 1.17107864
Iteration 8, loss = 1.08566167
Iteration 9, loss = 1.26236989
Iteration 10, loss = 1.01195651
Iteration 11, loss = 1.06853052
Iteration 12, loss = 1.33514093
Iteration 13, loss = 0.96646665
Iteration 14, loss = 1.04008875
Iteration 15, loss = 1.09292164
Iteration 16, loss = 0.97486766
Iteration 17, loss = 1.09177880
Iteration 18, loss = 1.04101205
Iteration 19, loss = 1.08604153
Iteration 20, loss = 1.07836387
Iteration 21, loss = 0.86352516
Iteration 22, loss = 1.04763003
Iteration 23, loss = 1.05892023
Iteration 24, loss = 0.96186054
Iteration 25, loss = 1.20383441
Iteration 26, loss = 0.88182579
Iteration 27, loss = 0.81838735
Iteration 28, loss = 1.08645150
Iteration 29, loss = 0.97096301
Iteration 30, loss = 0.99902621
Iteration 31, loss = 1.03528434
Iteration 32, loss = 0.91014887
Iteration 33, loss = 0.93077620
Iteration 34, loss = 1.07320321
Iteration 35, loss = 0.85089868
Iteration 36, loss = 1.10315512
Iteration 37, loss = 0.82035025
Iteration 38, loss = 0.91369858
Iteration 39, loss = 1.24613799
Iteration 40, loss = 0.94278211
Iteration 41, loss = 0.88159927
Iteration 42, loss = 0.87291412
Iteration 43, loss = 0.88104199
Iteration 44, loss = 0.92332637
Iteration 45, loss = 0.89485033
Iteration 46, loss = 0.92114819
Iteration 47, loss = 1.00690351
Iteration 48, loss = 0.90368709
Iteration 49, loss = 0.81423395
Iteration 50, loss = 0.83176484
Iteration 51, loss = 0.88605001
Iteration 52, loss = 0.90548254
Iteration 53, loss = 1.02810857
Iteration 54, loss = 0.78915009
Iteration 55, loss = 0.81378382
Iteration 56, loss = 0.87691690
Iteration 57, loss = 0.89378883
Iteration 58, loss = 0.78921027
Iteration 59, loss = 0.89900606
Iteration 60, loss = 0.75963209
Iteration 61, loss = 0.83220021
Iteration 62, loss = 0.86661455
Iteration 63, loss = 0.96512692
Iteration 64, loss = 0.75260658
Iteration 65, loss = 0.79255153
Iteration 66, loss = 0.78871267
Iteration 67, loss = 0.80718181
Iteration 68, loss = 0.79543310
Iteration 69, loss = 0.84315366
Iteration 70, loss = 0.86272460
Iteration 71, loss = 0.84966436
Iteration 72, loss = 0.79084635
Iteration 73, loss = 0.75279123
Iteration 74, loss = 0.90385064
Iteration 75, loss = 0.70515172
Iteration 76, loss = 0.75704015
Iteration 77, loss = 0.81035776
Iteration 78, loss = 0.74043245
Iteration 79, loss = 0.86794056
Iteration 80, loss = 0.71028132
Iteration 81, loss = 0.86952826
Iteration 82, loss = 0.79015892
Iteration 83, loss = 0.74038319
Iteration 84, loss = 0.76644735
Iteration 85, loss = 0.84167506
Iteration 86, loss = 0.73015720
Iteration 87, loss = 0.76480853
Iteration 88, loss = 0.79491209
Iteration 89, loss = 0.68282542
Iteration 90, loss = 0.74673908
Iteration 91, loss = 0.79059491
Iteration 92, loss = 0.75417712
Iteration 93, loss = 0.75466352
Iteration 94, loss = 0.74894522
Iteration 95, loss = 0.75861350
Iteration 96, loss = 0.73151363
Iteration 97, loss = 0.76166911
Iteration 98, loss = 0.75215300
Iteration 99, loss = 0.81386106
Iteration 100, loss = 0.75130861
Iteration 101, loss = 0.71433269
Iteration 102, loss = 0.75712729
Iteration 103, loss = 0.78384160
Iteration 104, loss = 0.79752448

Iteration 105, loss = 0.71125677
Iteration 106, loss = 0.70441467
Iteration 107, loss = 0.74422719
Iteration 108, loss = 0.73027083
Iteration 109, loss = 0.80718321
Iteration 110, loss = 0.70223014
Iteration 111, loss = 0.69551959
Iteration 112, loss = 0.76401172
Iteration 113, loss = 0.73870480
Iteration 114, loss = 0.72903780
Iteration 115, loss = 0.69759642
Iteration 116, loss = 0.71911195
Iteration 117, loss = 0.69581429
Iteration 118, loss = 0.73413799
Iteration 119, loss = 0.76884561
Iteration 120, loss = 0.70095472
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 5/5] END estimator__activation=relu, estimator__hidden_layer_sizes=100;, score=0.679 total time= 2.0min
Iteration 1, loss = 1.88066374
Iteration 2, loss = 0.91472497
Iteration 3, loss = 0.86064297
Iteration 4, loss = 0.80057603
Iteration 5, loss = 0.78845460
Iteration 6, loss = 0.75245838
Iteration 7, loss = 0.72519884
Iteration 8, loss = 0.70545289
Iteration 9, loss = 0.65600818
Iteration 10, loss = 0.66286595
Iteration 11, loss = 0.67108130
Iteration 12, loss = 0.65660428
Iteration 13, loss = 0.63748260
Iteration 14, loss = 0.64072040
Iteration 15, loss = 0.63736018
Iteration 16, loss = 0.63722384
Iteration 17, loss = 0.62967013
Iteration 18, loss = 0.62354738
Iteration 19, loss = 0.62117095
Iteration 20, loss = 0.62577382
Iteration 21, loss = 0.62094250
Iteration 22, loss = 0.61916626
Iteration 23, loss = 0.61797622
Iteration 24, loss = 0.62008588
Iteration 25, loss = 0.61589842
Iteration 26, loss = 0.61542148
Iteration 27, loss = 0.61437145
Iteration 28, loss = 0.61066019
Iteration 29, loss = 0.60981487
Iteration 30, loss = 0.60772536
Iteration 31, loss = 0.60713848
Iteration 32, loss = 0.60698469
Iteration 33, loss = 0.61162633
Iteration 34, loss = 0.60384887
Iteration 35, loss = 0.60362880
Iteration 36, loss = 0.60365413
Iteration 37, loss = 0.59925080
Iteration 38, loss = 0.59338600
Iteration 39, loss = 0.59568228
Iteration 40, loss = 0.60066112
Iteration 41, loss = 0.59261145
Iteration 42, loss = 0.62158386
Iteration 43, loss = 0.61380868
Iteration 44, loss = 0.59324745
Iteration 45, loss = 0.61884338
Iteration 46, loss = 0.61863534
Iteration 47, loss = 0.62073439
Iteration 48, loss = 0.65037511
Iteration 49, loss = 0.65060073
Iteration 50, loss = 0.61040708
Iteration 51, loss = 0.60690359
Iteration 52, loss = 0.60570246
Iteration 53, loss = 0.60196687
Iteration 54, loss = 0.60786731
Iteration 55, loss = 0.60238208
Iteration 56, loss = 0.60602372
Iteration 57, loss = 0.60209748
Iteration 58, loss = 0.60230445
Iteration 59, loss = 0.60666182
Iteration 60, loss = 0.60124589
Iteration 61, loss = 0.60280559
Iteration 62, loss = 0.60429094
Iteration 63, loss = 0.59858999
Iteration 64, loss = 0.59743195
Iteration 65, loss = 0.60138759
Iteration 66, loss = 0.60202008
Iteration 67, loss = 0.59948661
Iteration 68, loss = 0.60523911
Iteration 69, loss = 0.60203508
Iteration 70, loss = 0.60138094
Iteration 71, loss = 0.60413038
Iteration 72, loss = 0.60300735
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 1/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(100, 50, 25);, score=0.689 total time= 2.0min
Iteration 1, loss = 2.39235612
Iteration 2, loss = 1.17722975
Iteration 3, loss = 0.97788307
Iteration 4, loss = 0.90497378
Iteration 5, loss = 0.79577358
Iteration 6, loss = 0.75010709
Iteration 7, loss = 0.75006727
Iteration 8, loss = 0.69075648
Iteration 9, loss = 0.67735980
Iteration 10, loss = 0.66360360
Iteration 11, loss = 0.65802121
Iteration 12, loss = 0.65618388
Iteration 13, loss = 0.64390151
Iteration 14, loss = 0.63668870
Iteration 15, loss = 0.64016042
Iteration 16, loss = 0.63280110
Iteration 17, loss = 0.62925638
Iteration 18, loss = 0.63232299
Iteration 19, loss = 0.62367034
Iteration 20, loss = 0.62429177
Iteration 21, loss = 0.63005693
Iteration 22, loss = 0.61912897
Iteration 23, loss = 0.62022445
Iteration 24, loss = 0.61991156
Iteration 25, loss = 0.61753784
Iteration 26, loss = 0.61309865
Iteration 27, loss = 0.61246311
Iteration 28, loss = 0.61539868
Iteration 29, loss = 0.61253497
Iteration 30, loss = 0.61347077
Iteration 31, loss = 0.61146175
Iteration 32, loss = 0.61088991
Iteration 33, loss = 0.60848265
Iteration 34, loss = 0.64187907
Iteration 35, loss = 0.68904020
Iteration 36, loss = 0.68903041
Iteration 37, loss = 0.68903856
Iteration 38, loss = 0.68959847
Iteration 39, loss = 0.68920490
Iteration 40, loss = 0.68908904
Iteration 41, loss = 0.68904200
Iteration 42, loss = 0.68904051
Iteration 43, loss = 0.68898253
Iteration 44, loss = 0.68901349
Iteration 45, loss = 0.68904559
Iteration 46, loss = 0.68898669
Iteration 47, loss = 0.68902757
Iteration 48, loss = 0.68904927
Iteration 49, loss = 0.68902557
Iteration 50, loss = 0.68976356
Iteration 51, loss = 0.68900144
Iteration 52, loss = 0.68902163
Iteration 53, loss = 0.68947291
Iteration 54, loss = 0.68902724
Iteration 55, loss = 0.68922533
Iteration 56, loss = 0.68901501
Iteration 57, loss = 0.68915478
Iteration 58, loss = 0.68919527
Iteration 59, loss = 0.68899715
Iteration 60, loss = 0.68901420
Iteration 61, loss = 0.68899625
Iteration 62, loss = 0.68898453
Iteration 63, loss = 0.68895802
Iteration 64, loss = 0.68899860
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 2/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(100, 50, 25);, score=0.546 total time= 1.8min
Iteration 1, loss = 2.34591385
Iteration 2, loss = 1.29209215
Iteration 3, loss = 0.95674181
Iteration 4, loss = 0.87645807
Iteration 5, loss = 0.80084223
Iteration 6, loss = 0.79332019
Iteration 7, loss = 0.70184383
Iteration 8, loss = 0.66645709
Iteration 9, loss = 0.66345587
Iteration 10, loss = 0.65526603
Iteration 11, loss = 0.64123099
Iteration 12, loss = 0.64216492
Iteration 13, loss = 0.63429371
Iteration 14, loss = 0.63992157
Iteration 15, loss = 0.62845078
Iteration 16, loss = 0.62389157
Iteration 17, loss = 0.63091638
Iteration 18, loss = 0.62706482
Iteration 19, loss = 0.62806917
Iteration 20, loss = 0.62433529
Iteration 21, loss = 0.62859967
Iteration 22, loss = 0.61473927
Iteration 23, loss = 0.62515252
Iteration 24, loss = 0.62038957
Iteration 25, loss = 0.61721442
Iteration 26, loss = 0.61291891
Iteration 27, loss = 0.61277756
Iteration 28, loss = 0.61209054
Iteration 29, loss = 0.62262299
Iteration 30, loss = 0.61336643
Iteration 31, loss = 0.61504905
Iteration 32, loss = 0.61030238
Iteration 33, loss = 0.61079603
Iteration 34, loss = 0.60754728
Iteration 35, loss = 0.61070586
Iteration 36, loss = 0.60424537
Iteration 37, loss = 0.61220751
Iteration 38, loss = 0.61111742
Iteration 39, loss = 0.60617687
Iteration 40, loss = 0.60480505
Iteration 41, loss = 0.60815853
Iteration 42, loss = 0.60338825
Iteration 43, loss = 0.60293778
Iteration 44, loss = 0.60199670
Iteration 45, loss = 0.59681579
Iteration 46, loss = 0.60414474
Iteration 47, loss = 0.60338044
Iteration 48, loss = 0.59978232
Iteration 49, loss = 0.60763799
Iteration 50, loss = 0.60675690
Iteration 51, loss = 0.60473909
Iteration 52, loss = 0.61016260
Iteration 53, loss = 0.60365989
Iteration 54, loss = 0.59644230
Iteration 55, loss = 0.59456007
Iteration 56, loss = 0.61950954
Iteration 57, loss = 0.68363764
Iteration 58, loss = 0.68914149
Iteration 59, loss = 0.68911431
Iteration 60, loss = 0.68912237
Iteration 61, loss = 0.68920518
Iteration 62, loss = 0.68915303
Iteration 63, loss = 0.68913935
Iteration 64, loss = 0.68913535
Iteration 65, loss = 0.68914190
Iteration 66, loss = 0.68910493
Iteration 67, loss = 0.68913145
Iteration 68, loss = 0.68931018
Iteration 69, loss = 0.68907331
Iteration 70, loss = 0.68909967
Iteration 71, loss = 0.69360796
Iteration 72, loss = 0.68912036
Iteration 73, loss = 0.68907295
Iteration 74, loss = 0.68909529
Iteration 75, loss = 0.68908495
Iteration 76, loss = 0.68908842
Iteration 77, loss = 0.68922615
Iteration 78, loss = 0.68911247
Iteration 79, loss = 0.68916807
Iteration 80, loss = 0.68914127
Iteration 81, loss = 0.68927719
Iteration 82, loss = 0.68907572
Iteration 83, loss = 0.68908421
Iteration 84, loss = 0.68941252
Iteration 85, loss = 0.68909080
Iteration 86, loss = 0.68906469
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.

[CV 3/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(100, 50, 25);, score=0.546 total time= 2.1min
Iteration 1, loss = 2.52895535
Iteration 2, loss = 2.00705544
Iteration 3, loss = 1.35925731
Iteration 4, loss = 1.15124283
Iteration 5, loss = 1.01342518
Iteration 6, loss = 0.96808801
Iteration 7, loss = 0.79222817
Iteration 8, loss = 0.85410196
Iteration 9, loss = 0.93020878
Iteration 10, loss = 0.75123361
Iteration 11, loss = 0.81253162
Iteration 12, loss = 0.71749607
Iteration 13, loss = 0.69677848
Iteration 14, loss = 0.71460269
Iteration 15, loss = 0.68327064
Iteration 16, loss = 0.68552234
Iteration 17, loss = 0.65194398
Iteration 18, loss = 0.66773846
Iteration 19, loss = 0.66261915
Iteration 20, loss = 0.63550154
Iteration 21, loss = 0.64873471
Iteration 22, loss = 0.63526163
Iteration 23, loss = 0.63639757
Iteration 24, loss = 0.62684560
Iteration 25, loss = 0.63002020
Iteration 26, loss = 0.62426026
Iteration 27, loss = 0.62375016
Iteration 28, loss = 0.62047273
Iteration 29, loss = 0.61911020
Iteration 30, loss = 0.61711513
Iteration 31, loss = 0.62102000
Iteration 32, loss = 0.61931832
Iteration 33, loss = 0.60784882
Iteration 34, loss = 0.61593153
Iteration 35, loss = 0.62057757
Iteration 36, loss = 0.61432882
Iteration 37, loss = 0.61159544
Iteration 38, loss = 0.60547614
Iteration 39, loss = 0.60899821
Iteration 40, loss = 0.59855074
Iteration 41, loss = 0.61765636
Iteration 42, loss = 0.60312181
Iteration 43, loss = 0.60524286
Iteration 44, loss = 0.60898366
Iteration 45, loss = 0.60368579
Iteration 46, loss = 0.60356958
Iteration 47, loss = 0.59926990
Iteration 48, loss = 0.60792920
Iteration 49, loss = 0.60142537
Iteration 50, loss = 0.59365650
Iteration 51, loss = 0.58849902
Iteration 52, loss = 0.58987317
Iteration 53, loss = 0.58061011
Iteration 54, loss = 0.58538312
Iteration 55, loss = 0.58195806
Iteration 56, loss = 0.58088370
Iteration 57, loss = 0.57588657
Iteration 58, loss = 0.58799274
Iteration 59, loss = 0.57622484
Iteration 60, loss = 0.60692226
Iteration 61, loss = 0.59358299
Iteration 62, loss = 0.58637817
Iteration 63, loss = 0.60410445
Iteration 64, loss = 0.61601947
Iteration 65, loss = 0.60882705
Iteration 66, loss = 0.60459615
Iteration 67, loss = 0.60854808
Iteration 68, loss = 0.60132360
Iteration 69, loss = 0.60764139
Iteration 70, loss = 0.60212738
Iteration 71, loss = 0.60276824
Iteration 72, loss = 0.59823797
Iteration 73, loss = 0.60003377
Iteration 74, loss = 0.59787141
Iteration 75, loss = 0.59709109
Iteration 76, loss = 0.59711584
Iteration 77, loss = 0.59872128
Iteration 78, loss = 0.59827427
Iteration 79, loss = 0.59859129
Iteration 80, loss = 0.59412184
Iteration 81, loss = 0.59276038
Iteration 82, loss = 0.59236297
Iteration 83, loss = 0.59869662
Iteration 84, loss = 0.58917420
Iteration 85, loss = 0.59218645
Iteration 86, loss = 0.60182903
Iteration 87, loss = 0.59580494
Iteration 88, loss = 0.59640921
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 4/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(100, 50, 25);, score=0.689 total time= 2.3min
Iteration 1, loss = 2.81734083
Iteration 2, loss = 1.47948919
Iteration 3, loss = 1.08767524
Iteration 4, loss = 1.08981414
Iteration 5, loss = 0.91856729
Iteration 6, loss = 0.80678647
Iteration 7, loss = 0.87384094
Iteration 8, loss = 0.74867516
Iteration 9, loss = 0.72615491
Iteration 10, loss = 0.75072858
Iteration 11, loss = 0.70897465
Iteration 12, loss = 0.69036784
Iteration 13, loss = 0.66378097
Iteration 14, loss = 0.65796550
Iteration 15, loss = 0.65282830
Iteration 16, loss = 0.65050676
Iteration 17, loss = 0.63073749
Iteration 18, loss = 0.63222831
Iteration 19, loss = 0.62961558
Iteration 20, loss = 0.63538686
Iteration 21, loss = 0.62556471
Iteration 22, loss = 0.62606590
Iteration 23, loss = 0.62301499
Iteration 24, loss = 0.62145071
Iteration 25, loss = 0.62904422
Iteration 26, loss = 0.61774130
Iteration 27, loss = 0.61200947
Iteration 28, loss = 0.61496185
Iteration 29, loss = 0.61437499
Iteration 30, loss = 0.61041950
Iteration 31, loss = 0.61084599
Iteration 32, loss = 0.61016273
Iteration 33, loss = 0.60808333
Iteration 34, loss = 0.60955699
Iteration 35, loss = 0.60704806
Iteration 36, loss = 0.60580517
Iteration 37, loss = 0.60341998
Iteration 38, loss = 0.60229836
Iteration 39, loss = 0.60614148
Iteration 40, loss = 0.60149752
Iteration 41, loss = 0.60322642
Iteration 42, loss = 0.59491421
Iteration 43, loss = 0.60754430
Iteration 44, loss = 0.59198912
Iteration 45, loss = 0.59104478
Iteration 46, loss = 0.59241408
Iteration 47, loss = 0.58738271
Iteration 48, loss = 0.58907442
Iteration 49, loss = 0.58482351
Iteration 50, loss = 0.58450909
Iteration 51, loss = 0.58692230
Iteration 52, loss = 0.59704146
Iteration 53, loss = 0.60441944
Iteration 54, loss = 0.59559782
Iteration 55, loss = 0.61011705
Iteration 56, loss = 0.60757835
Iteration 57, loss = 0.60349736
Iteration 58, loss = 0.60470918
Iteration 59, loss = 0.60450221
Iteration 60, loss = 0.59992114
Iteration 61, loss = 0.60678895
Iteration 62, loss = 0.60029961
Iteration 63, loss = 0.59925667
Iteration 64, loss = 0.60185066
Iteration 65, loss = 0.60109839
Iteration 66, loss = 0.59825223
Iteration 67, loss = 0.60220071
Iteration 68, loss = 0.59642894
Iteration 69, loss = 0.59704316
Iteration 70, loss = 0.59984233
Iteration 71, loss = 0.59986954
Iteration 72, loss = 0.59703333
Iteration 73, loss = 0.59442340
Iteration 74, loss = 0.59692610
Iteration 75, loss = 0.59290045
Iteration 76, loss = 0.59258764
Iteration 77, loss = 0.59268243
Iteration 78, loss = 0.59521568
Iteration 79, loss = 0.58891009
Iteration 80, loss = 0.59365467
Iteration 81, loss = 0.59024902
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 5/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(100, 50, 25);, score=0.685 total time= 2.0min
Iteration 1, loss = 2.22202637
Iteration 2, loss = 0.90606228
Iteration 3, loss = 0.81214523
Iteration 4, loss = 0.73378833
Iteration 5, loss = 0.72043055
Iteration 6, loss = 0.69416222
Iteration 7, loss = 0.66922030
Iteration 8, loss = 0.67127523
Iteration 9, loss = 0.65520280
Iteration 10, loss = 0.64580451
Iteration 11, loss = 0.63807068
Iteration 12, loss = 0.63864360
Iteration 13, loss = 0.62873627
Iteration 14, loss = 0.62369313
Iteration 15, loss = 0.62285140
Iteration 16, loss = 0.61815376
Iteration 17, loss = 0.61644568
Iteration 18, loss = 0.61267126
Iteration 19, loss = 0.61276722
Iteration 20, loss = 0.62700467
Iteration 21, loss = 0.61396687
Iteration 22, loss = 0.61878318
Iteration 23, loss = 0.61386676
Iteration 24, loss = 0.61319070
Iteration 25, loss = 0.60984031
Iteration 26, loss = 0.61370409
Iteration 27, loss = 0.61143193
Iteration 28, loss = 0.60859618
Iteration 29, loss = 0.60812508
Iteration 30, loss = 0.61014629
Iteration 31, loss = 0.61128743
Iteration 32, loss = 0.60904098
Iteration 33, loss = 0.60650137
Iteration 34, loss = 0.60924504
Iteration 35, loss = 0.61044768
Iteration 36, loss = 0.60683109
Iteration 37, loss = 0.60480644
Iteration 38, loss = 0.60268977
Iteration 39, loss = 0.60150360
Iteration 40, loss = 0.59580100
Iteration 41, loss = 0.60305035
Iteration 42, loss = 0.59885710
Iteration 43, loss = 0.59492643
Iteration 44, loss = 0.59272488
Iteration 45, loss = 0.59320949
Iteration 46, loss = 0.61893831
Iteration 47, loss = 0.61730629
Iteration 48, loss = 0.61821879
Iteration 49, loss = 0.61864874
Iteration 50, loss = 0.60157933
Iteration 51, loss = 0.60364173
Iteration 52, loss = 0.61829124
Iteration 53, loss = 0.62079606
Iteration 54, loss = 0.62156445
Iteration 55, loss = 0.61957777
Iteration 56, loss = 0.62200397
Iteration 57, loss = 0.62182759
Iteration 58, loss = 0.61701692
Iteration 59, loss = 0.62236961
Iteration 60, loss = 0.61651022
Iteration 61, loss = 0.61522074
Iteration 62, loss = 0.61764857
Iteration 63, loss = 0.61605582
Iteration 64, loss = 0.61649063
Iteration 65, loss = 0.61530909
Iteration 66, loss = 0.61619369
Iteration 67, loss = 0.61603283
Iteration 68, loss = 0.61668138
Iteration 69, loss = 0.61425484
Iteration 70, loss = 0.61531385
Iteration 71, loss = 0.61552567
Iteration 72, loss = 0.61588103

Iteration 73, loss = 0.61629824
Iteration 74, loss = 0.61490074
Iteration 75, loss = 0.61504578
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 1/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(100, 100, 100);, score=0.589 total time= 3.3min
Iteration 1, loss = 1.76795652
Iteration 2, loss = 0.83071933
Iteration 3, loss = 0.76278347
Iteration 4, loss = 0.74067261
Iteration 5, loss = 0.70432604
Iteration 6, loss = 0.69939761
Iteration 7, loss = 0.66531767
Iteration 8, loss = 0.65588709
Iteration 9, loss = 0.64940448
Iteration 10, loss = 0.63906109
Iteration 11, loss = 0.64436037
Iteration 12, loss = 0.62991145
Iteration 13, loss = 0.62376339
Iteration 14, loss = 0.63400669
Iteration 15, loss = 0.62000274
Iteration 16, loss = 0.62155262
Iteration 17, loss = 0.61747406
Iteration 18, loss = 0.62088108
Iteration 19, loss = 0.61593657
Iteration 20, loss = 0.61290366
Iteration 21, loss = 0.60812226
Iteration 22, loss = 0.60934879
Iteration 23, loss = 0.60857306
Iteration 24, loss = 0.60653744
Iteration 25, loss = 0.60276281
Iteration 26, loss = 0.60411062
Iteration 27, loss = 0.60008687
Iteration 28, loss = 0.60144405
Iteration 29, loss = 0.60929488
Iteration 30, loss = 0.61677194
Iteration 31, loss = 0.61369396
Iteration 32, loss = 0.61447318
Iteration 33, loss = 0.61248827
Iteration 34, loss = 0.60784957
Iteration 35, loss = 0.60808517
Iteration 36, loss = 0.60581791
Iteration 37, loss = 0.60710205
Iteration 38, loss = 0.60222287
Iteration 39, loss = 0.60094727
Iteration 40, loss = 0.60063016
Iteration 41, loss = 0.60155913
Iteration 42, loss = 0.59790390
Iteration 43, loss = 0.59946448
Iteration 44, loss = 0.59414813
Iteration 45, loss = 0.59299980
Iteration 46, loss = 0.59480405
Iteration 47, loss = 0.59322882
Iteration 48, loss = 0.59332480
Iteration 49, loss = 0.59426299
Iteration 50, loss = 0.59695082
Iteration 51, loss = 0.59093229
Iteration 52, loss = 0.59811406
Iteration 53, loss = 0.59145067
Iteration 54, loss = 0.58493798
Iteration 55, loss = 0.58742593
Iteration 56, loss = 0.59619746
Iteration 57, loss = 0.59370617
Iteration 58, loss = 0.58591123
Iteration 59, loss = 0.60871453
Iteration 60, loss = 0.60338241
Iteration 61, loss = 0.60341847
Iteration 62, loss = 0.60211677
Iteration 63, loss = 0.60085361
Iteration 64, loss = 0.60069339
Iteration 65, loss = 0.60002880
Iteration 66, loss = 0.59815950
Iteration 67, loss = 0.60173769
Iteration 68, loss = 0.59886309
Iteration 69, loss = 0.59825212
Iteration 70, loss = 0.59760955
Iteration 71, loss = 0.59690235
Iteration 72, loss = 0.59926609
Iteration 73, loss = 0.59527480
Iteration 74, loss = 0.59429456
Iteration 75, loss = 0.59589223
Iteration 76, loss = 0.59473791
Iteration 77, loss = 0.59462147
Iteration 78, loss = 0.59687600
Iteration 79, loss = 0.59468377
Iteration 80, loss = 0.59741918
Iteration 81, loss = 0.59520155
Iteration 82, loss = 0.59625522
Iteration 83, loss = 0.59468932
Iteration 84, loss = 0.59371320
Iteration 85, loss = 0.59117846
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 2/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(100, 100, 100);, score=0.666 total time= 3.5min
Iteration 1, loss = 3.29686944
Iteration 2, loss = 1.17909120
Iteration 3, loss = 0.90531269
Iteration 4, loss = 0.84272850
Iteration 5, loss = 0.74720167
Iteration 6, loss = 0.73036037
Iteration 7, loss = 0.73029081
Iteration 8, loss = 0.68039018
Iteration 9, loss = 0.65442400
Iteration 10, loss = 0.65618587
Iteration 11, loss = 0.64826706
Iteration 12, loss = 0.64504831
Iteration 13, loss = 0.64123696
Iteration 14, loss = 0.63494527
Iteration 15, loss = 0.63529551
Iteration 16, loss = 0.62916803
Iteration 17, loss = 0.63910308
Iteration 18, loss = 0.62879787
Iteration 19, loss = 0.62454865
Iteration 20, loss = 0.62077159
Iteration 21, loss = 0.64524061
Iteration 22, loss = 0.68904622
Iteration 23, loss = 0.68903609
Iteration 24, loss = 0.68902724
Iteration 25, loss = 0.68891233
Iteration 26, loss = 0.68901526
Iteration 27, loss = 0.68901835
Iteration 28, loss = 0.68900453
Iteration 29, loss = 0.68903056
Iteration 30, loss = 0.68900257
Iteration 31, loss = 0.68899788
Iteration 32, loss = 0.68901090
Iteration 33, loss = 0.68896058
Iteration 34, loss = 0.68900136
Iteration 35, loss = 0.68899613
Iteration 36, loss = 0.68901894
Iteration 37, loss = 0.68899536
Iteration 38, loss = 0.68940897
Iteration 39, loss = 0.68899547
Iteration 40, loss = 0.68899418
Iteration 41, loss = 0.69047184
Iteration 42, loss = 0.68897225
Iteration 43, loss = 0.68900764
Iteration 44, loss = 0.68897660
Iteration 45, loss = 0.68896559
Iteration 46, loss = 0.68897861
Iteration 47, loss = 0.68897994
Iteration 48, loss = 0.68897490
Iteration 49, loss = 0.68897682
Iteration 50, loss = 0.68896535
Iteration 51, loss = 0.68901618
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 3/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(100, 100, 100);, score=0.546 total time= 2.4min
Iteration 1, loss = 2.76761561
Iteration 2, loss = 1.12382574
Iteration 3, loss = 0.95099473
Iteration 4, loss = 0.76847875
Iteration 5, loss = 0.75817486
Iteration 6, loss = 0.73195418
Iteration 7, loss = 0.69567302
Iteration 8, loss = 0.69074424
Iteration 9, loss = 0.65452270
Iteration 10, loss = 0.65289885
Iteration 11, loss = 0.64732668
Iteration 12, loss = 0.63891936
Iteration 13, loss = 0.62694110
Iteration 14, loss = 0.63352168
Iteration 15, loss = 0.62379544
Iteration 16, loss = 0.62216655
Iteration 17, loss = 0.62418219
Iteration 18, loss = 0.61980103
Iteration 19, loss = 0.61603573
Iteration 20, loss = 0.61723747
Iteration 21, loss = 0.61900940
Iteration 22, loss = 0.61244308
Iteration 23, loss = 0.61404279
Iteration 24, loss = 0.63211114
Iteration 25, loss = 0.61355643
Iteration 26, loss = 0.61428778
Iteration 27, loss = 0.61270826
Iteration 28, loss = 0.60856559
Iteration 29, loss = 0.60741477
Iteration 30, loss = 0.60910653
Iteration 31, loss = 0.61416164
Iteration 32, loss = 0.60898315
Iteration 33, loss = 0.60784165
Iteration 34, loss = 0.60729755
Iteration 35, loss = 0.60375124
Iteration 36, loss = 0.61030082
Iteration 37, loss = 0.60562185
Iteration 38, loss = 0.60759044
Iteration 39, loss = 0.60436762
Iteration 40, loss = 0.60589492
Iteration 41, loss = 0.60617809
Iteration 42, loss = 0.60281879
Iteration 43, loss = 0.60226572
Iteration 44, loss = 0.60609520
Iteration 45, loss = 0.60143431
Iteration 46, loss = 0.60923292
Iteration 47, loss = 0.60906616
Iteration 48, loss = 0.60687876
Iteration 49, loss = 0.59853041
Iteration 50, loss = 0.60682261
Iteration 51, loss = 0.60531194
Iteration 52, loss = 0.61065144
Iteration 53, loss = 0.60592570
Iteration 54, loss = 0.59797455
Iteration 55, loss = 0.59621826
Iteration 56, loss = 0.60673235
Iteration 57, loss = 0.60192166
Iteration 58, loss = 0.60074551
Iteration 59, loss = 0.60360057
Iteration 60, loss = 0.60240812
Iteration 61, loss = 0.60005926
Iteration 62, loss = 0.59986451
Iteration 63, loss = 0.60623371
Iteration 64, loss = 0.60330664
Iteration 65, loss = 0.60591050
Iteration 66, loss = 0.60364148
Iteration 67, loss = 0.60253664
Iteration 68, loss = 0.60775442
Iteration 69, loss = 0.60047950
Iteration 70, loss = 0.59774056
Iteration 71, loss = 0.60325850
Iteration 72, loss = 0.60225126
Iteration 73, loss = 0.60489652
Iteration 74, loss = 0.60361826
Iteration 75, loss = 0.60376679
Iteration 76, loss = 0.60538404
Iteration 77, loss = 0.59738704
Iteration 78, loss = 0.60437827
Iteration 79, loss = 0.60193411
Iteration 80, loss = 0.59835714
Iteration 81, loss = 0.60849853
Iteration 82, loss = 0.60724683
Iteration 83, loss = 0.60205753
Iteration 84, loss = 0.60211789
Iteration 85, loss = 0.60201635
Iteration 86, loss = 0.60047163
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 4/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(100, 100, 100);, score=0.546 total time= 3.7min
Iteration 1, loss = 2.51890615
Iteration 2, loss = 0.98603375
Iteration 3, loss = 0.82756523
Iteration 4, loss = 0.77287536
Iteration 5, loss = 0.72206023
Iteration 6, loss = 0.69983817

Iteration 7, loss = 0.67441186
Iteration 8, loss = 0.66414397
Iteration 9, loss = 0.65710743
Iteration 10, loss = 0.65842779
Iteration 11, loss = 0.64737673
Iteration 12, loss = 0.64228114
Iteration 13, loss = 0.63336207
Iteration 14, loss = 0.66300987
Iteration 15, loss = 0.67585730
Iteration 16, loss = 0.68610016
Iteration 17, loss = 0.68825531
Iteration 18, loss = 0.68914029
Iteration 19, loss = 0.68924003
Iteration 20, loss = 0.68903578
Iteration 21, loss = 0.68897617
Iteration 22, loss = 0.68900119
Iteration 23, loss = 0.68902803
Iteration 24, loss = 0.68901928
Iteration 25, loss = 0.68902720
Iteration 26, loss = 0.68896588
Iteration 27, loss = 0.68900744
Iteration 28, loss = 0.68903496
Iteration 29, loss = 0.68900411
Iteration 30, loss = 0.68906079
Iteration 31, loss = 0.68900995
Iteration 32, loss = 0.68900704
Iteration 33, loss = 0.68903292
Iteration 34, loss = 0.68964084
Iteration 35, loss = 0.68916330
Iteration 36, loss = 0.68900008
Iteration 37, loss = 0.68903335
Iteration 38, loss = 0.68900480
Iteration 39, loss = 0.68903184
Iteration 40, loss = 0.68892254
Iteration 41, loss = 0.68944496
Iteration 42, loss = 0.68897614
Iteration 43, loss = 0.68899816
Iteration 44, loss = 0.68898466
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
[CV 5/5] END estimator__activation=relu, estimator__hidden_layer_sizes=(100, 100, 100);, score=0.546 total time= 1.9min
Iteration 1, loss = 3.84012948
Iteration 2, loss = 1.38877425
Iteration 3, loss = 1.06320367
Iteration 4, loss = 0.99055704
Iteration 5, loss = 0.96986954
Iteration 6, loss = 0.91442890
Iteration 7, loss = 0.85388304
Iteration 8, loss = 1.06408892
Iteration 9, loss = 0.81431810
Iteration 10, loss = 0.87129570
Iteration 11, loss = 0.85586729
Iteration 12, loss = 0.85140128
Iteration 13, loss = 0.82993784
Iteration 14, loss = 0.85082773
Iteration 15, loss = 0.76668086
Iteration 16, loss = 0.83426936
Iteration 17, loss = 0.76985033
Iteration 18, loss = 0.75331716
Iteration 19, loss = 0.74298510
Iteration 20, loss = 0.80285104
Iteration 21, loss = 0.76565075
Iteration 22, loss = 0.71467470
Iteration 23, loss = 0.75076699
Iteration 24, loss = 0.72156879
Iteration 25, loss = 0.74677153
Iteration 26, loss = 0.73645213
Iteration 27, loss = 0.69747360
Iteration 28, loss = 0.71884128
Iteration 29, loss = 0.71608493
Iteration 30, loss = 0.70400731
Iteration 31, loss = 0.68531509
Iteration 32, loss = 0.71029622
Iteration 33, loss = 0.69031459
Iteration 34, loss = 0.69153341
Iteration 35, loss = 0.67691941
Iteration 36, loss = 0.67965086
Iteration 37, loss = 0.67606604
Iteration 38, loss = 0.66545404
Iteration 39, loss = 0.65347363
Iteration 40, loss = 0.67251968
Iteration 41, loss = 0.66360853
Iteration 42, loss = 0.66152336
Iteration 43, loss = 0.65788042
Iteration 44, loss = 0.66694192
Iteration 45, loss = 0.64007075
Iteration 46, loss = 0.65258398
Iteration 47, loss = 0.66892431
Iteration 48, loss = 0.64065295
Iteration 49, loss = 0.64743489
Iteration 50, loss = 0.65153306
Iteration 51, loss = 0.63708052
Iteration 52, loss = 0.63827654
Iteration 53, loss = 0.63776288
Iteration 54, loss = 0.63262558
Iteration 55, loss = 0.64290551
Iteration 56, loss = 0.63145419
Iteration 57, loss = 0.63746952
Iteration 58, loss = 0.63320233
Iteration 59, loss = 0.63021189
Iteration 60, loss = 0.62692344
Iteration 61, loss = 0.62973777
Iteration 62, loss = 0.62502540
Iteration 63, loss = 0.63101586
Iteration 64, loss = 0.62573315
Iteration 65, loss = 0.62078217
Iteration 66, loss = 0.61607184
Iteration 67, loss = 0.62237938
Iteration 68, loss = 0.61821211
Iteration 69, loss = 0.62331129
Iteration 70, loss = 0.62021594
Iteration 71, loss = 0.61704770
Iteration 72, loss = 0.61397800
Iteration 73, loss = 0.61368464
Iteration 74, loss = 0.61280288
Iteration 75, loss = 0.61415601
Iteration 76, loss = 0.61110534
Iteration 77, loss = 0.60943406
Iteration 78, loss = 0.61051847
Iteration 79, loss = 0.61307054
Iteration 80, loss = 0.60926079
Iteration 81, loss = 0.60738270
Iteration 82, loss = 0.60860352
Iteration 83, loss = 0.60838492
Iteration 84, loss = 0.60649489
Iteration 85, loss = 0.60497959
Iteration 86, loss = 0.60590431
Iteration 87, loss = 0.60546654
Iteration 88, loss = 0.58865295
Iteration 89, loss = 0.53990771
Iteration 90, loss = 0.46733866
Iteration 91, loss = 0.39552651
Iteration 92, loss = 0.35434855
Iteration 93, loss = 0.33333229
Iteration 94, loss = 0.31114246
Iteration 95, loss = 0.30123668
Iteration 96, loss = 0.30480123
Iteration 97, loss = 0.28617845
Iteration 98, loss = 0.28777684
Iteration 99, loss = 0.27162231
Iteration 100, loss = 0.28400683
Iteration 101, loss = 0.27502326
Iteration 102, loss = 0.26600412
Iteration 103, loss = 0.24734461
Iteration 104, loss = 0.23875301
Iteration 105, loss = 0.24333583
Iteration 106, loss = 0.22662775
Iteration 107, loss = 0.24033477
Iteration 108, loss = 0.21597389
Iteration 109, loss = 0.22099271
Iteration 110, loss = 0.22306890
Iteration 111, loss = 0.21074758
Iteration 112, loss = 0.21342443
Iteration 113, loss = 0.20540941
Iteration 114, loss = 0.21945584
Iteration 115, loss = 0.21482669
Iteration 116, loss = 0.19940064
Iteration 117, loss = 0.19891776
Iteration 118, loss = 0.20416512
Iteration 119, loss = 0.19859230
Iteration 120, loss = 0.20811871
Iteration 121, loss = 0.18640344
Iteration 122, loss = 0.19092678
Iteration 123, loss = 0.18633211
Iteration 124, loss = 0.19114084
Iteration 125, loss = 0.17035865
Iteration 126, loss = 0.17311068
Iteration 127, loss = 0.19851086
Iteration 128, loss = 0.16104711
Iteration 129, loss = 0.20317174
Iteration 130, loss = 0.17234795
Iteration 131, loss = 0.18227844
Iteration 132, loss = 0.16524693
Iteration 133, loss = 0.17776442
Iteration 134, loss = 0.21054621
Iteration 135, loss = 0.19075105
Iteration 136, loss = 0.18947475
Iteration 137, loss = 0.20310964
Iteration 138, loss = 0.18992576
Iteration 139, loss = 0.19390053
Iteration 140, loss = 0.18968401
Iteration 141, loss = 0.18822940
Iteration 142, loss = 0.18175799
Iteration 143, loss = 0.17355129
Iteration 144, loss = 0.17542610
Iteration 145, loss = 0.17138619
Iteration 146, loss = 0.17947512
Iteration 147, loss = 0.17887851
Iteration 148, loss = 0.18838769
Iteration 149, loss = 0.18637408
Iteration 150, loss = 0.17452017
Iteration 151, loss = 0.20753642
Iteration 152, loss = 0.17152171
Iteration 153, loss = 0.15391000
Iteration 154, loss = 0.17804545
Iteration 155, loss = 0.17398119
Iteration 156, loss = 0.16225940
Iteration 157, loss = 0.14838814
Iteration 158, loss = 0.14490340
Iteration 159, loss = 0.20552103
Iteration 160, loss = 0.17952955
Iteration 161, loss = 0.20332902
Iteration 162, loss = 0.19380861
Iteration 163, loss = 0.17432633
Iteration 164, loss = 0.18613407
Iteration 165, loss = 0.16948539
Iteration 166, loss = 0.16671544
Iteration 167, loss = 0.16263598
Iteration 168, loss = 0.17754288
Iteration 169, loss = 0.16916322
Iteration 170, loss = 0.15277636
Iteration 171, loss = 0.17923095
Iteration 172, loss = 0.17758874
Iteration 173, loss = 0.17713609
Iteration 174, loss = 0.15675793
Iteration 175, loss = 0.16134157
Iteration 176, loss = 0.15091190
Iteration 177, loss = 0.15748801
Iteration 178, loss = 0.17124671
Iteration 179, loss = 0.15639785
Iteration 180, loss = 0.16842218
Iteration 181, loss = 0.14333645
Iteration 182, loss = 0.16554019
Iteration 183, loss = 0.14728632
Iteration 184, loss = 0.17811045
Iteration 185, loss = 0.14448565
Iteration 186, loss = 0.14539315
Iteration 187, loss = 0.15062144
Iteration 188, loss = 0.13506252
Iteration 189, loss = 0.13978144
Iteration 190, loss = 0.17045782
Iteration 191, loss = 0.13946099
Iteration 192, loss = 0.13225145
Iteration 193, loss = 0.14937282
Iteration 194, loss = 0.13547333
Iteration 195, loss = 0.12957540
Iteration 196, loss = 0.15556488
Iteration 197, loss = 0.14838366
Iteration 198, loss = 0.27772794
Iteration 199, loss = 0.28675206
Iteration 200, loss = 0.26199619
Iteration 201, loss = 0.25457305
Iteration 202, loss = 0.24307932
Iteration 203, loss = 0.24619608
Iteration 204, loss = 0.23790626
Iteration 205, loss = 0.24373963
Iteration 206, loss = 0.23816397
Iteration 207, loss = 0.23592859
Iteration 208, loss = 0.24238539
Iteration 209, loss = 0.23734366

Iteration 210, loss = 0.24003557
Iteration 211, loss = 0.23560774
Iteration 212, loss = 0.23685809
Iteration 213, loss = 0.23793754
Iteration 214, loss = 0.23333509
Iteration 215, loss = 0.23408882
Iteration 216, loss = 0.23278329
Iteration 217, loss = 0.23485574
Iteration 218, loss = 0.23537018
Iteration 219, loss = 0.24004556
Iteration 220, loss = 0.23348219
Iteration 221, loss = 0.23376867
Iteration 222, loss = 0.23499233
Iteration 223, loss = 0.23462434
Iteration 224, loss = 0.23609507
Iteration 225, loss = 0.23164822
Iteration 226, loss = 0.24267135
Training loss did not improve more than tol=0.000100 for 30 consecutive epochs. Stopping.
Best parameter (CV score=0.899):
{'estimator__activation': 'relu', 'estimator__hidden_layer_sizes': (32, 32)}
CPU times: user 3h 4min 31s, sys: 58min 45s, total: 4h 3min 16s
Wall time: 1h 36min 16s
