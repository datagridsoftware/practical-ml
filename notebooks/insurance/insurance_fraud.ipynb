{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem definition and backgroud\n",
    "\n",
    "As defined by the Oxford dictionary fraud is \"wrongful or criminal deception intended to result in financial or personal gain\". Even if froud is not something new, the phenomenon grew in size with the increase of internet transactions. When we say transactions we are not referring strictly to the online payments, even if that is one of the most popular types of online frauds.\n",
    "\n",
    "For this example we are using a dataset that contains insurance claims, and each transaction includes the information if it was a fraudulent claim or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goals\n",
    "- read data\n",
    "- understand the data\n",
    "- prepare data \n",
    "- use Machine Learning algorithms to detect fraudulent claims\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary libraries to process the data\n",
    "\n",
    "Details for loading libraries and reading the data at [Import and read data details](\"C:\\work\\sources\\mlcourse\\notebooks\\insurance\\import_dataRead.md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay, roc_curve, auc\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option('display.max_columns', None) # by default the number of columns showed is trucated. This option will enabe \"show all\" feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from a location on the disk.\n",
    "#dataset source https://www.kaggle.com/datasets/incarnyx/car-insurance-fraud\n",
    "dataset = pd.read_excel(\"car_insurance_fraud.xlsx\")\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration\n",
    "The purpose of this step is to understand the data, the sstructure of it, in order to be able to modify it later to fi the purpose of the experiment. A lot of insights related to the data can be obtain here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The describe function gives an insight about the data by generating statistics on the numeric columns, like cout , average, min , max, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical features in the dataset\n",
    "dataset.describe(exclude=\"object\").transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#non numerical features\n",
    "dataset.describe(exclude=\"number\").transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The info() function outputs information about the structure of the data, and the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the histogram for each numerical feature in a separate subplot\n",
    "def createHistogramPlot(data):\n",
    "    data.hist(bins=25, figsize=(30, 25), layout=(-1, 3))\n",
    "    plt.tight_layout()     \n",
    "createHistogramPlot(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset.plot(lw=0, marker=\"*\", subplots=True, layout=(-1, 2),markersize=0.6, figsize=(15, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered= dataset.copy()\n",
    "dataset_filtered[['Age','DriverRating','RepNumber', 'Sex']].boxplot(figsize=(15, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add charts and stats\n",
    "hue_feature = 'FraudFound_P'\n",
    "plot_values = 'Age'\n",
    "g = sns.FacetGrid(dataset, hue=hue_feature, height = 7, aspect = 2)\n",
    "g.map(sns.kdeplot, plot_values)\n",
    "plt.title(plot_values +' distribution for fraud and no fraud claims')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add charts and stats\n",
    "hue_feature = 'FraudFound_P'\n",
    "plot_values = 'ClaimSize'\n",
    "g = sns.FacetGrid(dataset, hue=hue_feature, height = 7, aspect = 2)\n",
    "g.map(sns.kdeplot, plot_values)\n",
    "plt.title(plot_values +' distribution for fraud and no fraud claims')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "- resolve issues with the data\n",
    "- prepare data for model building\n",
    "\n",
    "More details [Here](here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Search for null values in the data\n",
    "dataset_nulls = dataset.copy()\n",
    "null_values = dataset_nulls.isnull().sum()\n",
    "print(\"Missing values in dataset:\")\n",
    "for name, value in null_values.iteritems():\n",
    "    if value > 0:\n",
    "        print(name, value)\n",
    "display(dataset_nulls.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interpolation\n",
    "\n",
    "dataset_interpolation = dataset_nulls.copy()\n",
    "age_index = dataset_nulls[dataset_nulls['Age'].isnull()].index.tolist()\n",
    "driver_index = dataset_nulls[dataset_nulls['DriverRating'].isnull()].index.tolist()\n",
    "#Linear\n",
    "ds_linear = dataset_interpolation.interpolate(method=\"linear\")\n",
    "#polynomial\n",
    "ds_poly = dataset_interpolation.interpolate(method=\"polynomial\", order=2)\n",
    "#padding\n",
    "ds_padding =dataset_interpolation.interpolate(method=\"pad\", limit=3)\n",
    "\n",
    "print(\"Age replacements\")\n",
    "for position in age_index:\n",
    "    print(f\"Original Value: {dataset_nulls.iloc[position]['Age']}  Linear: {ds_linear.iloc[position]['Age']} Poly: {ds_poly.iloc[position]['Age']} Padding: {ds_padding.iloc[position]['Age']}\" )\n",
    "\n",
    "print(\"Driver Rating replacements\")\n",
    "for position in driver_index:\n",
    "    print(f\"Original Value: {dataset_nulls.iloc[position]['DriverRating']}  Linear: {ds_linear.iloc[position]['DriverRating']} Poly: {ds_poly.iloc[position]['DriverRating']} Padding: {ds_padding.iloc[position]['DriverRating']}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputation\n",
    "\n",
    "dataframe_inputation = dataset_nulls.copy()\n",
    "age_index = dataset_nulls[dataset_nulls['Age'].isnull()].index.tolist()\n",
    "driver_index = dataset_nulls[dataset_nulls['DriverRating'].isnull()].index.tolist()\n",
    "\n",
    "#Mode\n",
    "ds_mode = dataframe_inputation.copy()\n",
    "ds_mode['Age'].fillna(ds_mode['Age'].mode()[0], inplace=True)\n",
    "ds_mode['DriverRating'].fillna(ds_mode['DriverRating'].mode()[0], inplace=True)\n",
    "\n",
    "#Median\n",
    "ds_median = dataframe_inputation.copy()\n",
    "ds_median['Age'].fillna(ds_median['Age'].median(), inplace=True)\n",
    "ds_median['DriverRating'].fillna(ds_median['DriverRating'].median(), inplace=True)\n",
    "\n",
    "#Mean\n",
    "ds_mean = dataframe_inputation.copy()\n",
    "ds_mean['Age'].fillna(ds_mean['Age'].mean(), inplace=True)\n",
    "ds_mean['DriverRating'].fillna(ds_mean['DriverRating'].mean(), inplace=True)\n",
    "\n",
    "print(\"Age replacements\")\n",
    "for position in age_index:\n",
    "    print(f\"Original Value: {dataset_nulls.iloc[position]['Age']}  Mode: {ds_mode.iloc[position]['Age']} Median: {ds_median.iloc[position]['Age']} Mean: {ds_mean.iloc[position]['Age']}\" )\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"Driver Rating replacements\")\n",
    "for position in driver_index:\n",
    "    print(f\"Original Value: {dataset_nulls.iloc[position]['DriverRating']}  Mode: {ds_mode.iloc[position]['DriverRating']} Median: {ds_median.iloc[position]['DriverRating']} Mean: {ds_mean.iloc[position]['DriverRating']}\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scikit interpolation\n",
    "dataframe_scikit_imputation = dataset_nulls.copy()\n",
    "age_index = dataset_nulls[dataset_nulls['Age'].isnull()].index.tolist()\n",
    "driver_index = dataset_nulls[dataset_nulls['DriverRating'].isnull()].index.tolist()\n",
    "\n",
    "imputer_simple = SimpleImputer(missing_values=np.NaN,strategy='most_frequent') #  possible values for strategy are mean, media, most_frequent and constant\n",
    "simple_imputer_arr = imputer_simple.fit_transform(dataframe_scikit_imputation)\n",
    "\n",
    "imputer_knn = KNNImputer(n_neighbors=2)\n",
    "knn_arr = imputer_knn.fit_transform(dataframe_scikit_imputation[['Age', 'DriverRating']])\n",
    "\n",
    "ds_simple_imputer = pd.DataFrame(simple_imputer_arr, columns=dataframe_scikit_imputation.columns)\n",
    "ds_knn = pd.DataFrame(knn_arr, columns=['Age','DriverRating'])\n",
    "\n",
    "print(\"Age replacements\")\n",
    "for position in age_index:\n",
    "    print(f\"Original Value: {dataframe_scikit_imputation.iloc[position]['Age']}  Simple Imputer: {ds_simple_imputer.iloc[position]['Age']} KNN: {ds_knn.iloc[position]['Age']}\" )\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"Driver Rating replacements\")\n",
    "for position in driver_index:\n",
    "    print(f\"Original Value: {dataframe_scikit_imputation.iloc[position]['DriverRating']}  Simple Imputer: {ds_simple_imputer.iloc[position]['DriverRating']} KNN: {ds_knn.iloc[position]['DriverRating']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping records that have null values\n",
    "dataset_nulls.dropna(inplace=True)\n",
    "dataset_nulls.isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List number of uniques values for each columns\n",
    "dataset_nulls.nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list possible values for MonthClaimed\n",
    "dataset_unique = dataset_nulls.copy()\n",
    "feature = \"MonthClaimed\"\n",
    "display(\"Posible values for feature field: \" + feature)\n",
    "display(dataset_unique[feature].unique())\n",
    "display((dataset_unique[dataset_unique[feature] == 0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the record where monthClimend is equal to 0\n",
    "dataset_unique.drop((dataset_unique[dataset_unique[feature] == 0]).index, inplace=True)\n",
    "display(f\"Number of uniques values for field {feature} is {(dataset_unique[feature]).nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the dataset for  exploratory data anslisys and model building\n",
    "moths_array = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep','Oct', 'Nov', 'Dec']\n",
    "week_array =['Monday','Tuesday','Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "number_claimed = ['none', '1', '2 to 4', 'more than 4']\n",
    "\n",
    "transformed_data = dataset_unique.copy()\n",
    "transformed_data['Month'].replace(moths_array, [1,2,3,4,5,6,7,8,9,10,11,12], inplace=True)\n",
    "transformed_data['MonthClaimed'].replace(moths_array, [1,2,3,4,5,6,7,8,9,10,11,12], inplace=True)\n",
    "transformed_data['DayOfWeek'].replace(week_array, [1,2,3,4,5,6,7], inplace=True)\n",
    "transformed_data['DayOfWeekClaimed'].replace(week_array, [1,2,3,4,5,6,7], inplace=True)\n",
    "transformed_data['PastNumberOfClaims'].replace(number_claimed, [0,1,2,3], inplace=True)\n",
    "transformed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Label Encoder to convert all non-numerical values\n",
    "dataset_label_encoded= transformed_data.copy()\n",
    "def createColumnLE(data_column, xdata):\n",
    "    print(\"conveting column \" + data_column)\n",
    "    xdata[data_column] = labelEncoder.fit_transform(xdata[data_column])\n",
    "\n",
    "labelEncoder = LabelEncoder()\n",
    "catergorical_columns = dataset_label_encoded.columns[dataset_label_encoded.dtypes == object]\n",
    "for col in catergorical_columns:\n",
    "   createColumnLE(col, dataset_label_encoded)\n",
    "\n",
    "\n",
    "dataset_label_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createHistogramPlot(dataset_label_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_corelation = dataset_label_encoded.corr(method=\"pearson\")\n",
    "display(feature_corelation.head())\n",
    "indicatives = np.where(np.abs(feature_corelation)>0.75, \"P\",\n",
    "                  np.where(np.abs(feature_corelation)>0.5, \"M\",\n",
    "                           np.where(np.abs(feature_corelation)>0.25, \"s\", \"\")))\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(feature_corelation, mask=np.eye(len(feature_corelation)), square=True,\n",
    "            center=0, fmt='',annot=indicatives, linewidths=.5,\n",
    "            cmap=\"vlag\", cbar_kws={\"shrink\": 0.8});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dds = transformed_data.copy()\n",
    "def create_dummy_dataframe(df):\n",
    "    temp_dataframe = pd.DataFrame()\n",
    "    non_numeric_columns = df.columns[df.dtypes == object]\n",
    "    for col in non_numeric_columns:\n",
    "        column_dataframe = pd.get_dummies(df[col], drop_first = True)\n",
    "        column_dataframe.columns = [str(col) + ': ' + str(name) for name in column_dataframe.columns]\n",
    "        temp_dataframe = pd.concat([temp_dataframe, column_dataframe], axis = 1)\n",
    "    aggregate_dataframe = pd.concat([df.drop(columns = non_numeric_columns), temp_dataframe], axis = 1)\n",
    "    return aggregate_dataframe\n",
    "\n",
    "dummy_dataframe = create_dummy_dataframe(dds)\n",
    "dummy_dataframe.info()\n",
    "dummy_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Various information about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_mms = dummy_dataframe.copy()\n",
    "#Scalers\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, NMF, KernelPCA\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler, RobustScaler\n",
    "scaller_df = pca_mms.drop('FraudFound_P', axis = 1)\n",
    "scaller_y_v = pca_mms['FraudFound_P'].values\n",
    "\n",
    "columns = scaller_df.columns\n",
    "minmax_scaler = MinMaxScaler()\n",
    "minmax_X_sc = minmax_scaler.fit_transform(scaller_df)\n",
    "minmax_df= pd.DataFrame(minmax_X_sc, columns=columns)\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "standard_X_sc = standard_scaler.fit_transform(scaller_df)\n",
    "standard_df= pd.DataFrame(standard_X_sc, columns=columns)\n",
    "\n",
    "robust_scaler = RobustScaler()\n",
    "robust_X_sc = robust_scaler.fit_transform(scaller_df)\n",
    "robust_df= pd.DataFrame(robust_X_sc, columns=columns)\n",
    "\n",
    "fig, (default, minmax, standard, robust) = plt.subplots(ncols = 4, figsize =(20, 5))\n",
    "\n",
    " \n",
    "sns.kdeplot(dataset_label_encoded[\"ClaimSize\"], ax = default, color='red')\n",
    "\n",
    "sns.kdeplot(robust_df['ClaimSize'], ax = minmax, color ='green')\n",
    "minmax.set_title('MinMax Scaller')\n",
    " \n",
    "sns.kdeplot(robust_df['ClaimSize'], ax = standard, color ='blue')\n",
    "standard.set_title('Standard Scaller')\n",
    " \n",
    "sns.kdeplot(standard_df['ClaimSize'], ax = robust, color ='black')\n",
    "robust.set_title('Robust Scaller')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, random_state = 1)\n",
    "df_pca = pca.fit_transform(minmax_X_sc)\n",
    "\n",
    "df_vis = pd.DataFrame(df_pca)\n",
    "df_vis['y'] = scaller_y_v\n",
    "plt.figure(figsize = (12, 8))\n",
    "sns.scatterplot(data = df_vis, x = 0, y = 1, hue = 'y')\n",
    "plt.show()\n",
    "df_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_building = minmax_df.copy()\n",
    "model_building['FraudFound_P']=scaller_y_v\n",
    "\n",
    "mb = model_building.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Split data \n",
    "training_data, validation_data  = train_test_split(mb,\n",
    "                                test_size=0.3,\n",
    "                                random_state = 101)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data.drop('FraudFound_P', axis = 1), \n",
    "                                                    training_data['FraudFound_P'], \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=101)\n",
    "\n",
    "                                                    \n",
    "print(f\"Number of examples used for training : {training_data.shape[0]}\")\n",
    "print(f\"Number of examples used for validation: {validation_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model building, metrics\n",
    "lgmodel = LogisticRegression(class_weight=\"balanced\",\n",
    "    n_jobs = -1,\n",
    "    random_state = 101)\n",
    "lgmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame(X_train.columns, columns = [\"feature\"])\n",
    "feature_importance[\"importance\"] = lgmodel.coef_[0]\n",
    "feature_importance = feature_importance.sort_values(by = [\"importance\"], ascending=False)\n",
    " \n",
    "plt.figure(figsize=(15, 25))\n",
    "sns.barplot(x=feature_importance.importance, y=feature_importance.feature,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputMetrics(model, prediction, test_y):\n",
    "        print(classification_report(test_y, prediction, target_names = ['Not Fraud', 'Fraud']))\n",
    "        display(pd.DataFrame(confusion_matrix(test_y, prediction), \n",
    "                         columns = ['Predicted Not Fraud', 'Predicted Fraud'],\n",
    "                         index = ['Not Fraud', 'Fraud']))\n",
    "        ConfusionMatrixDisplay.from_predictions(y_test, prediction, labels=[0, 1])\n",
    "        RocCurveDisplay.from_predictions(y_test,prediction)\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            featureI = pd.DataFrame({\n",
    "                'Variable'  :X_test.columns,\n",
    "                'Importance':model.feature_importances_\n",
    "            })\n",
    "            featureI.sort_values('Importance', ascending=False, inplace=True)\n",
    "            display(featureI.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions using the testing set\n",
    "prediction = lgmodel.predict(X_test)\n",
    "outputMetrics(lgmodel, prediction, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(random_state = 101)\n",
    "dtc.fit(X_train, y_train)\n",
    "prediction = dtc.predict(X_test)\n",
    "outputMetrics(dtc,prediction, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbr = XGBClassifier(\n",
    "    random_state = 1,\n",
    "    n_jobs = -1,\n",
    "    scale_pos_weight = 20,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric = 'logloss'\n",
    ")\n",
    "xgbr.fit(X_train, y_train)\n",
    "prediction = xgbr.predict(X_test)\n",
    "\n",
    "outputMetrics(xgbr, prediction, y_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "247e2044f4c49c9d914ae755d95f59c52d7c3d488cb85bbe2434f1ff03419f6d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
