{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer Segmentation\n",
    "This notebook uses publicly dataset from marketing campain to explore customer segmentation through unsupervised learning methods. The customer segmentation is the practice of separating customers into groups that reflect similarities among customers in each cluster.\n",
    "\n",
    "\n",
    "### Concept\n",
    "Customer segmentation is the problem of uncovering information about a firm's customer base, based on their interactions with the business. In most cases this interaction is in terms of their purchase behavior and patterns. \n",
    "\n",
    "Customer segmentation is similarly the process of dividing an organization’s customer bases into different sections or segments based on various customer attributes. \n",
    "\n",
    "The process of customer segmentation is based on the premise of finding differences among the customers’ behavior and patterns.\n",
    "\n",
    "The major objectives and benefits behind the motivation for customer segmentation are:\n",
    " - **Higher Revenue**: This is the most obvious requirement of any customer segmentation project.\n",
    " - **Customer Understanding**: One of the mostly widely accepted business paradigms is “know your customer” and a segmentation of the customer base allows for a perfect dissection of this paradigm.\n",
    " - **Target Marketing**: The most visible reason for customer segmentation is the ability to focus marketing efforts effectively and efficiently. If a firm knows the different segments of its customer base, it can devise better marketing campaigns which are tailor made for the segment. A good segmentation model allows for better understanding of customer requirements and hence increases the chances of the success of any marketing campaign developed by the organization.\n",
    " - **Optimal Product Placement**: A good customer segmentation strategy can also help the firm with developing or offering new products, or a bundle of products together as a combined offering.\n",
    "    Finding Latent Customer Segments: Finding out which segment of customers it might be missing to identifying untapped customer segments by focused on marketing campaigns or new business development.\n",
    "\n",
    "### Clustering\n",
    "\n",
    "The most obvious method to perform customer segmentation is using unsupervised Machine Learning methods like clustering. The method is as simple as collecting as much data about the customers as possible in the form of features or attributes and then finding out the different clusters that can be obtained from that data. Finally, we can find traits of customer segments by analyzing the characteristics of the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts of this notebook have been inspired by:\n",
    "\n",
    " - https://www.kaggle.com/code/karnikakapoor/customer-segmentation-clustering/notebook\n",
    " - https://www.kaggle.com/code/paulinan/bank-customer-segmentation\n",
    " - https://www.kaggle.com/code/mgmarques/customer-segmentation-and-market-basket-analysis\n",
    " - https://www.kaggle.com/code/kushal1996/customer-segmentation-k-means-analysis/notebook\n",
    " - https://www.kaggle.com/code/vjchoudhary7/kmeans-clustering-in-customer-segmentation\n",
    " - https://thecleverprogrammer.com/2021/02/08/customer-personality-analysis-with-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "- explore data\n",
    "- cleanup data\n",
    "- select & engineer features\n",
    "- handle outliers\n",
    "- train model\n",
    "- evaluate model\n",
    "- back to explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action = 'ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Source: \n",
    "- https://www.kaggle.com/code/karnikakapoor/customer-segmentation-clustering/data\n",
    "- https://www.kaggle.com/code/karnikakapoor/customer-segmentation-clustering/notebook\n",
    "- https://thecleverprogrammer.com/2021/02/08/customer-personality-analysis-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"marketing_campaign.csv\", sep=\"\\t\")\n",
    "print(\"Number of datapoints:\", data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    " \n",
    "| Column name |    Details | \n",
    "| ------------------- | ----------- |\n",
    "| ID                  | Customer’s unique identifier |\n",
    "| Year_Birth          | Customer's birth year |\n",
    "| Education           | Education Qualification of customer |\n",
    "| Marital_Status      | Marital Status of customer |\n",
    "| Income              | Customer's yearly household income |\n",
    "| Kidhome             | Number of children in customer's household |\n",
    "| Teenhome            | Number of teenagers in customer's household |\n",
    "| Dt_Customer         | Date of customer's enrollment with the company |\n",
    "| Recency             | Number of days since customer's last purchase |\n",
    "| MntWines            | Amount spent on wine |\n",
    "| MntFruits           | Amount spent on fruits |\n",
    "| MntMeatProducts     | Amount spent on meat |\n",
    "| MntFishProducts     | Amount spent on fish |\n",
    "| MntSweetProducts    | Amount spent on sweet products |\n",
    "| MntGoldProds        | Amount spent on gold products |\n",
    "| NumDealsPurchases   | Number of purchase |\n",
    "| NumWebPurchases     | Number of web purchase |\n",
    "| NumCatalogPurchases | Number of catalog purchase |\n",
    "| NumStorePurchases   | Number of store purchase |\n",
    "| NumWebVisitsMonth   | Number of web site visits per month |\n",
    "| AcceptedCmp3        | Accepted marketing campain 3 |\n",
    "| AcceptedCmp4        | Accepted marketing campain 4 |\n",
    "| AcceptedCmp5        | Accepted marketing campain 5 |\n",
    "| AcceptedCmp1        | Accepted marketing campain 1 |\n",
    "| AcceptedCmp2        | Accepted marketing campain 2 |\n",
    "| Complain            | Complained |\n",
    "| Z_CostContact       |  |\n",
    "| Z_Revenue           |  |\n",
    "| Response            |  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below was created to simplify the analysis of general characteristics of the data. Inspired on the str function of R, this function returns the types, counts, distinct, count nulls, min, max, missing ratio and uniques values of each field/feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_details(df:pd.DataFrame): \n",
    "    obs = df.shape[0]\n",
    "    types = df.dtypes\n",
    "    \n",
    "    counts = df.count()\n",
    "    uniques = df.apply(lambda x: x.unique())\n",
    "    nulls =  df.isnull().sum()\n",
    "    min = df.min()\n",
    "    max = df.max()\n",
    "    \n",
    "    distincts = df.nunique()\n",
    "    missing_ratio = (df.isnull().sum()/ obs) * 100\n",
    "    skewness = df.skew(skipna = True)\n",
    "    kurtosis = df.kurt(skipna = True)\n",
    "    print('Data shape:', df.shape)\n",
    "    \n",
    "    cols = ['types', 'counts', 'distincts', 'nulls', 'missing ratio', 'uniques', 'skewness', 'kurtosis', 'min', 'max']\n",
    "    df_res = pd.concat([types, counts, distincts, nulls, missing_ratio, uniques, skewness, kurtosis, min, max], axis = 1, sort=True)\n",
    "    df_res.columns = cols\n",
    "    dtypes = df_res.types.value_counts()\n",
    "    print('___________________________\\nData types:\\n',df_res.types.value_counts())\n",
    "    print('___________________________')\n",
    "    return df_res\n",
    "\n",
    "details = display_details(data)\n",
    "display(details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    " - `Income` field has missing values\n",
    " - `Marital_Status` and `Education` fiels are identified as string columns but we need to transform them to numeric values in order to use them\n",
    " - `Dt_Customer` is identified as string columns instead date. We need to transform it to date format and than to a numeric value\n",
    " \n",
    "**Note**: string fields are specified in pandas dataframe as having type `object`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing values\n",
    "\n",
    "The process of replacing missing data with substituted values is called Imputation. \n",
    "See more on https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n",
    "\n",
    "Options:\n",
    " 1. drop rows having missing data - `data = data.dropna()`\n",
    " 1. replace missing Income value with a fixed value - SKLearn encoders\n",
    " 1. replace missing Income value with mean value of our data - SKLearn encoders\n",
    " 1. replace missing Income with a more complex computed using autoencoders - see https://curiousily.com/posts/data-imputation-using-autoencoders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# use median for imputation\n",
    "incomeImputer = SimpleImputer(strategy='median').fit(data[['Income']])\n",
    "print(\"Values used to fill missing values: \", incomeImputer.statistics_)\n",
    "\n",
    "data[['Income']] = incomeImputer.transform(data[['Income']])\n",
    "# Let's print again the info about our dataset\n",
    "# Notice 'nulls' collumns for Income field\n",
    "#details = display_details(data)\n",
    "#display(details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "- Create a feature (\"Customer_For\") of the number of days the customers started to shop in the store relative to the last recorded date\n",
    "- Encode `Marital_Status` and `Education` into numeric values\n",
    "- Extract `Age` information of the customer from `Year_Birth`\n",
    "- Extract total spent in the last two years\n",
    "- Extract the total number of purchases in last two years\n",
    "- Extract the average spent per purchase\n",
    "- Derive `Familly_size` base on the number of `Kidhome` + `Teenhome` and `Marital_Status`\n",
    "- Combine `AcceptedCmpXXX` in a single categorical field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat ID field as string since this represents the customer ID not a numeric value \n",
    "data = data.astype({\"ID\": str}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Dt_Customer\"] = pd.to_datetime(data[\"Dt_Customer\"]).apply(lambda d: d.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newest = data[\"Dt_Customer\"].max()\n",
    "data[\"Customer_For\"] = (newest - data['Dt_Customer']).dt.days\n",
    "data[['Dt_Customer', \"Customer_For\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "encoder = OrdinalEncoder().fit(data[['Marital_Status', 'Education']])\n",
    "display(encoder.categories_)\n",
    "\n",
    "data[['Marital_Status', 'Education']] = encoder.transform(data[['Marital_Status', 'Education']])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Age'] = 2021 - data['Year_Birth']\n",
    "data[\"Spent\"] = data[\"MntWines\"] + data[\"MntFruits\"] + data[\"MntMeatProducts\"] + data[\"MntFishProducts\"] + data[\"MntSweetProducts\"] + data[\"MntGoldProds\"]\n",
    "data[\"Purchases\"] = data['NumDealsPurchases'] + data['NumWebPurchases'] + data['NumCatalogPurchases']+data['NumStorePurchases']\n",
    "data[\"AvgSpentPPurchase\"] = data[\"Spent\"]/data[\"Purchases\"]\n",
    "data[\"Family_Size\"]=data[\"Marital_Status\"].replace({\"Married\":2, \"Together\":2, \"Alone\":1, \"Absurd\":1, \"Widow\":1, \"YOLO\":1, \"Divorced\":1, \"Single\":1}) + data[\"Kidhome\"]+data[\"Teenhome\"]\n",
    "data[\"Campaigns\"]=data[\"AcceptedCmp5\"] + data[\"AcceptedCmp4\"] + data[\"AcceptedCmp3\"] + data[\"AcceptedCmp2\"] + data[\"AcceptedCmp1\"]\n",
    "data[\"Children\"]=data[\"Kidhome\"] + data[\"Teenhome\"]\n",
    "data[\"Is_Parent\"] = np.where(data.Children> 0, 1, 0)\n",
    "data[\"Living_With\"]=data[\"Marital_Status\"].replace({\"Married\":\"Partner\", \"Together\":\"Partner\", \"Absurd\":\"Alone\", \"Widow\":\"Alone\", \"YOLO\":\"Alone\", \"Divorced\":\"Alone\", \"Single\":\"Alone\",})\n",
    "\n",
    "display(data.head())\n",
    "display(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vizualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [ \"Income\", \"Recency\", \"Customer_For\", \"Age\", \"Spent\", \"Family_Size\", \"Purchases\"]\n",
    "\n",
    "plt.figure(figsize=(15, 25))\n",
    "g = sns.pairplot(data[features], hue= \"Family_Size\", corner=True, palette = sns.color_palette(\"crest\", 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Outliers\n",
    "\n",
    "Notice some outliers in `Income` and `Age` fields \n",
    "Let's remove these values and replot the fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = data[data[\"Age\"] < 90].copy()\n",
    "filtered_data = filtered_data[filtered_data[\"Income\"] < 600000]\n",
    "filtered_data = filtered_data[(filtered_data[\"Purchases\"]!=0)]\n",
    "\n",
    "print(\"Number of records removed:\", len(data) - len(filtered_data))\n",
    "\n",
    "#g = sns.pairplot(filtered_data[Features], hue= \"Familly_size\", corner=True)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vizualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25, 7))\n",
    "f1 = fig.add_subplot(121)\n",
    "\n",
    "ds_grouped = filtered_data.groupby([\"Family_Size\"]).Purchases.sum().sort_values(ascending = False)\n",
    "ds_grouped.plot(kind='bar', title='Total number of purchases based on familly size')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25, 7))\n",
    "gb_ID_data = filtered_data.groupby([\"ID\"])\n",
    "PercentSales = np.round((gb_ID_data.Spent.sum().sort_values(ascending = False)[:30].sum()/gb_ID_data.Spent.sum().sort_values(ascending = False).sum()) * 100, 2)\n",
    "\n",
    "gb_ID_data.Spent.sum().sort_values(ascending = False)[:30].plot(kind='bar', title='Top 30 Customers: {:3.2f}% Sales Amount'.format(PercentSales))\n",
    "\n",
    "############################################\n",
    "fig = plt.figure(figsize=(25, 7))\n",
    "\n",
    "f1 = fig.add_subplot(121)\n",
    "PercentSales =  np.round((gb_ID_data.Spent.sum().sort_values(ascending = False)[:10].sum()/gb_ID_data.Spent.sum().sort_values(ascending = False).sum()) * 100, 2)\n",
    "gb_ID_data.Spent.sum().sort_values(ascending = False)[:10].plot(kind='bar', title='Top 10 Customers: {:3.2f}% Sales Amont'.format(PercentSales))\n",
    "\n",
    "f1 = fig.add_subplot(122)\n",
    "PercentSales =  np.round((gb_ID_data.Purchases.sum().sort_values(ascending = False)[:10].sum()/gb_ID_data.Purchases.sum().sort_values(ascending = False).sum()) * 100, 2)\n",
    "gb_ID_data.Purchases.sum().sort_values(ascending = False)[:10].plot(kind='bar', title='Top 10 Customers: {:3.2f}% Event Sales'.format(PercentSales))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "Create a dataset with  fields that  are interesting for our experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_fields = [\n",
    " 'ID',\n",
    " 'Income',\n",
    " 'Recency',\n",
    " 'Customer_For',\n",
    " 'Marital_Status',\n",
    " 'Education',\n",
    " 'Age',\n",
    " 'Spent',\n",
    " 'Purchases',\n",
    " 'AvgSpentPPurchase',\n",
    " 'Family_Size',\n",
    " 'Campaigns',\n",
    " 'Kidhome',\n",
    " 'Teenhome',\n",
    " 'Children',\n",
    " 'Is_Parent',\n",
    " 'Living_With'\n",
    "]\n",
    " \n",
    "df_base = filtered_data[keep_fields].copy()\n",
    "df_base.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation matrix\n",
    "corrmat= df_base.corr()\n",
    "plt.figure(figsize=(20,20))  \n",
    "cmap = sns.color_palette(\"ch:start=.2,rot=-.3\", as_cmap=True)\n",
    "display(type(cmap))\n",
    "sns.heatmap(corrmat,annot=True, cmap=cmap, center=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFM Model for Customer Value\n",
    "\n",
    "The RFM(*Recency, Frequency and Monetary Value) model will take the transactions of a customer and analyse important informational attributes about each customer:\n",
    "\n",
    " - Recency: The value of how recently a customer purchased in campain\n",
    " - Frequency: How frequent the customer’s transactions are in campan\n",
    " - Monetary value: The ammount value of all that the customer purchases made in campain\n",
    "\n",
    "We will plot the Recency Distribution and QQ-plot to identify substantive departures from normality, likes outliers, skewness and kurtosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew, norm, probplot, boxcox\n",
    "\n",
    "def QQ_plot(data, measure):\n",
    "    fig = plt.figure(figsize=(20,7))\n",
    "\n",
    "    #Get the fitted parameters used by the function\n",
    "    (mu, sigma) = norm.fit(data)\n",
    "\n",
    "    #Kernel Density plot\n",
    "    fig1 = fig.add_subplot(121)\n",
    "    sns.distplot(data, fit=norm)\n",
    "    fig1.set_title(measure + ' Distribution ( mu = {:.2f} and sigma = {:.2f} )'.format(mu, sigma), loc='center')\n",
    "    fig1.set_xlabel(measure)\n",
    "    fig1.set_ylabel('Frequency')\n",
    "\n",
    "    #QQ plot\n",
    "    fig2 = fig.add_subplot(122)\n",
    "    res = probplot(data, plot=fig2)\n",
    "    fig2.set_title(measure + ' Probability Plot (skewness: {:.6f} and kurtosis: {:.6f} )'.format(data.skew(), data.kurt()), loc='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_base[['ID','Recency']].reset_index().describe().transpose())\n",
    "QQ_plot(df_base.Recency, 'Recency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QQ_plot(df_base.Purchases, 'Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QQ_plot(df_base.Spent, 'Amount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "From the first graph above we can see that sales recency distribution is not **skewed**, and has no long tail. \n",
    "\n",
    "From the Probability Plot, we could see that sales recency also partially aligns with the diagonal red line which represent normal distribution.\n",
    "\n",
    "With a **low negative skewness** of -0.004299, we confirm the symmetry of our sales recency. The skewness for a normal distribution is zero, and any symmetric data should have a skewness near zero.\n",
    "A distribution, or data set, is symmetric if it looks the same to the left and right of the center point.\n",
    "\n",
    "**Kurtosis** is a measure of whether the data is heavy-tailed or light-tailed relative to a normal distribution. \n",
    "That is, data sets with high kurtosis tend to have heavy tails, or outliers, and positive kurtosis indicates a heavy-tailed distribution and negative kurtosis indicates a light tailed distribution. \n",
    "So, with **1.2 of negative kurtosis** sales recency are not heavy-tailed and does not have outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data Scaling\n",
    "\n",
    "We will be using the K-means clustering algorithm. One of the requirements for proper functioning of the algorithm is the mean centering of the variable values. Centering of a variable value means that we will replace the actual value of the variable with a standardized value, so that the variable has a mean of 0 and variance of 1. This ensures that all the variables are in the same range and the difference in ranges of values doesn't cause the algorithm to not perform well. This is akin to feature scaling.\n",
    "\n",
    "Another problem that you can investigate about is the range of values each variable can take. This problem is particularly noticeable for the monetary amount variable. To take care of this problem, we transform all the variables on the log scale. This transformation, along with the standardization, will ensure that the input to our algorithm is a homogenous set of scaled and transformed values.\n",
    "\n",
    "An important point about the data preprocessing step is that we need it to be reversible. In our case, we will have the clustering results in terms of the log transformed and scaled variable. But to make inferences in terms of the original data, we will need to reverse transform all the variable so that we get back the actual RFM figures. This can be done by using the preprocessing capabilities of Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(df_base)\n",
    "df_scaled = pd.DataFrame(scaler.transform(df_base), columns=df_base.columns )\n",
    "print(\"All features are now scaled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "df_subset = df_base[['Spent', 'Recency', 'Purchases']].copy()\n",
    "df_subset.head()\n",
    "\n",
    "df_subset['Purchases'] = df_subset['Purchases'].apply(math.log)\n",
    "df_subset['Spent'] = df_subset['Spent'].apply(math.log)\n",
    "scaler = StandardScaler().fit(df_subset)\n",
    "arr_subset_scaled = scaler.transform(df_subset)\n",
    "df_subset = pd.DataFrame(arr_subset_scaled, columns=df_subset.columns)\n",
    "\n",
    "display(df_subset.describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "ax1 = fig.add_subplot(121); sns.regplot(x='Recency', y='Spent', data=df_subset)\n",
    "ax1.title.set_text('Recency Log')\n",
    "\n",
    "ax2 = fig.add_subplot(122); sns.regplot(x='Recency', y='Spent', data=df_base)\n",
    "ax2.title.set_text('Recency Base')\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax3 = fig.add_subplot(121); sns.regplot(x='Purchases', y='Spent', data=df_subset)\n",
    "ax3.title.set_text('Frequency Log')\n",
    "\n",
    "ax4 = fig.add_subplot(122); sns.regplot(x='Purchases', y='Spent', data=df_base)\n",
    "ax4.title.set_text('Frequency Raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install PyQt5\n",
    "#%matplotlib qt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3D view \n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "xs = df_subset.Recency\n",
    "ys = df_subset.Purchases\n",
    "zs = df_subset.Spent\n",
    "ax.scatter(xs, ys, zs, s=5)\n",
    "\n",
    "ax.set_xlabel('Recency')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_zlabel('Monetary')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Clustering for Segments\n",
    "#### K-Means Clustering\n",
    "\n",
    "The K-means clustering belongs to the partition based\\centroid based hard clustering family of algorithms, a family of algorithms where each sample in a dataset is assigned to exactly one cluster.\n",
    "\n",
    "Based on this Euclidean distance metric, we can describe the k-means algorithm as a simple optimization problem, an iterative approach for minimizing the within-cluster sum of squared errors (SSE), which is sometimes also called cluster inertia. So, the objective of K-Means clustering is to minimize total intra-cluster variance, or, the squared error function: \n",
    "\n",
    "objective func  $$    J = \\sum _{j=1} ^k \\sum _{i=1} ^n || x _i^{(j)} - c _j|| ^2 $$\n",
    "\n",
    "where:\n",
    " - k number of clusters\n",
    " - n number of cases\n",
    " - c centroid of cluster j\n",
    "\n",
    "\n",
    "The steps that happen in the K-means algorithm for partitioning the data are as given follows:\n",
    "\n",
    "The algorithm starts with random point initializations of the required number of centers. The “K” in K-means stands for the number of clusters.\n",
    "In the next step, each of the data point is assigned to the center closest to it. The distance metric used in K-means clustering is normal Euclidian distance.\n",
    "Once the data points are assigned, the centers are recalculated by averaging the dimensions of the points belonging to the cluster.\n",
    "The process is repeated with new centers until we reach a point where the assignments become stable. In this case, the algorithm terminates.\n",
    "\n",
    "\n",
    "#### The Elbow Method\n",
    "\n",
    "Using the elbow method to find the optimal number of clusters. The idea behind the elbow method is to identify the value of k where the distortion begins to increase most rapidly. If k increases, the distortion will decrease, because the samples will be closer to the centroids they are assigned to.\n",
    "\n",
    "This method looks at the percentage of variance explained as a function of the number of clusters. More precisely, if one plots the percentage of variance explained by the clusters against the number of clusters, the first clusters will add much information (explain a lot of variance), but at some point the marginal gain will drop, giving an angle in the graph. The number of clusters is chosen at this point, hence the \"elbow criterion\". This \"elbow\" cannot always be unambiguously identified.Percentage of variance explained is the ratio of the between-group variance to the total variance, also known as an F-test. A slight variation of this method plots the curvature of the within group variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {}\n",
    "for k in range (1, 10):\n",
    "    # Create a kmeans model on our data, using k clusters.  \n",
    "    # random_state helps ensure that the algorithm returns the same results each time.\n",
    "    model = KMeans(\n",
    "        n_clusters=k, \n",
    "        init='k-means++',\n",
    "        n_init=10,\n",
    "        max_iter=300,\n",
    "        tol=1e-04,\n",
    "        random_state=101)\n",
    "\n",
    "    clusters[k] = model.fit_predict(df_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick examination of elbow method to find numbers of clusters to make.\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "kmeans = KMeans(init='k-means++', n_init=10, max_iter=300, tol=1e-04, random_state=101)\n",
    "\n",
    "elbow_m = KElbowVisualizer(kmeans, k=10, size=(1000, 500))\n",
    "elbow_m.fit(df_subset)\n",
    "\n",
    "best_k = elbow_m.elbow_value_\n",
    "print(\"BEST K:\", best_k)\n",
    "\n",
    "elbow_m.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in (2, 3, 4 , 5, 6):\n",
    "    fig = plt.figure(figsize=(20,5))\n",
    "    plt.title( \"{0} Clusters\".format(k))\n",
    "\n",
    "    ax = fig.add_subplot(121)\n",
    "    plt.scatter(x = df_subset[\"Recency\"], y = df_subset[\"Spent\"], c=clusters[k], cmap=plt.cm.Set1)\n",
    "    ax.set_xlabel(\"Recency\")\n",
    "    ax.set_ylabel(\"Spent\")\n",
    "\n",
    "    ax = fig.add_subplot(122)\n",
    "    plt.scatter(x = df_subset[\"Purchases\"], y = df_subset[\"Spent\"], c=clusters[k],cmap=plt.cm.Set1)\n",
    "    ax.set_xlabel(\"Purchases\")\n",
    "    ax.set_ylabel(\"Spent\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count = df_base.copy()\n",
    "\n",
    "df_count['clusters_3'] = clusters[3] #cluster_centers[3]['labels'] \n",
    "df_count['clusters_4'] = clusters[4] #cluster_centers[5]['labels']\n",
    "df_count['clusters_7'] = clusters[7] #cluster_centers[7]['labels']\n",
    "\n",
    "fig = plt.figure(figsize=(20,7))\n",
    "f1 = fig.add_subplot(131)\n",
    "market = df_count['clusters_3'].value_counts()\n",
    "plt.pie(market, labels=market.index, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "plt.title('3 Clusters')\n",
    "\n",
    "f1 = fig.add_subplot(132)\n",
    "market = df_count['clusters_4'].value_counts()\n",
    "plt.pie(market, labels=market.index, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "plt.title('4 Clusters')\n",
    "\n",
    "f1 = fig.add_subplot(133)\n",
    "market = df_count['clusters_7'].value_counts()\n",
    "plt.pie(market, labels=market.index, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "plt.title('7 Clusters')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,15))\n",
    "pal = [\"#682F2F\", \"#9E726F\", \"#D6B2B1\", \"#B9C0C9\", \"#9F8A78\", \"#F3AB60\"]\n",
    "pl=sns.swarmplot(x=clusters[best_k], y=df_base[\"Spent\"], color= \"#CBEDDD\", alpha=0.5 )\n",
    "pl=sns.boxenplot(x=clusters[best_k], y=df_base[\"Spent\"], palette=pal)\n",
    "plt.title(\"Spent\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,15))\n",
    "pal = [\"#682F2F\", \"#9E726F\", \"#D6B2B1\", \"#B9C0C9\", \"#9F8A78\", \"#F3AB60\"]\n",
    "pl=sns.swarmplot(x=clusters[best_k], y=df_base[\"Recency\"], color= \"#CBEDDD\", alpha=0.5 )\n",
    "pl=sns.boxenplot(x=clusters[best_k], y=df_base[\"Recency\"], palette=pal)\n",
    "plt.title(\"Recency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "https://scikit-learn.org/stable/modules/decomposition.html#pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca.fit(df_scaled)\n",
    "df_pca = pd.DataFrame(pca.transform(df_scaled), columns=([\"col1\",\"col2\", \"col3\"]))\n",
    "print('Explained variance ratio:', pca.explained_variance_ratio_)\n",
    "df_pca.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib qt\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A 3D Projection Of Data In The Reduced Dimension\n",
    "x = df_pca[\"col1\"]\n",
    "y = df_pca[\"col2\"]\n",
    "z = df_pca[\"col3\"]\n",
    "\n",
    "#To plot\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(x, y, z, c=\"maroon\", marker=\"o\" )\n",
    "ax.set_title(\"A 3D Projection Of Data In The Reduced Dimension\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering on PCA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "# Quick examination of elbow method to find numbers of clusters to make.\n",
    "print('Elbow Method to determine the number of clusters to be formed:')\n",
    "elbow_m = KElbowVisualizer(KMeans(), k=10)\n",
    "elbow_m.fit(df_pca)\n",
    "elbow_m.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# create the Agglomerative Clustering model \n",
    "aggc = AgglomerativeClustering(n_clusters=4)\n",
    "\n",
    "# fit model and predict clusters\n",
    "yhat_aggc = aggc.fit_predict(df_pca)\n",
    "df_pca[\"Clusters\"] = yhat_aggc\n",
    "\n",
    "# add the Clusters feature to the orignal dataframe.\n",
    "df_base[\"Clusters\"] = yhat_aggc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib qt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the clusters\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "ax = plt.subplot(111, projection='3d', label=\"bla\")\n",
    "ax.scatter(x, y, z, s=40, c=df_pca[\"Clusters\"], marker='o', cmap = cmap )\n",
    "ax.set_title(\"The Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting countplot of clusters\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "pal = [\"#682F2F\",\"#B9C0C9\", \"#9F8A78\",\"#F3AB60\"]\n",
    "pl = sns.countplot(x=df_base[\"Clusters\"], palette= pal)\n",
    "pl.set_title(\"Distribution Of The Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting count of total campaign accepted.\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "pl = sns.countplot(x=df_base[\"Campaigns\"], hue=df_base[\"Clusters\"], palette= pal)\n",
    "pl.set_title(\"Count Of Promotion Accepted\")\n",
    "pl.set_xlabel(\"Number Of Total Accepted Promotions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "profile_features = [ \"Kidhome\", \"Teenhome\", \"Income\", \"Age\", \"Children\", \n",
    "                    \"Family_Size\", \"Is_Parent\", \"Education\",\"Living_With\"]\n",
    "\n",
    "for pf in profile_features:\n",
    "    fig = plt.figure(figsize=(25,12))\n",
    "    sns.jointplot(x=df_base[pf], y=df_base[\"Spent\"], hue=df_base[\"Clusters\"], kind='hist', palette=pal)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Properties\n",
    "\n",
    "### Cluster 0\n",
    "- definitelly a parent\n",
    "- 2 to 4 family members\n",
    "- most have teenager children\n",
    "- relatively older\n",
    "\n",
    "### Cluster 1\n",
    "- definitelly NOT a parent\n",
    "- max 2 members in the family\n",
    "- all ages\n",
    "- high income\n",
    "\n",
    "### Cluster 2\n",
    "- most are parents\n",
    "- max 3 fmaily members\n",
    "- one kid, typically not a teenager\n",
    "- relatively younger\n",
    "\n",
    "### Cluster 3\n",
    "- definitelly a parent\n",
    "- 2 to 5 family members\n",
    "- teenager kids\n",
    "- relatively older\n",
    "- low income\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus\n",
    "\n",
    "render 3d interactive plots inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "df = df_base\n",
    "\n",
    "trace1 = go.Scatter3d(\n",
    "    x= df['Age'],\n",
    "    y= df['Spent'],\n",
    "    z= df['Purchases'],\n",
    "    mode='markers',\n",
    "     marker=dict(\n",
    "        color = df['Clusters'], \n",
    "        size= 10,\n",
    "        line=dict(\n",
    "            color= df['Clusters'],\n",
    "            width= 12\n",
    "        ),\n",
    "        opacity=0.8\n",
    "     )\n",
    ")\n",
    "data = [trace1]\n",
    "layout = go.Layout(\n",
    "    title= 'Clusters wrt Age, Income and Spending Scores',\n",
    "    scene = dict(\n",
    "            xaxis = dict(title  = 'Age'),\n",
    "            yaxis = dict(title  = 'Spending Score'),\n",
    "            zaxis = dict(title  = 'Annual Income')\n",
    "        )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.offline.iplot(fig)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac32a5cfdc42629f64c1ccaa7144552e6937cf528f4ff088dc2622e0738363a5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
