{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcc265c1",
   "metadata": {},
   "source": [
    "### Image Recognition with Keras\n",
    "CNN with Keras on the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c783d9",
   "metadata": {},
   "source": [
    "***\n",
    "#### Environment\n",
    "`conda activate tf-env`\n",
    "\n",
    "***\n",
    "#### Goals\n",
    "- Build a neural network model\n",
    "- Observe time taken to train\n",
    "- Observe ease of use\n",
    "- Experiment predicting on samples from test data\n",
    "\n",
    "***\n",
    "#### References:\n",
    "https://keras.io/examples/vision/mnist_convnet/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7f7d45",
   "metadata": {},
   "source": [
    "#### Basic python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ba4a0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a63a00",
   "metadata": {},
   "source": [
    "#### Load and prepare data\n",
    "\n",
    "Predefined dataset consisting in 6000 28x28 images for train and 1000 28x28 images for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5f96bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312f47d5",
   "metadata": {},
   "source": [
    "#### Define the Neural Network's Architecture\n",
    "\n",
    "This is a multiclass classification, hence Softmax is used on the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77e43cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                16010     \n",
      "=================================================================\n",
      "Total params: 34,826\n",
      "Trainable params: 34,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5523e087",
   "metadata": {},
   "source": [
    "#### Train the model\n",
    "\n",
    "Observe time taken for a small data set of 6000 28x28 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76c085f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "422/422 [==============================] - 23s 53ms/step - loss: 0.3660 - accuracy: 0.8906 - val_loss: 0.0817 - val_accuracy: 0.9782\n",
      "Epoch 2/15\n",
      "422/422 [==============================] - 22s 52ms/step - loss: 0.1105 - accuracy: 0.9667 - val_loss: 0.0555 - val_accuracy: 0.9863\n",
      "Epoch 3/15\n",
      "422/422 [==============================] - 22s 52ms/step - loss: 0.0816 - accuracy: 0.9742 - val_loss: 0.0479 - val_accuracy: 0.9872\n",
      "Epoch 4/15\n",
      "422/422 [==============================] - 22s 53ms/step - loss: 0.0681 - accuracy: 0.9791 - val_loss: 0.0405 - val_accuracy: 0.9895\n",
      "Epoch 5/15\n",
      "422/422 [==============================] - 22s 53ms/step - loss: 0.0606 - accuracy: 0.9814 - val_loss: 0.0413 - val_accuracy: 0.9882\n",
      "Epoch 6/15\n",
      "422/422 [==============================] - 24s 56ms/step - loss: 0.0546 - accuracy: 0.9832 - val_loss: 0.0368 - val_accuracy: 0.9908\n",
      "Epoch 7/15\n",
      "422/422 [==============================] - 22s 52ms/step - loss: 0.0502 - accuracy: 0.9845 - val_loss: 0.0348 - val_accuracy: 0.9915\n",
      "Epoch 8/15\n",
      "422/422 [==============================] - 21s 49ms/step - loss: 0.0445 - accuracy: 0.9861 - val_loss: 0.0313 - val_accuracy: 0.9907\n",
      "Epoch 9/15\n",
      "422/422 [==============================] - 21s 51ms/step - loss: 0.0433 - accuracy: 0.9865 - val_loss: 0.0337 - val_accuracy: 0.9927\n",
      "Epoch 10/15\n",
      "422/422 [==============================] - 21s 51ms/step - loss: 0.0390 - accuracy: 0.9877 - val_loss: 0.0319 - val_accuracy: 0.9915\n",
      "Epoch 11/15\n",
      "422/422 [==============================] - 21s 51ms/step - loss: 0.0384 - accuracy: 0.9877 - val_loss: 0.0289 - val_accuracy: 0.9918\n",
      "Epoch 12/15\n",
      "422/422 [==============================] - 21s 50ms/step - loss: 0.0342 - accuracy: 0.9891 - val_loss: 0.0308 - val_accuracy: 0.9912\n",
      "Epoch 13/15\n",
      "422/422 [==============================] - 21s 50ms/step - loss: 0.0336 - accuracy: 0.9890 - val_loss: 0.0296 - val_accuracy: 0.9910\n",
      "Epoch 14/15\n",
      "422/422 [==============================] - 21s 51ms/step - loss: 0.0325 - accuracy: 0.9890 - val_loss: 0.0325 - val_accuracy: 0.9913\n",
      "Epoch 15/15\n",
      "422/422 [==============================] - 21s 50ms/step - loss: 0.0310 - accuracy: 0.9901 - val_loss: 0.0297 - val_accuracy: 0.9922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24a26e694f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 15 # use 15 for a better model\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1,\n",
    "         callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e49695",
   "metadata": {},
   "source": [
    "#### Run on test data\n",
    "\n",
    "Asses model qulity on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95e7de62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.024754339829087257\n",
      "Test accuracy: 0.9914000034332275\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "predictions = model.predict(x_test)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6800a58c",
   "metadata": {},
   "source": [
    "#### Pick a random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e8fb3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using test digit sample 418\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24a274eab20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_digit = random.randint(1, 1000)\n",
    "print(\"Using test digit sample\", test_digit)\n",
    "plt.imshow(x_test[test_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4cd5e5",
   "metadata": {},
   "source": [
    "#### Make the prediction on the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b474cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max probability:  0.9999999\n",
      "I reckon the digit is:  2\n"
     ]
    }
   ],
   "source": [
    "prediction = predictions[test_digit]\n",
    "\n",
    "index_min = np.argmax(predictions[test_digit])\n",
    "print( \"Max probability: \", max(predictions[test_digit]))\n",
    "print( \"I reckon the digit is: \", index_min)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffca6146",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39199dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
